{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **í”„ë¡œì íŠ¸ ë£¨ë¸Œë¦­**"
      ],
      "metadata": {
        "id": "aG98q9r19loJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```markdown\n",
        "\n",
        "í•™ìŠµëª©í‘œ\n",
        "\n",
        "1. ê¸°ì¡´ KoGPT2ì™€ SFT ì ìš© ëª¨ë¸ ê²°ê³¼ ë¶„ì„í–ˆëŠ”ê°€?.\n",
        "\n",
        "     -ê¸°ì¡´ ëª¨ë¸ì˜ ê²°ê³¼ë¬¼ê³¼ SFTë¥¼ ì ìš©í•œ ëª¨ë¸ì˜ ê²°ê³¼ë¬¼ì„ ì •ëŸ‰/ì •ì„±ì ìœ¼ë¡œ ë¹„êµ/ë¶„ì„í–ˆë‹¤.\n",
        "\n",
        "2. SFT ëª¨ë¸ê³¼ RM ëª¨ë¸ ê²°ê³¼ ë¶„ì„ì„ í•´ë³´ì•˜ëŠ”ê°€?\n",
        "\n",
        "     - SFTë¥¼ ì ìš©í•œ ëª¨ë¸ì˜ ê²°ê³¼ë¬¼ê³¼ RMì„ ì ìš©í•œ ëª¨ë¸ì˜ ê²°ê³¼ë¬¼ì„ ì •ëŸ‰/ì •ì„±ì ìœ¼ë¡œ ë¹„êµ/ë¶„ì„í–ˆë‹¤\n",
        "\n",
        "3. ë°ì´í„°ì…‹ ì •ì œ / ìƒˆë¡œìš´ ë°ì´í„°ì…‹ / foundation model êµì²´ ì¤‘ í•˜ë‚˜ë¥¼ ì´ìš©í•´ ì •ëŸ‰ì  ì„±ëŠ¥ í–¥ìƒì„ í•´ë³´ì•˜ëŠ”ê°€?\n",
        "\n",
        "   1. ê¸°ì¡´ ë°ì´í„°ì…‹ì„ ì¶”ê°€ë¡œ ì •ì œí•˜ê³ , generation ì„±ëŠ¥ì„ ì˜¬ë¦¬ê¸° ìœ„í•œ ê¸°ë²•(Beam search, Top-k sampling ë“±)ì„ ì‹¤í—˜í•´ ëª¨ë¸ ì„±ëŠ¥ì„ í–¥ìƒì‹œì¼°ë‹¤.\n",
        "   2. ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•´ ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•˜ì—¬ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œì¼°ë‹¤.\n",
        "   3. ë” ì ì ˆí•œ í•™ìŠµ ì „ëµ(SFT, RM, PPO)ì„ ì ìš©í•˜ê±°ë‚˜ initial modelì„ ë³€ê²½í•´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œì¼°ë‹¤."
      ],
      "metadata": {
        "id": "Q9xejIp-9CBY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ì¤‘ìš” ì½”ë©ì—ì„œ ë‹¤ì‹œ ì—ëŸ¬ ë°œìƒ. ì¶œë ¥ì„ ëª¨ë‘ ì§€ìš°ê³  ë‹¤ì‹œ ì˜¬ë¦½ë‹ˆë‹¤. ì¶œë ¥ë¬¼ì„ ì°¸ì¡°í•˜ì‹œë ¤ë©´ ì•„ë˜ ì½”ë© ë§í¬ë¥¼ ë“¤ì–´ê°€ì£¼ì„¸ìš”.**\n",
        "\n",
        "https://colab.research.google.com/drive/1esHrnI15nXICQpGta8qxYINiND02X3t1?usp=sharing"
      ],
      "metadata": {
        "id": "4D8ohv_kbePN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ë°ì´í„°ì…‹ ì„¤ëª…**"
      ],
      "metadata": {
        "id": "B1SxbFYkK8Ii"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```markdown\n",
        "\n",
        "chatgptì˜ RLHFë¥¼ í•™ìŠµí•˜ê¸° ìœ„í•œ 4ê°€ì§€ stepì˜ í•œêµ­ì–´ ë°ì´í„°ì…‹\n",
        "\n",
        "ë‹¤ì–‘í•œ í•œêµ­ì–´ ì§ˆë¬¸ì„ í¬í•¨í•˜ëŠ” ë°ì´í„°ì…‹ì—ì„œ ì§ˆë¬¸ì„ ìˆ˜ì§‘í•˜ì˜€ìœ¼ë©°, ì—¬ê¸°ì—ëŠ” ChatbotData, AIí—ˆë¸Œì˜ í•œêµ­ì–´ ëŒ€í™” ë° ì¼ë°˜ìƒì‹ ë°ì´í„°, KorQuad ë“±ì´ í¬í•¨ë˜ì–´ìˆìŠµë‹ˆë‹¤. ì´ ì¤‘ 12,000ê°œì˜ ì§ˆë¬¸ì„ ëœë¤ìœ¼ë¡œ ì¶”ì¶œí•˜ì˜€ìŠµë‹ˆë‹¤.\n",
        "\n",
        "ìˆ˜ì§‘ëœ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•˜ê¸° ìœ„í•´ ChatGPTë¥¼ í™œìš©í•˜ì˜€ìœ¼ë©° ê° ì§ˆë¬¸ì„ ChatGPTì— ì…ë ¥í•˜ì—¬ ìë™ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•˜ì˜€ìŠµë‹ˆë‹¤.\n",
        "\n",
        "ì‚¬ëŒì´ ì§ì ‘ ë ˆì´ë¸”ë§í•˜ëŠ” ëŒ€ì‹  ChatGPTë¥¼ í™œìš©í•˜ì—¬ ëŒ€ëŸ‰ì˜ ì§ˆë¬¸-ë‹µë³€ ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ìƒì„±í•˜ì˜€ìœ¼ë©°, ì´ë¥¼ í†µí•´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê³ ì í•˜ì˜€ìŠµë‹ˆë‹¤.\n",
        "\n",
        "1.1) í•œêµ­ì–´ ì§ˆë¬¸ ë°ì´í„°ì…‹ì—ì„œ ì§ˆë¬¸ ìˆ˜ì§‘\n",
        "        \n",
        "        ì¶œì²˜\n",
        "        data1, ChatbotData/11824ë¬¸ì¥\n",
        "        data2, AIí—ˆë¸Œ_í•œêµ­ì–´ ëŒ€í™”/49711ë¬¸ì¥\n",
        "        data3, AIí—ˆë¸Œ_ì¼ë°˜ìƒì‹/100268\n",
        "        data4, KorQuad/5774\n",
        "        4ê°€ì§€ ë°ì´í„°ì…‹ì—ì„œ ì´ 167,577 ì§ˆë¬¸ì…‹ ìˆ˜ì§‘, 12,000ê°œ ì§ˆë¬¸ ëœë¤ ì¶”ì¶œ\n",
        "\n",
        "RLHF ë°ì´í„°ì…‹ ìƒì„±\n",
        "1.2) step1, SFT(Supervised Fine-tuning) ë°ì´í„°ì…‹ êµ¬ì¶•\n",
        "\n",
        "1.3) step2, RM(Reward Model, ì¢‹ì€ê¸€ ì±„ì ëª¨ë¸) ë°ì´í„°ì…‹ êµ¬ì¶•\n",
        "\n",
        "1.4) step3, PPO(ì§ˆë¬¸ì— ``ë”`` ì˜ ë‹µí•˜ëŠ” ëª¨ë¸) ë°ì´í„°ì…‹ êµ¬ì¶•"
      ],
      "metadata": {
        "id": "lEGGFklcOYrj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ì „ë°˜ì  í”„ë¡œì íŠ¸ ì†ŒíšŒ ë° ëŠë‚€ ì **"
      ],
      "metadata": {
        "id": "QKE5YCVrLQ6A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) ê³¼ì •ì„ í†µí•˜ë©´ì„œ, ì¸ê³µì§€ëŠ¥ì˜ í™œìš© ë°©ë²•ê³¼ ì¥ë‹¨ì ì„ ë¹„íŒì  ì•Œê²Œ ë˜ì—ˆë‹¤.\n",
        "   êµ¬ì²´ì ìœ¼ë¡œ, ê°ê°ì˜ ì¸ê³µì§€ëŠ¥ì˜ ì„±ëŠ¥ê³¼ í™œìš©ë°©ë²•ì— ëŒ€í•´ì„œ ë¹„íŒì ì¸ ì‹œê°ì„ ê°€ì§€ê²Œ ë˜ì—ˆìŒ.\n",
        "    \n",
        "    - Claudeì˜ ì„±ëŠ¥, í™œìš©ë²•ì— ì´ì œëŠ” ë¬´ì¡°ê±´ ë§¡ê¸°ì§€ ì•Šê³  ìª¼ê°œì„œ ê³¼ì œë¥¼ ë§¡ê¸°ê³  ê¼­ ê²€ì‚¬í•œë‹¤. ìƒë‹¹íˆ ì—‰í„°ë¦¬ê°€ ë§ë‹¤.\n",
        "    - Chatgpt: ë‹¤ë¥¸ ì¸ê³µì§€ëŠ¥ì— ë¹„í•˜ë©´ ì„±ëŠ¥ì´ ë©”ë¡±ì´ë‹¤.\n",
        "    - Google Gemini: ìƒë‹¹íˆ ì¸ìƒì ì´ë‹¤. ë‹¤ë§Œ, ì½”ë”© ë¶„ì•¼ì—ì„œëŠ” ì–´ë–»ê²Œ í™œìš©í•  ì§€ ì˜ íŒë‹¨í•´ë´ì•¼í•œë‹¤.  \n",
        "\n",
        "2) ê¸°ëŠ¥/ ëª¨ë“ˆì„ ë„ì…í•  ë•Œ, ê³„íšê³¼ ì˜ˆì¸¡ì„ ì˜ ì„¸ìš°ê³  í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤ëŠ” ê²ƒì„ ëŠê¼ˆìŒ. ì˜ëª» ì‹œì‘í•˜ë©´ 1%ì˜ ê°œì„ ì„ ìœ„í•´ì„œ 50% ì‹œê°„ê³¼ ìì›ì„ ë‚­ë¹„í•˜ëŠ” ê²ƒì´ ë‹¤ë°˜ì‚¬ë¡œ ë°œìƒ\n",
        "\n",
        "3) ì…‹íŒ… ë° í™˜ê²½ì„¤ì •ì— ë°˜ë³µì ìœ¼ ë§ì€ ì‹œê°„ ì†Œìš”ì™€ ë¹„ìš©ì´ ë°œìƒí•´ì„œ, ì–´ë–»ê²Œ í•˜ë©´ ì´ë¥¼ íš¨ìœ¨í™”í•  ê²ƒì¸ê°€ì— ëŒ€í•´ì„œ ê³ ë¯¼í•˜ê²Œ ë˜ì—ˆìŒ. ì…‹íŒ…ì„ ì˜ëª»í•˜ë©´ ì¶œë°œë„ ëª»í•˜ëŠ” ìˆ˜ê°€ ë‹¤ë°˜ì‚¬!"
      ],
      "metadata": {
        "id": "nCULdoeRuocE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **í”„ë¡œì íŠ¸ ê²°ê³¼ ìš”ì•½**"
      ],
      "metadata": {
        "id": "22qNEWqHLNlz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ê¸°ì´ˆ ëª¨ë¸ì—ì„œëŠ” accuracyê°€ 50%ìˆ˜ì¤€ì´ì–´ì„œ ì˜ë¯¸ì—†ëŠ” ì„±ëŠ¥ì´ë¼ê³  ë¶„ì„ë¨\n",
        "\n",
        "<SFT>\n",
        "ìƒë‹¹íˆ ìˆ˜ì¤€ ë†’ì€ ì–¸ì–´êµ¬ì‚¬ ëŠ¥ë ¥ì´ í™•ë³´ë¨ì„ íŒŒì•…í•˜ì˜€ìŒ. ê·¸ëŸ¬ë‚˜, í›„ë°˜ë¶€ë¡œ ê°ˆìˆ˜ë¡ ì—‰ëš±í•œ ëŒ€í™”ë¥¼ ë˜ì§€ê³  ìˆìŒì„ ë°œê²¬í•¨ -> alignment (í˜¹ì€ catastropic forgetting ì´ìŠˆ ë°œê²¬)\n",
        "\n",
        "    ### Instruction(ëª…ë ¹ì–´):\n",
        "    ë¶ˆê³ ê¸°ìš© ê³ ê¸° í•œìš°ì—ìš”?\n",
        "\n",
        "    ### Response(ì‘ë‹µ):'ì €ëŠ” ì¸ê³µì§€ëŠ¥ ì–´ì‹œìŠ¤í„´íŠ¸ì´ê¸° ë•Œë¬¸ì— ì‹¤ì œë¡œ ê³ ê¸°ë¥¼ ë¨¹ì„ ìˆ˜ëŠ” ì—†ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì¼ë°˜ì ìœ¼ë¡œ ë¶ˆê³ ê¸°ìš© ê³ ê¸°ëŠ” í•œìš°, ì‡ ê³ ê¸°, ë¼ì§€ê³ ê¸° ë“± ë‹¤ì–‘í•œ ì¢…ë¥˜ì˜ ê³ ê¸°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, í•œìš°ëŠ” ëŒ€í‘œì ì¸ ê³ ê¸‰ ìœ¡ë¥˜ë¡œ ìœ ëª…í•©ë‹ˆë‹¤. ë˜í•œ, ë¼ì§€ê³ ê¸°ëŠ” ì†Œê³ ê¸°ì˜ ëŒ€ëª…ì‚¬ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. ì‘ì„±ëœ ë¬¸êµ¬ì—ì„œëŠ” \"ë¶ˆ\n",
        "\n",
        "<RM> - ìŠ¤ì½”ì–´ë§ì„ í†µí•´ì„œ ë³´ë‹¤ ì¢‹ì€ ë‹µë³€ì„ í‰ê°€í•¨ì„ ì•Œ ìˆ˜ ìˆë‹¤.\n",
        "\n",
        "input: ì¸ê³µì§€ëŠ¥(AI)ì€ ì»´í“¨í„°ì—ì„œ ìŒì„± ë° ì‘ì„±ëœ ì–¸ì–´ë¥¼ ë³´ê³  ì´í•´í•˜ê³  ë²ˆì—­í•˜ê³  ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ì¶”ì²œí•˜ëŠ” ê¸°ëŠ¥ì„ í¬í•¨í•˜ì—¬ ë‹¤ì–‘í•œ ê³ ê¸‰ ê¸°ëŠ¥ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” ì¼ë ¨ì˜ ê¸°ìˆ ì…ë‹ˆë‹¤.\n",
        "reward score: 4.9\n",
        "\n",
        "input: ì¸ê³µì§€ëŠ¥(AI)ì€ ì»´í“¨í„°ì—ì„œ ìŒì„± ë° ì‘ì„±ëœ ì–¸ì–´ë¥¼ ë³´ê³  ì´í•´í•˜ê³  ë²ˆì—­í•˜ê³  ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ì¶”ì²œí•˜ëŠ” ê¸°ëŠ¥ì„ í¬í•¨í•˜ì—¬ ë‹¤ì–‘í•œ ê³ ê¸‰ ê¸°ëŠ¥ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” ì¼ë ¨ì˜ ê¸°ìˆ ì…ë‹ˆë‹¤. AIëŠ” í˜„ëŒ€ì ì¸ ì»´í“¨íŒ… í˜ì‹ ì—ì„œ ì¤‘ì¶”ì ì¸ ì—­í• ì„ í•˜ë©° ê°œì¸ê³¼ ë¹„ì¦ˆë‹ˆìŠ¤ì˜ ê°€ì¹˜ë¥¼ ì°½ì¶œí•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ê´‘í•™ ë¬¸ì ì¸ì‹(OCR)ì€ AIë¥¼ ì‚¬ìš©í•´ ì´ë¯¸ì§€ ë° ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ ë° ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ê³ , êµ¬ì¡°í™”ë˜ì§€ ì•Šì€ ì½˜í…ì¸ ë¥¼ ë¹„ì¦ˆë‹ˆìŠ¤ì— ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ ë§Œë“¤ê³ , ìœ ìš©í•œ ì •ë³´ë¥¼ ì°½ì¶œí•©ë‹ˆë‹¤.\n",
        "reward score: 7.8\n",
        "\n",
        "<PPO: ì‹¤í—˜ #2 ì‚¬ë¡€> ê°œì„ ë˜ì—ˆìœ¼ë‚˜, í›„ë°˜ìœ¼ë¡œ ê°€ë©´ ì‚¼ì²œí¬ë¡œ ë¹ ì§€ëŠ” í˜„ìƒ ë°œìƒ!\n",
        "\n",
        "Instruction(ëª…ë ¹ì–´):\n",
        "ì˜¤ëŠ˜ ë¯¸ì„¸ë¨¼ì§€ ì–´ë•Œ?\n",
        "\n",
        "Response(ì‘ë‹µ):'ë¯¸ì„¸ë¨¼ì§€ ë¬¸ì œëŠ” ë§¤ìš° ì‹¬ê°í•œ ìƒí™©ì…ë‹ˆë‹¤. ì´ëŸ¬í•œ ìƒí™©ì€ ì—¬ëŸ¬ ìš”ì¸ë“¤ë¡œ ì¸í•´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¼ìƒì ì¸ ê±´ê°• ê´€ë¦¬ë„ ì¤‘ìš”í•˜ë©°, í•„ìš”í•˜ë‹¤ë©´ ì‹¤ë‚´ ê³µê¸° ì§ˆì„ ê°œì„ í•˜ê³  ì²­ê²°í•œ í™˜ê²½ì„ ìœ ì§€í•˜ëŠ” ë“±ì˜ ëŒ€ì²˜ë¥¼ í•´ì•¼í•©ë‹ˆë‹¤. ë§Œì•½ ì´ëŸ¬í•œ ì¡°ì¹˜ë¥¼ ì·¨í•˜ë©´ ë¯¸ì„¸ë¨¼ì§€ê°€ ë°œìƒí•˜ëŠ” ì‹œê°„ì€ ë” ë¹¨ë¼ì§€ê³ , í•´ê²°ì±…ì€ ì°¾ê¸° ì–´ë ¤ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê´€í•œ ë” ìì„¸í•œ ì •ë³´ëŠ” í™˜ê²½ë¶€ í™ˆí˜ì´ì§€ì— ê²€ìƒ‰í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê±´ê°•í•˜ê²Œ ëŒ€ì²˜í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤. ê±´ê°• ë¬¸ì œì— ëŒ€í•´ì„œëŠ” ì „ë¬¸ê°€ë“¤ê³¼ì˜ ìƒë‹´ë„ ë°›ì•„ë³´ì‹œëŠ” ê²ƒì„ ì¶”ì²œí•©ë‹ˆë‹¤. ë¶ˆì•ˆê³¼ ë¬¸ì œ í•´ê²°ì„ ìœ„í•´ ìµœì„ ì„ ë‹¤í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤. ë¶ˆí•„ìš”í•œ ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ì¤„ì´ê¸° ìœ„í•´ ì¼ìƒ ìƒí™œ ì‹œê°„ì— íœ´ì‹ì´ë‚˜ ì—¬ê°€ ì‹œê°„ì„ ê°€ì§€ëŠ” ê²ƒë„ ì¢‹ìŠµë‹ˆë‹¤. ê´€ì—¬í•  ê°€ì¹˜ê°€ ìˆìŠµë‹ˆë‹¤. ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ í•´ì†Œí•˜ê³  ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ í•´ì†Œí•˜ê¸° ìœ„í•œ ê²ƒì€ ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤.è‡£ê»˜ì„œ ì¼ìƒ ìƒí™œì— ëŒ€í•œ ì¡°ì–¸ì„ ë“œë¦´ ìˆ˜ ìˆë‹¤ë©´ ê¼­ ë¶€íƒë“œë¦½ë‹ˆë‹¤. ê±´ê°•ì„ ìœ ì§€í•˜ëŠ”ë°ëŠ” ì—¬ëŸ¬ ìš”ì¸ë“¤ì— ì˜í•œ ëŒ€ì²˜ì±…ë„ í•„ìš”í•©ë‹ˆë‹¤. ì ì ˆí•œ ì˜ˆë°© ëŒ€ì±…ë“¤ì´ ìˆëŠ”ì§€ ìˆ˜ì‹œë¡œ í™•ì¸í•˜ì—¬ íš¨ê³¼ì ì¸ í•´ê²°ì±…ì„ ì°¾ì•„ë³´ì„¸ìš”. ëŒ€ì²˜í•˜ê¸°ë€ ì–´ë µê² ì§€ë§Œ, ê°€ëŠ¥í•œ í•œ ì ì ˆíˆ ëŒ€ì²˜í•˜ê³  ì¡°ì ˆí•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ì§ˆë³‘ ê´€ë¦¬ ë° ê±´ê°• ìƒíƒœ ê°œì„  ë“±ì˜ ì¶”ê°€ì ì¸ ë¬¸ì œê°€ ìˆë‹¤ë©´ ì „ë¬¸ê°€ì˜ ë„ì›€ë„ ë°›ì•„\n",
        "\n",
        "# **í›„ë°˜ë¶€ë¡œ ê°ˆìˆ˜ë¡ ì‚¼ì²œí¬ë¡œ ë¹ ì§€ëŠ” í˜„ìƒ í˜¹ì€ ëŠ¥ë ¥ì´ ë§¤ìš° í¥ë¯¸ë¡­ìŠµë‹ˆë‹¤!**\n"
      ],
      "metadata": {
        "id": "laoY5MjP0bqk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tsa4-pys5nZK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **í”„ë¡œì íŠ¸ ì½”ë“œ**"
      ],
      "metadata": {
        "id": "tNoTHPI0LBxo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-v31FRQr_OZH"
      },
      "outputs": [],
      "source": [
        "#ê´€ë¦¬ìë¡œ ì„¤ì¹˜í•´ì•¼ì§€ ëœë‹¤. í°íŠ¸ ë¬¸ì œ í•´ê²°ì„ ë°°ì› ë‹¤. ì½”ë©ì—ì„œëŠ” í•­ìƒ ë¬¸ì œê°€ ë°œìƒí•œë‹¤. ì¢€ ë¬¸ì œ ì¢€ í•´ê²°í•´ì¤˜ë¼.\n",
        "!sudo apt update\n",
        "!sudo apt install fonts-nanum"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ì½”ë©ì—ì„œ í•œê¸€í°íŠ¸ ì„¤ì¹˜ëŠ” ì–¸ì œë‚˜ ì• ë¥¼ ë¨¹ì¸ë‹¤. ì–¸ì œê¹Œì§€ ì´ëŸ´êº¼ë‹ˆ...!\n",
        "\n",
        "# 1ë‹¨ê³„: í°íŠ¸ ìºì‹œ ì™„ì „ ì‚­ì œ\n",
        "!rm -rf ~/.cache/matplotlib\n",
        "!rm -rf ~/.fontconfig\n",
        "print(\"âœ“ í°íŠ¸ ìºì‹œ ì‚­ì œ ì™„ë£Œ\")\n",
        "\n",
        "\n",
        "# 3ë‹¨ê³„: í°íŠ¸ ì„¤ì • ì¬êµ¬ì„±\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.font_manager as fm\n",
        "import os\n",
        "\n",
        "# matplotlib í°íŠ¸ ìºì‹œ ê°•ì œ ì´ˆê¸°í™”ëŠ” í•„ìš” ì—†ìŒ (ëŸ°íƒ€ì„ ì¬ì‹œì‘ìœ¼ë¡œ ì¶©ë¶„)\n",
        "# fm._rebuild() # ì´ ë¼ì¸ì„ ì œê±° ë˜ëŠ” ì£¼ì„ ì²˜ë¦¬í•˜ì—¬ ì˜¤ë¥˜ë¥¼ í•´ê²°í•©ë‹ˆë‹¤.\n",
        "\n",
        "# 4ë‹¨ê³„: ì‚¬ìš© ê°€ëŠ¥í•œ í°íŠ¸ í™•ì¸\n",
        "available_fonts = [f.name for f in fm.fontManager.ttflist]\n",
        "nanum_check = [f for f in available_fonts if 'nanum' in f.lower()]\n",
        "print(f\"\\në‚˜ëˆ” í°íŠ¸ í™•ì¸: {nanum_check}\")\n",
        "\n",
        "# 5ë‹¨ê³„: í°íŠ¸ ì„¤ì •\n",
        "plt.rcParams['font.family'] = 'NanumGothic'\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "print(f\"\\ní˜„ì¬ ì„¤ì • í°íŠ¸: {plt.rcParams['font.family']}\")"
      ],
      "metadata": {
        "id": "b9FXqGA-OPxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#í•œê¸€ í°íŠ¸ ì„¤ì¹˜ ì„±ê³µ í™•ì¸\n",
        "import matplotlib.font_manager as fm # ì´ ì¤„ì„ ì¶”ê°€í•˜ì—¬ fmì„ ì •ì˜í•©ë‹ˆë‹¤.\n",
        "\n",
        "# í°íŠ¸ ìºì‹œ ê°•ì œ ì¬ë¡œë“œ\n",
        "fm._load_fontmanager(try_read_cache=False)\n",
        "\n",
        "# ì„¤ì¹˜ëœ ë‚˜ëˆ” í°íŠ¸ í™•ì¸\n",
        "nanum_fonts = [f.name for f in fm.fontManager.ttflist if 'nanum' in f.name.lower()]\n",
        "print(\"ì„¤ì¹˜ëœ ë‚˜ëˆ” í°íŠ¸:\", nanum_fonts)\n",
        "\n",
        "# matplotlib ì„¤ì •\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['font.family'] = 'NanumGothic'\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸ í”Œë¡¯\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot([1, 2, 3], [1, 2, 3])\n",
        "plt.title('í•œê¸€ í°íŠ¸ í…ŒìŠ¤íŠ¸')\n",
        "plt.xlabel('Xì¶• ë ˆì´ë¸”')\n",
        "plt.ylabel('Yì¶• ë ˆì´ë¸”')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "X4PEAVhAOltY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlYN3tC59DN6"
      },
      "outputs": [],
      "source": [
        "#ê°€ì ¸ì˜¤ê¸°\n",
        "!git clone https://github.com/airobotlab/KoChatGPT\n",
        "!cp -r /content/KoChatGPT/colossalai_ChatGPT_230319/chatgpt /content/chatgpt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install loralib\n",
        "!pip install trl\n",
        "!pip install accelerate\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "cCyI20tXBR2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "í˜„ì¬ ì½”ë© ë²„ì „ì—ì„œëŠ” ì¼ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ëŠ”ë° ì¥ì• ê°€ ìˆì–´ ì›ë³¸ ì†ŒìŠ¤ ì½”ë“œì˜ ì¼ë¶€ë¥¼ ìˆ˜ì •í•œ ë’¤ ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤. ë‹¤ìŒ ì½”ë“œë¥¼ ìœ„ clone ì´í›„ ë™ì‘ì‹œì¼œì•¼í•©ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "O5_O-lbaJcWl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "modifications = [\n",
        "    {\n",
        "        \"file\": \"/content/chatgpt/trainer/callbacks/save_checkpoint.py\",\n",
        "        \"changes\": [\n",
        "            {\"line\": 3, \"old\": \"from chatgpt.trainer.strategies import ColossalAIStrategy, Strategy\",\n",
        "             \"new\": \"from chatgpt.trainer.strategies import Strategy\"},\n",
        "            {\"line\": 71, \"old\": \"only_rank0 = not isinstance(self.strategy, ColossalAIStrategy)\",\n",
        "             \"new\": \"            only_rank0 = not isinstance(self.strategy)\"},\n",
        "        ],\n",
        "    },\n",
        "    {\n",
        "        \"file\": \"/content/chatgpt/trainer/strategies/__init__.py\",\n",
        "        \"changes\": [\n",
        "            {\"line\": 1, \"old\": \"from .colossalai import ColossalAIStrategy\", \"new\": \"\"},  # ì‚­ì œ\n",
        "            {\"line\": 5, \"old\": \"__all__ = ['Strategy', 'NaiveStrategy', 'DDPStrategy', 'ColossalAIStrategy']\",\n",
        "             \"new\": \"__all__ = ['Strategy', 'NaiveStrategy', 'DDPStrategy']\"},\n",
        "        ],\n",
        "    },\n",
        "    {\n",
        "        \"file\": \"/content/chatgpt/dataset/reward_dataset.py\",\n",
        "        \"changes\": [\n",
        "            {\"line\": 3, \"old\": \"from tqdm import tqdm\", \"new\": \"from tqdm.notebook import tqdm\"},\n",
        "        ],\n",
        "    },\n",
        "    {\n",
        "        \"file\": \"/content/chatgpt/trainer/base.py\",\n",
        "        \"changes\": [\n",
        "            {\"line\": 8, \"old\": \"from tqdm import tqdm\", \"new\": \"from tqdm.notebook import tqdm\"},\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"file\": \"/content/chatgpt/trainer/rm.py\",\n",
        "        \"changes\": [\n",
        "            {\"line\": 8, \"old\": \"from tqdm import tqdm\", \"new\": \"from tqdm.notebook import tqdm\"},\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "\n",
        "\n",
        "def modify_file(file_path, changes):\n",
        "    \"\"\"íŒŒì¼ì—ì„œ ì§€ì •ëœ ì¤„ì„ ì°¾ì•„ ë‚´ìš©ì„ ìˆ˜ì •í•˜ëŠ” í•¨ìˆ˜\"\"\"\n",
        "\n",
        "    if not os.path.exists(file_path):\n",
        "        print(f\"âš ï¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_path}\")\n",
        "        return\n",
        "\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    modified = False\n",
        "\n",
        "    for change in changes:\n",
        "        line_index = change[\"line\"]\n",
        "        if 0 <= line_index < len(lines):\n",
        "            if lines[line_index].strip() == change[\"old\"]:\n",
        "                lines[line_index] = change[\"new\"] + \"\\n\"\n",
        "                modified = True\n",
        "            else:\n",
        "                print(f\"âš ï¸ {file_path} íŒŒì¼ì˜ {change['line']}ë²ˆì§¸ ì¤„ì´ ì˜ˆìƒê³¼ ë‹¤ë¦…ë‹ˆë‹¤.\")\n",
        "                print(f\"   ì˜ˆìƒ: {change['old']}\")\n",
        "                print(f\"   ì‹¤ì œ: {lines[line_index].strip()}\")\n",
        "\n",
        "    if modified:\n",
        "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
        "            file.writelines(lines)\n",
        "        print(f\"âœ… ìˆ˜ì • ì™„ë£Œ: {file_path}\")\n",
        "    else:\n",
        "        print(f\"âš ï¸ {file_path} ìˆ˜ì •í•  ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "for mod in modifications:\n",
        "    modify_file(mod[\"file\"], mod[\"changes\"])"
      ],
      "metadata": {
        "id": "_muiB_gSCxJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import pandas as pd\n",
        "import numpy\n",
        "\n",
        "print(\"Torch version:{}\".format(torch.__version__)) # Torch version:1.12.1\n",
        "print(\"Cuda version: {}\".format(torch.version.cuda)) # Cuda version: 11.3\n",
        "print(\"transformers version: {}\".format(transformers.__version__)) # transformers 4.28.0\n",
        "print(\"GPU ì‚¬ìš© ê°€ëŠ¥ì—¬ë¶€: {}\".format(torch.cuda.is_available()))\n",
        "\n",
        "# ë§Œì¼ ì•„ë˜ ëª¨ë“ˆì´ ë¶ˆëŸ¬ì™€ì§€ì§€ ì•ŠëŠ”ë‹¤ë©´ Clone ë° ìˆ˜ì •ì„ ì˜ ì§„í–‰í–ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.\n",
        "# ì•„ë˜ ëª¨ë“ˆ ë¶ˆëŸ¬ì ¸ì™”ëŠ”ì§€ í™•ì¸ í›„ ì¶œë ¥ ì¶”\n",
        "from chatgpt.trainer.strategies import NaiveStrategy\n",
        "\n",
        "print(NaiveStrategy)\n",
        "print(\"âœ… ëª¨ë“ˆ import ì„±ê³µ!\")"
      ],
      "metadata": {
        "id": "KIswcYQyJs-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model_name = \"skt/kogpt2-base-v2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
      ],
      "metadata": {
        "id": "Jyl9KOA7Lyk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ì•„ë˜ ì½”ë“œ ë¸”ëŸ­ë“¤ì€ ëª¨ë¸ì— ëŒ€í•œ ê¸°ë³¸ì ì¸ ì„±ëŠ¥ ì ê²€ (ë””ë²„ê¹… ì´ìŠˆê°€ ì—†ìœ¼ë©´ í–¥í›„ íŒ¨ìŠ¤)"
      ],
      "metadata": {
        "id": "eTyClEcxMTGo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.model_max_length"
      ],
      "metadata": {
        "id": "4laDHPYVME8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.n_positions"
      ],
      "metadata": {
        "id": "GdcxwJcOR8G5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_txt = \"ë°”ëŒë„ ì—†ëŠ” ê³µì¤‘ì— ìˆ˜ì§ì˜ íŒŒë¬¸ì„ ë‚´ì´ë©° ê³ ìš”íˆ ë–¨ì–´ì§€ëŠ” ì˜¤ë™ìì€ ëˆ„êµ¬ì˜ ë°œìì·¨ ì…ë‹ˆê¹Œ.\""
      ],
      "metadata": {
        "id": "CFK33nmwSBXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer(input_txt).tokens()\n",
        "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].numpy()"
      ],
      "metadata": {
        "id": "ufgrr5agSI8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.options.display.max_columns = 40\n",
        "pd.options.display.max_rows = 60\n",
        "df = pd.DataFrame([tokens, input_ids[0]], index=[\"kogpt-2_tokens\", \"Input_IDs\"])\n",
        "df"
      ],
      "metadata": {
        "id": "Z7MdLnqBSLOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length=128\n",
        "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
        "output_greedy = model.generate(input_ids, max_length=max_length, do_sample=False)\n",
        "print(tokenizer.decode(output_greedy[0]))"
      ],
      "metadata": {
        "id": "uDFHcyYLSRDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ë¶„ì„ ê²°ê³¼: ì²˜ìŒ ë””ì½”ë”©ì‹œ greedy ë°©ì‹ë§Œìœ¼ë¡œ ë°˜ë³µë˜ëŠ” ë‹µë³€ì˜ ë§¥ë½ë„ ì „í˜€ ìƒê´€ ì—†ê±°ë‚˜ í˜¹ì€ ì´í•´í•˜ì§€ ëª»í•œ ìƒíƒœì—ì„œ....  ê·¸ ë‹¨ì–´ë¥¼ ë°˜ë³µì ìœ¼ë¡œ ë°°ì¶œí•˜ì˜€ë‹¤."
      ],
      "metadata": {
        "id": "fIwTrqV2UptD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
        "output_beam = model.generate(input_ids, max_length=max_length, num_beams=10, no_repeat_ngram_size=2,\n",
        "                             do_sample=False)\n",
        "print(tokenizer.decode(output_beam[0]))"
      ],
      "metadata": {
        "id": "cSv0YbnKSUkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ë¶„ì„ ê²°ê³¼: n-gram size=2 ì¶”ê°€ ì‹œ, ì§ˆë¬¸ì—ëŠ” ì „í˜€ ìƒê´€ ì—†ëŠ” ë‹µë³€ì´ì§€ë§Œ ë‹µë³€ ë‚´ì—ì„œëŠ” ì•½ê°„ì˜ ë§¥ë½ì´ íë¦„ì§€ë§Œ ì—¬ì „íˆ ë°˜ë³µëœ ë‹µë³€ì„ êµ¬ì‚¬í•¨ì„ ì•Œ ìˆ˜ ìˆë‹¤."
      ],
      "metadata": {
        "id": "Z1E84P3OTr1y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_beam = model.generate(input_ids, max_length=max_length, num_beams=7, no_repeat_ngram_size=2,\n",
        "                             do_sample=True, temperature=2.0, top_k=50)\n",
        "print(tokenizer.decode(output_beam[0]))"
      ],
      "metadata": {
        "id": "lLoyux0KSZ44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ë¶„ì„ ê²°ê³¼: top_k=50 ì¶”ê°€ ì‹œ, ì›ë˜ inputë’¤ì— ì™œ ë‹µë³€ì´ ë¶™ì–´ë‚˜ì˜¤ëŠ”ì§€ ì›ì¸ ë¶„ì„ ì¤‘ì´ë‹¤. ì•„ì§ ì˜ ëª¨ë¥´ê² ë‹¤.\n",
        "ë‹µë³€ì— ë§¥ë½ì€ ì•„ì§ ì˜ ì´ì–´ì§€ì§€ ì•Šê³  ë‹µë³€ ë‚´ì—ì„œëŠ” ì¡°ê¸ˆ ë” ë§¥ë½ì´ íë¥´ì§€ë§Œ ì—¬ì „íˆ ì´í•´ê°€ ì•ˆë˜ëŠ” ë‹µë³€ì„ í•˜ê³  ìˆë‹¤."
      ],
      "metadata": {
        "id": "lpTr6PtCXdSv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_beam = model.generate(input_ids, max_length=max_length, num_beams=7, no_repeat_ngram_size=2,\n",
        "                             do_sample=True, top_p=0.90)\n",
        "print(tokenizer.decode(output_beam[0]))"
      ],
      "metadata": {
        "id": "MxwsAX0aSj8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ë¶„ì„ ê²°ê³¼: top_p=0.90 ì¶”ê°€ ì‹œ,\n",
        "ë‹µë³€ì— ë§¥ë½ì€ ì•„ì§ ì˜ ì´ì–´ì§€ì§€ ì•Šê³  ë‹µë³€ ë‚´ì—ì„œëŠ” ì¡°ê¸ˆ ë” ë§¥ë½ì´ íë¥´ì§€ë§Œ ì•„ê¹Œë³´ë‹¤ëŠ” ì¡°ê¸ˆë” ì´í•´ê°€ ì•ˆë˜ëŠ” ë‹µë³€ì˜ ë§¥ë½ì´ ë“œëŸ¬ë‚˜ëŠ” ê²ƒ ê°™ë‹¤."
      ],
      "metadata": {
        "id": "jgeBQyXWYPex"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DATASET í™•ì¸**"
      ],
      "metadata": {
        "id": "YWt-HGEuYi64"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3ê°€ì§€ ë°ì´í„°ë¥¼ ë¹„êµí•¨\n",
        "import json\n",
        "\n",
        "# 1ë‹¨ê³„: íŒŒì¼ ê²½ë¡œ ì •ì˜\n",
        "data_path_1_SFT = 'KoChatGPT/data_kochatgpt/kochatgpt_1_SFT.jsonl'\n",
        "data_path_2_RM = 'KoChatGPT/data_kochatgpt/kochatgpt_2_RM.jsonl'\n",
        "data_path_3_PPO = 'KoChatGPT/data_kochatgpt/kochatgpt_3_PPO.jsonl'\n",
        "\n",
        "# 2ë‹¨ê³„: ê°ê° ë‹¤ë¥¸ ë³€ìˆ˜ì— ì €ì¥í•˜ê¸°\n",
        "# JSONL íŒŒì¼ ì½ê¸° í•¨ìˆ˜ (ìˆ˜ì •ë¨: ì „ì²´ JSON íŒŒì¼ì„ ë¡œë“œí•˜ë„ë¡ ë³€ê²½)\n",
        "def load_json_file(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8-sig') as f:\n",
        "        data = json.load(f)\n",
        "    return data\n",
        "\n",
        "# SFT ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "data_SFT = load_json_file(data_path_1_SFT)\n",
        "print(f\"ğŸ“Š SFT ë°ì´í„° ê°œìˆ˜: {len(data_SFT)}\")\n",
        "print(\"ì²« 3ê°œ ìƒ˜í”Œ:\")\n",
        "print(data_SFT[:3])\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# RM ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "data_RM = load_json_file(data_path_2_RM)\n",
        "print(f\"ğŸ“Š RM ë°ì´í„° ê°œìˆ˜: {len(data_RM)}\")\n",
        "print(\"ì²« 3ê°œ ìƒ˜í”Œ:\")\n",
        "print(data_RM[:3])\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# PPO ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "data_PPO = load_json_file(data_path_3_PPO)\n",
        "print(f\"ğŸ“Š PPO ë°ì´í„° ê°œìˆ˜: {len(data_PPO)}\")\n",
        "print(\"ì²« 3ê°œ ìƒ˜í”Œ:\")\n",
        "print(data_PPO[:3])\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# 3ë‹¨ê³„: ë¹„êµí•˜ê¸°\n",
        "print(\"=\" * 50)\n",
        "print(\"ğŸ“‹ ë°ì´í„° ë¹„êµ ìš”ì•½\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"SFT ë°ì´í„°:  {len(data_SFT):,}ê°œ\")\n",
        "print(f\"RM ë°ì´í„°:   {len(data_RM):,}ê°œ\")\n",
        "print(f\"PPO ë°ì´í„°:  {len(data_PPO):,}ê°œ\")\n",
        "print(f\"ì „ì²´ í•©ê³„:   {len(data_SFT) + len(data_RM) + len(data_PPO):,}ê°œ\")"
      ],
      "metadata": {
        "id": "h0xqBTzE_K2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ë¶„ì„ ê²°ê³¼\n",
        "1) 3ê°€ì§€ ë°ì´í„°ì˜ ê°¯ìˆ˜ê°€ ë‹¤ë¥´ë‹¤\n",
        "    \n",
        "    -> ê°€ì„¤) í•™ìŠµì— ë¬¸ì œê°€ ì—†ì„ê¹Œ?\n",
        "    \n",
        "    -> ì²´í¬ê²°ê³¼) ë‹¨ê³„ë³„ ëª©ì ì´ ë‹¤ë¥´ë¯€ë¡œ í° ë¬¸ì œê°€ ì—†ë‹¤. RMì€ ì¢‹ì€ ë‹µë³€ì„ êµ¬ë³„í•˜ë¯€ë¡œ ìƒëŒ€ì ìœ¼ë¡œ ì ì€ ë°ì´í„°ë¡œ ì¶©ë¶„, ë°˜ë©´ì— SFT ë° PPO ë‹µë³€ì—ì„œëŠ” ìƒëŒ€ì ìœ¼ë¡œ ë§ì€ ìŒê³¼ ì‹œë‚˜ë¦¬ì˜¤ ë°˜ì˜ì´ í•„ìš”í•¨. ë˜í•œ RM formatì—ëŠ” ì—¬ëŸ¬ ìŒì˜ ë‹µë³€ì´ ë‚´ì¬í•´ ìˆë‹¤."
      ],
      "metadata": {
        "id": "snILvpxOADzs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SFT,RM, PPOì— ëŒ€í•œ ê³µë™ ì „ì²˜ë¦¬**"
      ],
      "metadata": {
        "id": "MCqzxqib0g04"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ì „ì²˜ë¦¬ì— ëŒ€í•œ ê³ ë¯¼ê³¼ ì‹¤í—˜ ê¸°ë¡\n",
        "1) í•­ìƒ ì „ì²˜ë¦¬ê°€ ì¤‘ìš”í•˜ë‹¤ëŠ” ê²ƒì„ ëŠë¼ê³ \n",
        "2) ê³µí†µì˜ ë°©ì‹ìœ¼ë¡œ í•œë²ˆì— ë¬¶ì–´ì„œ ì „ì²˜ë¦¬ë¥¼ í•˜ë ¤ê³  í•˜ì˜€ë‹¤. ì´ë²ˆ ê³¼ì œëŠ” ì„œë¡œ ë‹¤ë¥¸ ë°ì´í„° í˜•ì‹ì„ SFT, RM, PPOê°€ ê°€ì§€ê³  ìˆì–´ì„œ ì´ë¥¼ ë§ì¶”ëŠ”ë° ê½¤ ì˜¤ë˜ì‹œê°„ì„ ë‚­ë¹„í–ˆë‹¤.\n",
        "3) ìƒê°ë³´ë‹¤ ì—¬ëŸ¬ ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•  ë•ŒëŠ” ì¸ê³µì§€ëŠ¥ì´ í¬ë§· ë³€í™”ë¥¼ ìœ ë™ì ìœ¼ë¡œ ì ìš©ì‹œí‚¤ì§€ ëª»í•˜ê³  í—¤ë§¸ë‹¤. ë‹¨ìˆœí•œ í¬ë§·ë³€í™”ë¥¼ ì ìš©ì‹œí‚¤ëŠ” ì¼ì¸ë° ë°˜ë³µì ìœ¼ë¡œ ì‹œí–‰ì°©ì˜¤ë¥¼ ê³„ì†í•˜ëŠ” ê²ƒì„ ë³´ë©´ì„œ ì¸ê³µì§€ëŠ¥ì˜ ë¬¸ì œì ì„ í™•ì¸í–ˆë‹¤. ê·¸ë¦¬ê³  ë­ë„ê¹Œ ë°˜ë³µëœ ì‹¤ìˆ˜ë¥¼ ê³„ì† ì§€ì í–ˆìŒì—ë„ ê·¸ê±¸ ìê¾¸ ë°˜ë³µí•˜ëŠ” ê³ ì§‘ì´ë¼ê³  í•´ì•¼í• ê¹Œ? ì´ëŸ° íŠ¹ì„±ì´ ìˆì–´ì„œ í¥ë¯¸ë¡œì› ë‹¤.\n",
        "4) ì „ì²˜ë¦¬ì—ì„œëŠ” ì¼ë°˜ì ì¸ ê²°ì¸¡, ê³µë°±ì²˜ë¦¬, íŠ¹ìˆ˜ë¬¸ìì²˜ë¦¬ ë¿ë§Œ ì•„ë‹ˆë¼, ì´ë©”ì¼, URL ì²˜ë¦¬ ë“±ë„ í¬í•¨ì‹œì¼°ê³ , ì•„ìš¸ëŸ¬ì„œ í•œê¸€ ì±—ë´‡ ìë£Œì¸ë° ì™¸êµ­ì–´ ë¹„ìœ¨ë„ ê³ ë ¤í–ˆë‹¤. ì™¸êµ­ì–´ ìë£Œê°€ ë§ì€ ë°ì´í„°ì˜ ê²½ìš°ëŠ” í•™ìŠµìë£Œë¡œ ë¶€ì ì ˆí•˜ë‹¤ê³  íŒë‹¨í–ˆê¸° ë•Œë¬¸ì´ë‹¤.\n",
        "5) ì•„ìš¸ëŸ¬ì„œ, ì¤‘ê°„ ì‚°ì¶œë¬¼ì„ ê´€ë¦¬í•˜ê¸° ìœ„í•´ì„œ ë””ë ‰í† ë¦¬ë¥¼ ì§€ì •í•´ì„œ ìë£Œê°€ ì˜ ì €ì¥ë˜ë„ë¡ ë…¸ë ¸í•´ë´¤ë‹¤. í•˜ì§€ë§Œ íƒ€ì„ìŠ¤íƒœí•‘ê¹Œì§€ ì ìš©í–ˆë”ë‹ˆ, ë§¤ë²ˆ í™”ì¼ëª…ì— íƒ€ì„ì´ ì íˆê³  ì´ í™”ì¼ë“¤ì„ ì´í›„ ë¸”ëŸ­ì—ì„œ ë¶ˆëŸ¬ë“¤ì—¬ì•¼í•˜ëŠ”ë°, ì—¬ê¸°ì— ë‚œì ì´ ë°œìƒí–ˆë‹¤. ê·¸ë˜ì„œ ì ì ˆí•˜ê²Œ íƒ€ì„ìŠ¤íƒœí•‘ ê¸°ëŠ¥ì„ ì‚¬ìš©í•´ì•¼ê² ë‹¤ê³  ëŠê¼ˆë‹¤.\n",
        "6) ë””ë ‰í† ë¦¬ë¥¼ ì˜ëª» ì§€ì •í•˜ê±°ë‚˜, ë…¼ë¦¬ì  ìœ ê¸°ì„±ì´ ì—†ìœ¼ë©´ ë‚˜ì¤‘ì— ê´€ë ¨ í™”ì¼ì„ ì°¾ëŠ”ë°ë„ ìƒë‹¹í•œ ì–´ë ¤ì›€ì´ ë°œìƒí•´ì„œ ì–´ë–¤ ì‹ìœ¼ë¡œ ìë£Œë¥¼ ìƒì„±/ê´€ë¦¬í•  ì§€ ì˜ íŒë‹¨í•´ì•¼í•¨ì„ ëŠê¼ˆë‹¤.\n",
        "7) ê·¸ë¦¬ê³  ê¸°ë¡ì„ ì¶©ì‹¤íˆ ë‚¨ê²¨ì•¼ë§Œ ë°˜ë³µì ì¸ ì‹¤ìˆ˜ë¥¼ ë°©ì§€í•  ìˆ˜ ìˆìŒì„ ë°œê²¬í–ˆë‹¤.\n",
        "8) json, jsonl ìë£Œ ì²˜ë¦¬ì—ëŠ” ì „ì²´ë¥¼ í•œë²ˆì— ì½ì–´ë“¤ì–´ì•¼í•œë‹¤ëŠ” ì—ëŸ¬ê°€ ë§ì´ ë°œìƒí–ˆëŠ”ë°, ìì„¸íˆëŠ” ì•„ì§ íŒŒì•…í•˜ì§€ ëª»í–ˆë‹¤. ë‹¤ë§Œ, ë°ì´í„°ì²˜ë¦¬ ë°©ì‹ì´ csv, txtì™€ëŠ” ë‹¤ë¥´ë‹¤ëŠ” ê²ƒì„ ì•Œê²Œ ë˜ì—ˆë‹¤. ì´ì— ëŒ€í•´ì„œ ìŠ¤í„°ë””ê°€ í•„ìš”í•˜ë‹¤."
      ],
      "metadata": {
        "id": "yoXudCNPRJfH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# KoChatGPT ìµœì¢… ì „ì²˜ë¦¬ (ìµœì í™”ëœ ê¸°ì¤€)\n",
        "# í•œê¸€ 60% + ì˜ì–´ 30% + ê¸°íƒ€ì–¸ì–´ 25% + íŠ¹ìˆ˜ë¬¸ì 20%\n",
        "# ==========================================\n",
        "\n",
        "import re\n",
        "import json\n",
        "import os\n",
        "from datetime import datetime\n",
        "from collections import defaultdict\n",
        "\n",
        "print(\"ğŸ“š ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë”© ì¤‘...\")\n",
        "print(\"âœ“ ì¤€ë¹„ ì™„ë£Œ\\n\")\n",
        "\n",
        "# ==========================================\n",
        "# 1ï¸âƒ£ ì„¤ì •\n",
        "# ==========================================\n",
        "\n",
        "DATA_DIR = '/content/KoChatGPT/data_kochatgpt'\n",
        "OUTPUT_DIR = '/content/KoChatGPT_Pre_Results'\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# ë°ì´í„° íŒŒì¼ ê²½ë¡œ (ë‹¹ì‹ ì´ ì§€ì •í•œ ê²½ë¡œ)\n",
        "SFT_FILE = f'{DATA_DIR}/kochatgpt_1_SFT.jsonl'\n",
        "RM_FILE = f'{DATA_DIR}/kochatgpt_2_RM.jsonl'\n",
        "PPO_FILE = f'{DATA_DIR}/kochatgpt_3_PPO.jsonl'\n",
        "\n",
        "# ì •ì œ ê¸°ì¤€ (ìµœì í™”)\n",
        "CRITERIA = {\n",
        "    'min_korean_ratio': 0.6,      # í•œê¸€ ìµœì†Œ 60%\n",
        "    'max_english_ratio': 0.3,     # ì˜ì–´ ìµœëŒ€ 30%\n",
        "    'max_other_lang_ratio': 0.25, # ê¸°íƒ€ì–¸ì–´ ìµœëŒ€ 25%\n",
        "    'max_special_ratio': 0.2,     # íŠ¹ìˆ˜ë¬¸ì ìµœëŒ€ 20%\n",
        "    'min_length': 5,              # ìµœì†Œ 5ì\n",
        "}\n",
        "\n",
        "# ==========================================\n",
        "# 2ï¸âƒ£ í…ìŠ¤íŠ¸ ì •ì œ í•¨ìˆ˜\n",
        "# ==========================================\n",
        "\n",
        "def normalize_text(text):\n",
        "    \"\"\"í…ìŠ¤íŠ¸ ì •ê·œí™”\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    text = text.strip()\n",
        "    text = re.sub(r'\\n\\n+', '\\n', text)  # ì—¬ëŸ¬ ì¤„ë°”ê¿ˆ â†’ ë‹¨ì¼\n",
        "    text = re.sub(r' +', ' ', text)      # ì—¬ëŸ¬ ê³µë°± â†’ ë‹¨ì¼\n",
        "    text = text.replace('\"', '\"').replace('\"', '\"')  # íŠ¹ìˆ˜ ë”°ì˜´í‘œ\n",
        "\n",
        "    return text\n",
        "\n",
        "def remove_garbage(text):\n",
        "    \"\"\"HTML, URL, ì´ë©”ì¼ ì œê±°\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    text = re.sub(r'<[^>]+>', '', text)        # HTML\n",
        "    text = re.sub(r'http[s]?://\\S+', '', text) # URL\n",
        "    text = re.sub(r'\\S+@\\S+', '', text)        # ì´ë©”ì¼\n",
        "\n",
        "    return text\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"ì¢…í•© ì •ì œ\"\"\"\n",
        "    text = normalize_text(text)\n",
        "    text = remove_garbage(text)\n",
        "    return text\n",
        "\n",
        "def analyze_text_composition(text):\n",
        "    \"\"\"í…ìŠ¤íŠ¸ êµ¬ì„± ë¶„ì„\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return {}\n",
        "\n",
        "    korean_count = len(re.findall(r'[ê°€-í£]', text))\n",
        "    english_count = len(re.findall(r'[a-zA-Z]', text))\n",
        "    digit_count = len(re.findall(r'[0-9]', text))\n",
        "\n",
        "    # ê¸°íƒ€ ì–¸ì–´ ì¶”ì¶œ\n",
        "    other_chars = re.findall(r'[^ê°€-í£a-zA-Z0-9\\s]', text)\n",
        "    other_lang_count = len(re.findall(\n",
        "        r'[\\u4E00-\\u9FFF\\u3040-\\u309F\\u30A0-\\u30FF\\u0400-\\u04FF]',\n",
        "        ''.join(other_chars)\n",
        "    ))\n",
        "    special_count = len(other_chars) - other_lang_count\n",
        "\n",
        "    total_len = len(text)\n",
        "\n",
        "    return {\n",
        "        'korean_ratio': korean_count / total_len if total_len > 0 else 0,\n",
        "        'english_ratio': english_count / total_len if total_len > 0 else 0,\n",
        "        'other_lang_ratio': other_lang_count / total_len if total_len > 0 else 0,\n",
        "        'special_ratio': special_count / total_len if total_len > 0 else 0,\n",
        "    }\n",
        "\n",
        "def validate_text(text):\n",
        "    \"\"\"í…ìŠ¤íŠ¸ ìœ íš¨ì„± ê²€ì¦\"\"\"\n",
        "    if not isinstance(text, str) or len(text.strip()) < CRITERIA['min_length']:\n",
        "        return False, \"too_short\"\n",
        "\n",
        "    comp = analyze_text_composition(text)\n",
        "\n",
        "    if comp['korean_ratio'] < CRITERIA['min_korean_ratio']:\n",
        "        return False, \"korean_low\"\n",
        "    if comp['english_ratio'] > CRITERIA['max_english_ratio']:\n",
        "        return False, \"english_high\"\n",
        "    if comp['other_lang_ratio'] > CRITERIA['max_other_lang_ratio']:\n",
        "        return False, \"other_lang_high\"\n",
        "    if comp['special_ratio'] > CRITERIA['max_special_ratio']:\n",
        "        return False, \"special_high\"\n",
        "\n",
        "    return True, None\n",
        "\n",
        "def detect_duplicate(text1, text2):\n",
        "    \"\"\"ì™„ì „ ë™ì¼ ì¤‘ë³µ ê²€ì‚¬\"\"\"\n",
        "    return text1.strip() == text2.strip()\n",
        "\n",
        "# ==========================================\n",
        "# 3ï¸âƒ£ ë°ì´í„° ë¡œë“œ í•¨ìˆ˜\n",
        "# ==========================================\n",
        "\n",
        "def load_json_file(file_path):\n",
        "    \"\"\"JSON/JSONL íŒŒì¼ ë¡œë“œ\"\"\"\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        try:\n",
        "            data = json.load(f)\n",
        "            return data if isinstance(data, list) else [data]\n",
        "        except:\n",
        "            f.seek(0)\n",
        "            return [json.loads(line.strip()) for line in f if line.strip()]\n",
        "\n",
        "# ==========================================\n",
        "# 4ï¸âƒ£ ì „ì²˜ë¦¬ í•¨ìˆ˜\n",
        "# ==========================================\n",
        "\n",
        "def preprocess_sft(data):\n",
        "    \"\"\"SFT ë°ì´í„° ì „ì²˜ë¦¬\"\"\"\n",
        "    processed = []\n",
        "    stats = {'kept': 0, 'removed': 0, 'reasons': defaultdict(int)}\n",
        "\n",
        "    for idx, item in enumerate(data):\n",
        "        if (idx + 1) % 5000 == 0:\n",
        "            print(f\"  ì§„í–‰: {idx+1:,}/{len(data):,}\")\n",
        "\n",
        "        try:\n",
        "            prompt = clean_text(item.get('prompt', ''))\n",
        "            completion = clean_text(item.get('completion', ''))\n",
        "            tokens = item.get('tokens')  # ì›ë³¸ ìœ ì§€\n",
        "\n",
        "            # í•„ìˆ˜ í•„ë“œ í™•ì¸\n",
        "            if not all([prompt, completion]):\n",
        "                stats['reasons']['empty'] += 1\n",
        "                stats['removed'] += 1\n",
        "                continue\n",
        "\n",
        "            # ìœ íš¨ì„± ê²€ì¦\n",
        "            valid_p, rp = validate_text(prompt)\n",
        "            if not valid_p:\n",
        "                stats['reasons'][rp] += 1\n",
        "                stats['removed'] += 1\n",
        "                continue\n",
        "\n",
        "            valid_c, rc = validate_text(completion)\n",
        "            if not valid_c:\n",
        "                stats['reasons'][rc] += 1\n",
        "                stats['removed'] += 1\n",
        "                continue\n",
        "\n",
        "            # ë°ì´í„° ì €ì¥ (ì›ë³¸ í˜•íƒœ ìœ ì§€)\n",
        "            processed.append({\n",
        "                'prompt': prompt,\n",
        "                'completion': completion,\n",
        "                'tokens': tokens\n",
        "            })\n",
        "            stats['kept'] += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            stats['reasons']['error'] += 1\n",
        "            stats['removed'] += 1\n",
        "            continue\n",
        "\n",
        "    return processed, stats\n",
        "\n",
        "def preprocess_rm(data):\n",
        "    \"\"\"RM ë°ì´í„° ì „ì²˜ë¦¬\"\"\"\n",
        "    processed = []\n",
        "    stats = {'kept': 0, 'removed': 0, 'reasons': defaultdict(int)}\n",
        "\n",
        "    for idx, item in enumerate(data):\n",
        "        if (idx + 1) % 5000 == 0:\n",
        "            print(f\"  ì§„í–‰: {idx+1:,}/{len(data):,}\")\n",
        "\n",
        "        try:\n",
        "            prompt = clean_text(item.get('prompt', ''))\n",
        "            c0 = clean_text(item.get('completion_0', ''))\n",
        "            c1 = clean_text(item.get('completion_1', ''))\n",
        "            c2 = clean_text(item.get('completion_2', ''))\n",
        "            ranking = item.get('ranking', [])\n",
        "\n",
        "            # í•„ìˆ˜ í•„ë“œ í™•ì¸\n",
        "            if not all([prompt, c0, c1, c2, ranking]):\n",
        "                stats['reasons']['empty'] += 1\n",
        "                stats['removed'] += 1\n",
        "                continue\n",
        "\n",
        "            # ê° í•„ë“œ ìœ íš¨ì„± ê²€ì¦\n",
        "            valid_p, rp = validate_text(prompt)\n",
        "            if not valid_p:\n",
        "                stats['reasons'][rp] += 1\n",
        "                stats['removed'] += 1\n",
        "                continue\n",
        "\n",
        "            valid_c0, rc0 = validate_text(c0)\n",
        "            if not valid_c0:\n",
        "                stats['reasons'][rc0] += 1\n",
        "                stats['removed'] += 1\n",
        "                continue\n",
        "\n",
        "            valid_c1, rc1 = validate_text(c1)\n",
        "            if not valid_c1:\n",
        "                stats['reasons'][rc1] += 1\n",
        "                stats['removed'] += 1\n",
        "                continue\n",
        "\n",
        "            valid_c2, rc2 = validate_text(c2)\n",
        "            if not valid_c2:\n",
        "                stats['reasons'][rc2] += 1\n",
        "                stats['removed'] += 1\n",
        "                continue\n",
        "\n",
        "            # ì¤‘ë³µ ê²€ì‚¬\n",
        "            completions = [c0, c1, c2]\n",
        "            if any(detect_duplicate(c1, c2) for i, c1 in enumerate(completions)\n",
        "                   for c2 in completions[i+1:]):\n",
        "                stats['reasons']['duplicate'] += 1\n",
        "                stats['removed'] += 1\n",
        "                continue\n",
        "\n",
        "            # ë°ì´í„° ì €ì¥ (ì›ë³¸ í˜•íƒœ ìœ ì§€)\n",
        "            processed.append({\n",
        "                'prompt': prompt,\n",
        "                'completion_0': c0,\n",
        "                'completion_1': c1,\n",
        "                'completion_2': c2,\n",
        "                'ranking': ranking\n",
        "            })\n",
        "            stats['kept'] += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            stats['reasons']['error'] += 1\n",
        "            stats['removed'] += 1\n",
        "            continue\n",
        "\n",
        "    return processed, stats\n",
        "\n",
        "def preprocess_ppo(data):\n",
        "    \"\"\"PPO ë°ì´í„° ì „ì²˜ë¦¬\"\"\"\n",
        "    processed = []\n",
        "    stats = {'kept': 0, 'removed': 0, 'reasons': defaultdict(int)}\n",
        "\n",
        "    for idx, item in enumerate(data):\n",
        "        if (idx + 1) % 5000 == 0:\n",
        "            print(f\"  ì§„í–‰: {idx+1:,}/{len(data):,}\")\n",
        "\n",
        "        try:\n",
        "            prompt = clean_text(item.get('prompt', ''))\n",
        "\n",
        "            # í•„ìˆ˜ í•„ë“œ í™•ì¸\n",
        "            if not prompt:\n",
        "                stats['reasons']['empty'] += 1\n",
        "                stats['removed'] += 1\n",
        "                continue\n",
        "\n",
        "            # ìœ íš¨ì„± ê²€ì¦\n",
        "            valid_p, rp = validate_text(prompt)\n",
        "            if not valid_p:\n",
        "                stats['reasons'][rp] += 1\n",
        "                stats['removed'] += 1\n",
        "                continue\n",
        "\n",
        "            # ë°ì´í„° ì €ì¥\n",
        "            processed.append({'prompt': prompt})\n",
        "            stats['kept'] += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            stats['reasons']['error'] += 1\n",
        "            stats['removed'] += 1\n",
        "            continue\n",
        "\n",
        "    return processed, stats\n",
        "\n",
        "# ==========================================\n",
        "# 5ï¸âƒ£ ë©”ì¸ ì‹¤í–‰\n",
        "# ==========================================\n",
        "\n",
        "print(\"ğŸ“Š ì‹¤ì œ ë°ì´í„° ë¡œë“œ ì¤‘...\\n\")\n",
        "\n",
        "data_SFT = load_json_file(SFT_FILE)\n",
        "data_RM = load_json_file(RM_FILE)\n",
        "data_PPO = load_json_file(PPO_FILE)\n",
        "\n",
        "print(f\"âœ“ SFT: {len(data_SFT):,}ê°œ\")\n",
        "print(f\"âœ“ RM: {len(data_RM):,}ê°œ\")\n",
        "print(f\"âœ“ PPO: {len(data_PPO):,}ê°œ\\n\")\n",
        "\n",
        "# SFT ì „ì²˜ë¦¬\n",
        "print(\"ğŸ¯ SFT ì •ì œ ì¤‘...\\n\")\n",
        "processed_sft, stats_sft = preprocess_sft(data_SFT)\n",
        "print(f\"âœ“ SFT: {stats_sft['kept']:,}/{len(data_SFT):,} ({stats_sft['kept']/len(data_SFT)*100:.1f}%)\\n\")\n",
        "\n",
        "# RM ì „ì²˜ë¦¬\n",
        "print(\"ğŸ¯ RM ì •ì œ ì¤‘...\\n\")\n",
        "processed_rm, stats_rm = preprocess_rm(data_RM)\n",
        "print(f\"âœ“ RM: {stats_rm['kept']:,}/{len(data_RM):,} ({stats_rm['kept']/len(data_RM)*100:.1f}%)\\n\")\n",
        "\n",
        "# PPO ì „ì²˜ë¦¬\n",
        "print(\"ğŸ¯ PPO ì •ì œ ì¤‘...\\n\")\n",
        "processed_ppo, stats_ppo = preprocess_ppo(data_PPO)\n",
        "print(f\"âœ“ PPO: {stats_ppo['kept']:,}/{len(data_PPO):,} ({stats_ppo['kept']/len(data_PPO)*100:.1f}%)\\n\")\n",
        "\n",
        "# ==========================================\n",
        "# 6ï¸âƒ£ ê²°ê³¼ ì €ì¥\n",
        "# ==========================================\n",
        "\n",
        "print(\"ğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘...\\n\")\n",
        "\n",
        "sft_path = f'{OUTPUT_DIR}/sft_processed.json'\n",
        "rm_path = f'{OUTPUT_DIR}/rm_processed.json'\n",
        "ppo_path = f'{OUTPUT_DIR}/ppo_processed.json'\n",
        "\n",
        "with open(sft_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(processed_sft, f, ensure_ascii=False, indent=2)\n",
        "print(f\"âœ… SFT: {sft_path}\")\n",
        "\n",
        "with open(rm_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(processed_rm, f, ensure_ascii=False, indent=2)\n",
        "print(f\"âœ… RM: {rm_path}\")\n",
        "\n",
        "with open(ppo_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(processed_ppo, f, ensure_ascii=False, indent=2)\n",
        "print(f\"âœ… PPO: {ppo_path}\")\n",
        "\n",
        "# ==========================================\n",
        "# 7ï¸âƒ£ ë¦¬í¬íŠ¸ ìƒì„±\n",
        "# ==========================================\n",
        "\n",
        "report_path = f'{OUTPUT_DIR}/preprocessing_report.txt'\n",
        "\n",
        "with open(report_path, 'w', encoding='utf-8') as f:\n",
        "    f.write(\"=\"*70 + \"\\n\")\n",
        "    f.write(\"ğŸ¯ KoChatGPT ìµœì¢… ì „ì²˜ë¦¬ ë¦¬í¬íŠ¸\\n\")\n",
        "    f.write(\"=\"*70 + \"\\n\\n\")\n",
        "\n",
        "    f.write(f\"ì²˜ë¦¬ ì‹œê°„: \\n\\n\")\n",
        "\n",
        "    f.write(\"ì •ì œ ê·œì¹™:\\n\")\n",
        "    f.write(\"âœ“ í…ìŠ¤íŠ¸ ì •ê·œí™” (ê³µë°±, ì¤„ë°”ê¿ˆ)\\n\")\n",
        "    f.write(\"âœ“ HTML, URL, ì´ë©”ì¼ ì œê±°\\n\")\n",
        "    f.write(\"âœ“ ì¸ì½”ë”© ê²€ì¦\\n\")\n",
        "    f.write(\"âœ“ í•œê¸€ ë¹„ìœ¨ ê²€ì¦ (ìµœì†Œ 60% - í•œê¸€ ê¸°ë°˜)\\n\")\n",
        "    f.write(\"âœ“ ì˜ì–´ ë¹„ìœ¨ ê²€ì¦ (ìµœëŒ€ 30% - ê³ ìœ ëª…ì‚¬/ê¸°ìˆ ìš©ì–´)\\n\")\n",
        "    f.write(\"âœ“ ê¸°íƒ€ ì–¸ì–´ ë¹„ìœ¨ ê²€ì¦ (ìµœëŒ€ 25%)\\n\")\n",
        "    f.write(\"âœ“ íŠ¹ìˆ˜ë¬¸ì ë¹„ìœ¨ ê²€ì¦ (ìµœëŒ€ 20%)\\n\")\n",
        "    f.write(\"âœ“ ê¸¸ì´ ê²€ì¦ (ìµœì†Œ 5ì)\\n\")\n",
        "    f.write(\"âœ“ ì¤‘ë³µ ê²€ì‚¬ (ë™ì¼ ë‚´ìš© ì œê±°)\\n\\n\")\n",
        "\n",
        "    f.write(\"=\"*70 + \"\\n\")\n",
        "    f.write(\"ğŸ“Š ìµœì¢… ê²°ê³¼\\n\")\n",
        "    f.write(\"=\"*70 + \"\\n\\n\")\n",
        "\n",
        "    f.write(\"ğŸ“ˆ SFT (Supervised Fine-Tuning)\\n\")\n",
        "    f.write(f\"  ì›ë³¸: {len(data_SFT):,}ê°œ\\n\")\n",
        "    f.write(f\"  ìœ ì§€: {stats_sft['kept']:,}ê°œ\\n\")\n",
        "    f.write(f\"  ì œê±°: {stats_sft['removed']:,}ê°œ\\n\")\n",
        "    f.write(f\"  ìœ ì§€ìœ¨: {stats_sft['kept']/len(data_SFT)*100:.1f}%\\n\\n\")\n",
        "\n",
        "    f.write(\"ğŸ“Š RM (Reward Model)\\n\")\n",
        "    f.write(f\"  ì›ë³¸: {len(data_RM):,}ê°œ\\n\")\n",
        "    f.write(f\"  ìœ ì§€: {stats_rm['kept']:,}ê°œ\\n\")\n",
        "    f.write(f\"  ì œê±°: {stats_rm['removed']:,}ê°œ\\n\")\n",
        "    f.write(f\"  ìœ ì§€ìœ¨: {stats_rm['kept']/len(data_RM)*100:.1f}%\\n\")\n",
        "    f.write(f\"  í•™ìŠµ ìŒ: {stats_rm['kept']*3:,}ê°œ (ê° ìƒ˜í”Œë‹¹ 3ê°œ ìŒ ìƒì„±)\\n\\n\")\n",
        "\n",
        "    f.write(\"ğŸ”„ PPO (ê°•í™”í•™ìŠµ)\\n\")\n",
        "    f.write(f\"  ì›ë³¸: {len(data_PPO):,}ê°œ\\n\")\n",
        "    f.write(f\"  ìœ ì§€: {stats_ppo['kept']:,}ê°œ\\n\")\n",
        "    f.write(f\"  ì œê±°: {stats_ppo['removed']:,}ê°œ\\n\")\n",
        "    f.write(f\"  ìœ ì§€ìœ¨: {stats_ppo['kept']/len(data_PPO)*100:.1f}%\\n\\n\")\n",
        "\n",
        "    total_original = len(data_SFT) + len(data_RM)*3 + len(data_PPO)\n",
        "    total_kept = stats_sft['kept'] + stats_rm['kept']*3 + stats_ppo['kept']\n",
        "\n",
        "    f.write(\"=\"*70 + \"\\n\")\n",
        "    f.write(\"ğŸ’¡ ì „ì²´ í†µê³„\\n\")\n",
        "    f.write(\"=\"*70 + \"\\n\\n\")\n",
        "    f.write(f\"ì´ í•™ìŠµ ë°ì´í„°: {total_kept:,}ê°œ (ì›ë³¸ {total_original:,}ê°œ ì¤‘ {total_kept/total_original*100:.1f}%)\\n\\n\")\n",
        "\n",
        "    f.write(\"SFT ì œê±° ì´ìœ :\\n\")\n",
        "    for reason, count in sorted(stats_sft['reasons'].items(), key=lambda x: x[1], reverse=True):\n",
        "        f.write(f\"  - {reason}: {count:,}ê°œ\\n\")\n",
        "\n",
        "    f.write(\"\\nRM ì œê±° ì´ìœ :\\n\")\n",
        "    for reason, count in sorted(stats_rm['reasons'].items(), key=lambda x: x[1], reverse=True):\n",
        "        f.write(f\"  - {reason}: {count:,}ê°œ\\n\")\n",
        "\n",
        "    f.write(\"\\nPPO ì œê±° ì´ìœ :\\n\")\n",
        "    for reason, count in sorted(stats_ppo['reasons'].items(), key=lambda x: x[1], reverse=True):\n",
        "        f.write(f\"  - {reason}: {count:,}ê°œ\\n\")\n",
        "\n",
        "print(f\"âœ… ë¦¬í¬íŠ¸: {report_path}\\n\")\n",
        "\n",
        "# ==========================================\n",
        "# 8ï¸âƒ£ ìµœì¢… ì¶œë ¥\n",
        "# ==========================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"âœ¨ ì •ì œ ì™„ë£Œ!\")\n",
        "print(\"=\"*70)\n",
        "print(f\"âœ… ë¦¬í¬íŠ¸: {report_path}\\n\")\n",
        "\n",
        "print(\"ğŸ“Š ìµœì¢… ê²°ê³¼:\")\n",
        "print(f\"   SFT: {stats_sft['kept']:,}/{len(data_SFT):,} ({stats_sft['kept']/len(data_SFT)*100:.1f}%)\")\n",
        "print(f\"   RM:  {stats_rm['kept']:,}/{len(data_RM):,} ({stats_rm['kept']/len(data_RM)*100:.1f}%)\")\n",
        "print(f\"   PPO: {stats_ppo['kept']:,}/{len(data_PPO):,} ({stats_ppo['kept']/len(data_PPO)*100:.1f}%)\\n\")\n",
        "\n",
        "print(\"ğŸ’¡ í•™ìŠµ ë°ì´í„°ëŸ‰:\")\n",
        "print(f\"   SFT: {stats_sft['kept']:,}ê°œ ìƒ˜í”Œ\")\n",
        "print(f\"   RM:  {stats_rm['kept']:,}ê°œ ìƒ˜í”Œ â†’ {stats_rm['kept']*3:,}ê°œ ìŒ\")\n",
        "print(f\"   PPO: {stats_ppo['kept']:,}ê°œ í”„ë¡¬í”„íŠ¸\\n\")\n",
        "\n",
        "print(\"âœ“ ì „ì²˜ë¦¬ ì™„ë£Œ! ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰í•˜ì„¸ìš”. ğŸš€\")"
      ],
      "metadata": {
        "id": "Aw_ejzymMmlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ê´€ì°° ê²°ê³¼: RM ë°ì´í„°ê°€ ê¸‰ê²©í•˜ê²Œ ì¤„ì–´ë“¤ì–´ì„œ ë§ì´ ê³ ë¯¼í–ˆë‹¤. ê°€ì¥ í° ì›ì¸ì€ 3ê°€ì§€ ë‹µë³€ ìŒì´ í•˜ë‚˜ë¼ë„ ì „ì²˜ë¦¬ í”„ë¡œí† ì½œì— ìœ„ë°°ë˜ë©´ ì œê±°ëœ ê²ƒì´ì—ˆëŠ”ë°, ê³µí†µì˜ í”„ë¡œí† ì½œì„ ì ìš©í•˜ë‹¤ë³´ë‹ˆ ì–´ì©” ìˆ˜ ì—†ì—ˆë˜ ìƒí™©ì´ë‹¤. ì–‘ì§ˆì˜ í•™ìŠµ ë°ì´í„°ë¥¼ í™•ë³´í•˜ê¸° ìœ„í•´ì„œ ì–´ì©” ìˆ˜ ì—†ë¨ë‹¤. ë‚˜ì¤‘ì— ì¡°ì‚¬í•´ë³´ë‹ˆ, RMì•ˆì—ëŠ” ì„œë¡œ ë‹¤ë¥¸ ë‹µë³€ ì„¸íŠ¸ê°€ ë¬¶ì—¬ìˆê¸° ë•Œë¬¸ì— ì„¸íŠ¸ ìˆ«ìëŠ” ì ì–´ë„ ì‹¤ì œ í•™ìŠµ ìˆ«ìëŠ” ì ì§€ ì•Šë‹¤ê³  íŒë‹¨í•˜ê²Œ ë˜ì—ˆë‹¤.  "
      ],
      "metadata": {
        "id": "C3wxun1_de5l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **í† í¬ë‚˜ì´ì €**"
      ],
      "metadata": {
        "id": "9RSoCzW37U_G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional, Dict, Sequence\n",
        "from torch.utils.data import Dataset\n",
        "from dataclasses import dataclass\n",
        "import logging\n",
        "import copy"
      ],
      "metadata": {
        "id": "O8qv31FUac2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# KoChatGPT ë°ì´í„° í† í¬ë‚˜ì´ì§• (ê°„ê²°í™”)\n",
        "# ==========================================\n",
        "\n",
        "import json\n",
        "import os\n",
        "from datetime import datetime\n",
        "from transformers import AutoTokenizer\n",
        "import gc\n",
        "\n",
        "print(\"ğŸ“š ì´ˆê¸°í™” ì¤‘...\")\n",
        "\n",
        "# ì„¤ì •\n",
        "output_dir = '/content/KoChatGPT_Token_Results'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "pre_dir = '/content/KoChatGPT_Pre_Results'\n",
        "\n",
        "# í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    'skt/kogpt2-base-v2',\n",
        "    bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
        "    padding_side=\"right\", model_max_length=512,\n",
        ")\n",
        "print(\"âœ“ ì¤€ë¹„ ì™„ë£Œ\\n\")\n",
        "\n",
        "# ==========================================\n",
        "# ê³µí†µ í•¨ìˆ˜\n",
        "# ==========================================\n",
        "\n",
        "def load_data(file_path):\n",
        "    \"\"\"ë°ì´í„° ë¡œë“œ\"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            return json.load(f)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"âš ï¸ íŒŒì¼ ì—†ìŒ: {file_path}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ ë¡œë“œ ì˜¤ë¥˜: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def tokenize_text(text):\n",
        "    \"\"\"ë‹¨ì¼ í…ìŠ¤íŠ¸ í† í°í™”\"\"\"\n",
        "    encoded = tokenizer(\n",
        "        text, truncation=True, max_length=512,\n",
        "        padding='max_length', return_tensors='pt'\n",
        "    )\n",
        "    return {\n",
        "        'input_ids': encoded['input_ids'].squeeze().tolist(),\n",
        "        'attention_mask': encoded['attention_mask'].squeeze().tolist(),\n",
        "    }\n",
        "\n",
        "def get_token_length(input_ids):\n",
        "    \"\"\"ì‹¤ì œ í† í° ê¸¸ì´ ê³„ì‚° (íŒ¨ë”© ì œì™¸)\"\"\"\n",
        "    return len([x for x in input_ids if x != tokenizer.pad_token_id])\n",
        "\n",
        "def process_dataset(data, dataset_name, processor_func):\n",
        "    \"\"\"ë°ì´í„°ì…‹ ì²˜ë¦¬ (ê³µí†µ)\"\"\"\n",
        "    if not data:\n",
        "        print(f\"âš ï¸ {dataset_name}: ë°ì´í„° ì—†ìŒ\")\n",
        "        return []\n",
        "\n",
        "    tokenized = []\n",
        "    print(f\"ğŸ”„ [{dataset_name}] í† í¬ë‚˜ì´ì§• ì¤‘...\")\n",
        "\n",
        "    for idx, item in enumerate(data):\n",
        "        try:\n",
        "            result = processor_func(item)\n",
        "            if result:\n",
        "                tokenized.append(result)\n",
        "\n",
        "            if (idx + 1) % 5000 == 0:\n",
        "                print(f\"   {idx + 1:,}/{len(data):,}\")\n",
        "        except Exception as e:\n",
        "            if idx < 3:  # ì²˜ìŒ 3ê°œë§Œ ì¶œë ¥\n",
        "                print(f\"   ì˜¤ë¥˜ (ìƒ˜í”Œ {idx}): {str(e)[:40]}\")\n",
        "\n",
        "    print(f\"âœ“ {dataset_name}: {len(tokenized):,}/{len(data):,}\\n\")\n",
        "    return tokenized\n",
        "\n",
        "def save_data(data, name):\n",
        "    \"\"\"ë°ì´í„° ì €ì¥\"\"\"\n",
        "    path = f'{output_dir}/{name}_tokenized.json'\n",
        "    with open(path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(data, f, ensure_ascii=False)\n",
        "    return path\n",
        "\n",
        "# ==========================================\n",
        "# ë°ì´í„°ì…‹ë³„ ì²˜ë¦¬ í•¨ìˆ˜\n",
        "# ==========================================\n",
        "\n",
        "def process_sft(item):\n",
        "    \"\"\"SFT ì²˜ë¦¬\"\"\"\n",
        "    instruction = item.get('instruction', '')\n",
        "    output = item.get('output', '')\n",
        "    text = f\"{instruction}\\n{output}\"\n",
        "\n",
        "    encoded = tokenize_text(text)\n",
        "    encoded['instruction'] = instruction\n",
        "    encoded['output'] = output\n",
        "    encoded['token_length'] = get_token_length(encoded['input_ids'])\n",
        "    return encoded\n",
        "\n",
        "def process_rm(item):\n",
        "    \"\"\"RM ì²˜ë¦¬\"\"\"\n",
        "    prompt = item.get('prompt', '')\n",
        "    chosen = item.get('chosen', '')\n",
        "    rejected = item.get('rejected', '')\n",
        "\n",
        "    prompt_enc = tokenize_text(prompt)\n",
        "    chosen_enc = tokenize_text(chosen)\n",
        "    rejected_enc = tokenize_text(rejected)\n",
        "\n",
        "    return {\n",
        "        'prompt': prompt,\n",
        "        'chosen': chosen,\n",
        "        'rejected': rejected,\n",
        "        'prompt_input_ids': prompt_enc['input_ids'],\n",
        "        'prompt_attention_mask': prompt_enc['attention_mask'],\n",
        "        'chosen_input_ids': chosen_enc['input_ids'],\n",
        "        'chosen_attention_mask': chosen_enc['attention_mask'],\n",
        "        'rejected_input_ids': rejected_enc['input_ids'],\n",
        "        'rejected_attention_mask': rejected_enc['attention_mask'],\n",
        "    }\n",
        "\n",
        "def process_ppo(item):\n",
        "    \"\"\"PPO ì²˜ë¦¬\"\"\"\n",
        "    prompt = item.get('prompt', '')\n",
        "    completion = item.get('completion', '')\n",
        "    text = f\"{prompt}\\n{completion}\"\n",
        "\n",
        "    encoded = tokenize_text(text)\n",
        "    encoded['prompt'] = prompt\n",
        "    encoded['completion'] = completion\n",
        "    encoded['token_length'] = get_token_length(encoded['input_ids'])\n",
        "    return encoded\n",
        "\n",
        "# ==========================================\n",
        "# í† í¬ë‚˜ì´ì§• ì‹¤í–‰\n",
        "# ==========================================\n",
        "\n",
        "print(\"ğŸ“Š ë°ì´í„° í† í¬ë‚˜ì´ì§• ì‹œì‘\\n\")\n",
        "\n",
        "# SFT\n",
        "sft_data = load_data(f'{pre_dir}/sft_processed.json')\n",
        "tokenized_sft = process_dataset(sft_data, 'SFT', process_sft)\n",
        "\n",
        "# RM\n",
        "rm_data = load_data(f'{pre_dir}/rm_processed.json')\n",
        "tokenized_rm = process_dataset(rm_data, 'RM', process_rm)\n",
        "\n",
        "# PPO\n",
        "ppo_data = load_data(f'{pre_dir}/ppo_processed.json')\n",
        "tokenized_ppo = process_dataset(ppo_data, 'PPO', process_ppo)\n",
        "\n",
        "# ==========================================\n",
        "# ê²°ê³¼ ì €ì¥\n",
        "# ==========================================\n",
        "\n",
        "print(\"ğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘...\\n\")\n",
        "\n",
        "paths = {}\n",
        "if tokenized_sft:\n",
        "    paths['sft'] = save_data(tokenized_sft, 'sft')\n",
        "    print(f\"âœ… {paths['sft']}\")\n",
        "\n",
        "if tokenized_rm:\n",
        "    paths['rm'] = save_data(tokenized_rm, 'rm')\n",
        "    print(f\"âœ… {paths['rm']}\")\n",
        "\n",
        "if tokenized_ppo:\n",
        "    paths['ppo'] = save_data(tokenized_ppo, 'ppo')\n",
        "    print(f\"âœ… {paths['ppo']}\")\n",
        "\n",
        "# ==========================================\n",
        "# ë¦¬í¬íŠ¸ ìƒì„±\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\nğŸ“ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...\")\n",
        "\n",
        "report_path = f'{output_dir}/tokenization_report.txt'\n",
        "with open(report_path, 'w', encoding='utf-8') as f:\n",
        "    f.write(\"=\"*70 + \"\\n\")\n",
        "    f.write(\"ğŸ“Š í† í¬ë‚˜ì´ì§• ë¦¬í¬íŠ¸\\n\")\n",
        "    f.write(f\"ìƒì„±: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "    f.write(f\"ëª¨ë¸: skt/kogpt2-base-v2 (Max Length: 512)\\n\")\n",
        "    f.write(\"=\"*70 + \"\\n\\n\")\n",
        "\n",
        "    if tokenized_sft:\n",
        "        avg_len = sum([x['token_length'] for x in tokenized_sft]) / len(tokenized_sft)\n",
        "        f.write(f\"[SFT] {len(tokenized_sft):,}ê°œ í† í°í™” (í‰ê· : {avg_len:.1f})\\n\")\n",
        "\n",
        "    if tokenized_rm:\n",
        "        f.write(f\"[RM] {len(tokenized_rm):,}ê°œ í† í°í™” (prompt/chosen/rejected)\\n\")\n",
        "\n",
        "    if tokenized_ppo:\n",
        "        avg_len = sum([x['token_length'] for x in tokenized_ppo]) / len(tokenized_ppo)\n",
        "        f.write(f\"[PPO] {len(tokenized_ppo):,}ê°œ í† í°í™” (í‰ê· : {avg_len:.1f})\\n\")\n",
        "\n",
        "print(f\"âœ… {report_path}\\n\")\n",
        "\n",
        "# ==========================================\n",
        "# ìµœì¢… ìš”ì•½\n",
        "# ==========================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"âœ¨ í† í¬ë‚˜ì´ì§• ì™„ë£Œ!\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "if tokenized_sft:\n",
        "    print(f\"âœ“ SFT: {len(tokenized_sft):,}ê°œ\")\n",
        "if tokenized_rm:\n",
        "    print(f\"âœ“ RM: {len(tokenized_rm):,}ê°œ\")\n",
        "if tokenized_ppo:\n",
        "    print(f\"âœ“ PPO: {len(tokenized_ppo):,}ê°œ\")\n",
        "\n",
        "print(f\"\\nğŸ“ ì €ì¥: {output_dir}\")\n",
        "\n",
        "# ë©”ëª¨ë¦¬ ì •ë¦¬\n",
        "del sft_data, rm_data, ppo_data\n",
        "del tokenized_sft, tokenized_rm, tokenized_ppo\n",
        "gc.collect()\n",
        "\n",
        "print(\"\\nâœ“ ì™„ë£Œ!\")"
      ],
      "metadata": {
        "id": "VFnt_ZLD12fO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SFT**"
      ],
      "metadata": {
        "id": "5plvV9AWOiVS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import transformers\n",
        "from typing import Optional, Dict, Sequence\n",
        "from torch.utils.data import Dataset\n",
        "from dataclasses import dataclass\n",
        "import logging\n",
        "import copy\n",
        "import json\n",
        "\n",
        "class SFT_dataset(Dataset):\n",
        "\n",
        "    def __init__(self, data_path_1_SFT: str, tokenizer: transformers.PreTrainedTokenizer, verbose=False):\n",
        "        super(SFT_dataset, self).__init__()\n",
        "        logging.warning(\"Loading data...\")\n",
        "\n",
        "        pattern_instruction = 'prompt'  # instruction\n",
        "        pattern_output = 'completion'  # response\n",
        "\n",
        "        with open(data_path_1_SFT, \"r\", encoding='utf-8-sig') as json_file:\n",
        "            list_data_dict = json.load(json_file)\n",
        "\n",
        "        PROMPT_DICT = {\n",
        "            \"prompt_input\": (\n",
        "                \"### Instruction(ëª…ë ¹ì–´):\\n{prompt}\\n\\n### Response(ì‘ë‹µ):\"\n",
        "            )\n",
        "        }\n",
        "\n",
        "        prompt_input = PROMPT_DICT[\"prompt_input\"]\n",
        "\n",
        "        sources = []\n",
        "        for example in list_data_dict:\n",
        "            tmp = prompt_input.format_map(example)\n",
        "            sources.append(tmp)\n",
        "\n",
        "        targets = []\n",
        "        for example in list_data_dict:\n",
        "            targets.append(f\"{example[pattern_output]}{tokenizer.eos_token}\")\n",
        "        examples = [s + t for s, t in zip(sources, targets)]\n",
        "\n",
        "        sources_tokenized = self._tokenize_fn(sources, tokenizer)  # source\n",
        "        examples_tokenized = self._tokenize_fn(examples, tokenizer)  # source + target\n",
        "\n",
        "        input_ids = examples_tokenized[\"input_ids\"]\n",
        "        labels = copy.deepcopy(input_ids)\n",
        "        for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
        "            label[:source_len] = -100 # ì•ì— ì§ˆë¬¸ì€ í•™ìŠµí•˜ì§€ ì•Šë„ë¡!\n",
        "\n",
        "        data_dict = dict(input_ids=input_ids, labels=labels)\n",
        "\n",
        "        self.input_ids = data_dict[\"input_ids\"]\n",
        "        self.labels = data_dict[\"labels\"]\n",
        "        logging.warning(\"Loading data done!!: %d\"%(len(self.labels)))\n",
        "\n",
        "\n",
        "    def _tokenize_fn(self, strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
        "        tokenized_list = [\n",
        "            tokenizer(\n",
        "                text,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=\"longest\",\n",
        "                max_length=tokenizer.model_max_length,\n",
        "                truncation=True,\n",
        "            )\n",
        "            for text in strings\n",
        "        ]\n",
        "        input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
        "        input_ids_lens = labels_lens = [\n",
        "            tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n",
        "        ]\n",
        "        return dict(\n",
        "            input_ids=input_ids,\n",
        "            labels=labels,\n",
        "            input_ids_lens=input_ids_lens,\n",
        "            labels_lens=labels_lens,\n",
        "        )\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "\n",
        "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
        "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])"
      ],
      "metadata": {
        "id": "g9CPSQ1QazIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class DataCollatorForSupervisedDataset(object):\n",
        "\n",
        "    tokenizer: transformers.PreTrainedTokenizer\n",
        "\n",
        "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
        "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
        "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
        "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
        "        )\n",
        "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value= -100)\n",
        "        return dict(\n",
        "            input_ids=input_ids,\n",
        "            labels=labels,\n",
        "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
        "        )"
      ],
      "metadata": {
        "id": "ZAmK_q97bFR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = SFT_dataset(data_path_1_SFT='/content/KoChatGPT_Pre_Results/sft_processed.json', tokenizer=tokenizer)\n",
        "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
        "\n",
        "print('input : %s'%train_dataset.input_ids[0])\n",
        "print('output: %s'%train_dataset.labels[0])"
      ],
      "metadata": {
        "id": "FAu8jDB0bKX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = transformers.TrainingArguments(\n",
        "    output_dir=\"test\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    warmup_steps=5,\n",
        "    prediction_loss_only=True,\n",
        "    fp16 = True\n",
        "    )\n",
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset\n",
        ")"
      ],
      "metadata": {
        "id": "tEsIJUuybyMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n",
        "model.save_pretrained('models/output_1_SFT')"
      ],
      "metadata": {
        "id": "9P56n5wlb3KH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ê´€ì°° ê²°ê³¼: ì²˜ìŒì— ì–´ëŠ ì •ë„ ë¡œìŠ¤ê°€ ê°ì†Œí•˜ë‹¤ê°€ ì¼ì • ë¶€ë¶„ ì´í›„ëŠ” ë¡œìŠ¤ ê°ì†Œê°€ ì •ì²´ë¨ì„ ì•Œ ìˆ˜ ìˆìŒ."
      ],
      "metadata": {
        "id": "N6dRmquuexpd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generator = transformers.pipeline('text-generation', model='models/output_1_SFT', tokenizer=tokenizer)\n",
        "\n",
        "generation_args = dict(\n",
        "    num_beams=4,\n",
        "    repetition_penalty=2.0,\n",
        "    no_repeat_ngram_size=4,\n",
        "    eos_token_id=375, # \\n\n",
        "    max_new_tokens=64,\n",
        "    do_sample=True,\n",
        "    top_k=50,\n",
        "    early_stopping=True\n",
        ")\n",
        "\n",
        "PROMPT_DICT = {\n",
        "    \"prompt_input\": (\n",
        "        \"### Instruction(ëª…ë ¹ì–´):\\n{prompt}\\n\\n### Response(ì‘ë‹µ):\"\n",
        "    )\n",
        "}\n",
        "\n",
        "list_prompt = ['ë¶ˆê³ ê¸°ìš© ê³ ê¸° í•œìš°ì—ìš”?',\n",
        "               'ë¦¬ì²˜ë“œ ë‹‰ìŠ¨ì´ 43ëŒ€ ë¶€í†µë ¹ì§ì„ ìˆ˜í–‰í•œ ë…„ë„ëŠ”?',\n",
        "               'ì‹œì¹´ê³  ì˜¤í—¤ì–´ êµ­ì œê³µí•­ì€ ì–´ë””ì— ìˆì–´?',\n",
        "               'ì˜¤ëŠ˜ ë¯¸ì„¸ë¨¼ì§€ ì–´ë•Œ?']\n",
        "\n",
        "list_prompt = [PROMPT_DICT['prompt_input'].format_map({'prompt' : tmp}) for tmp in list_prompt]\n",
        "\n",
        "list_result = generator(list_prompt, **generation_args)\n",
        "for prompt, result in zip(list_prompt, list_result):\n",
        "    print()\n",
        "    print((result[0]['generated_text']))"
      ],
      "metadata": {
        "id": "CfoDeVJtcQ0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ê´€ì°° ê²°ê³¼: SFTë¥¼ í†µí•´ì„œ, ì§ˆë¬¸ì— ëŒ€í•´ì„œ ìƒë‹¹íˆ ì–‘ì§ˆì˜ ë‹µë³€ì„ í•˜ê³  ìˆë‹¤ê³  íŒë‹¨ëœë‹¤. ë§ˆì¹˜ ì‚¬ëŒì˜ ëŒ€ë‹µê³¼ ë¹„ìŠ·í•œ ëŠë‚Œì„"
      ],
      "metadata": {
        "id": "zN9QZZ48Z521"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RM**"
      ],
      "metadata": {
        "id": "0-YNcigdPQmW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "8aBpWUBBcdWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from chatgpt.dataset import RewardDataset\n",
        "from chatgpt.models.base import RewardModel\n",
        "from chatgpt.trainer.strategies import NaiveStrategy\n",
        "from chatgpt.trainer.rm import RewardModelTrainer\n",
        "\n",
        "from transformers.models.gpt2.configuration_gpt2 import GPT2Config\n",
        "from transformers.models.gpt2.modeling_gpt2 import GPT2Model\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "import random"
      ],
      "metadata": {
        "id": "EseEndX2ciZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTRM_custom(RewardModel):\n",
        "\n",
        "    def __init__(self,\n",
        "                 pretrained: Optional[str] = None,\n",
        "                 config: Optional[GPT2Config] = None,\n",
        "                 checkpoint: bool = False,\n",
        "                 lora_rank: int = 0,\n",
        "                 lora_train_bias: str = 'none',\n",
        "                 tokenizer=None) -> None:\n",
        "        if pretrained is not None:\n",
        "            model = GPT2Model.from_pretrained(pretrained)\n",
        "            model.resize_token_embeddings(len(tokenizer))\n",
        "        elif config is not None:\n",
        "            model = GPT2Model(config)\n",
        "        else:\n",
        "            model = GPT2Model(GPT2Config())\n",
        "        if checkpoint:\n",
        "            model.gradient_checkpointing_enable()\n",
        "\n",
        "        value_head = nn.Linear(model.config.n_embd, 1)\n",
        "        super().__init__(model, value_head, lora_rank, lora_train_bias)\n",
        "\n",
        "        if pretrained is not None:\n",
        "            self.model = model\n",
        "            self.pretrained = pretrained\n",
        "\n",
        "\n",
        "    def save_pretrained(self, dir):\n",
        "        if self.pretrained is not None:\n",
        "            self.model.save_pretrained(dir)"
      ],
      "metadata": {
        "id": "O3LnKAP1clsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained('skt/kogpt2-base-v2')\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    'skt/kogpt2-base-v2', bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
        "    padding_side=\"right\",\n",
        "    model_max_length=512,\n",
        ")\n",
        "\n",
        "with NaiveStrategy().model_init_context():\n",
        "        model = GPTRM_custom(pretrained='skt/kogpt2-base-v2', lora_rank=0, tokenizer=tokenizer).cuda()"
      ],
      "metadata": {
        "id": "PvAO-TnDcmtJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ranking dataset ë§Œë“¤ê¸°\n",
        "with open('/content/KoChatGPT_Pre_Results/rm_processed.json', \"r\", encoding='utf-8-sig') as json_file:\n",
        "    list_data_dict = json.load(json_file)\n",
        "\n",
        "total_data_ranking2chosen = []\n",
        "for tmp in list_data_dict:\n",
        "    one_data_ranking2chosen = []\n",
        "\n",
        "    data = {}\n",
        "    data['prompt'] = tmp['prompt']\n",
        "    if tmp['ranking'][0] < tmp['ranking'][1]:\n",
        "        data['chosen'] = tmp['completion_0']\n",
        "        data['rejected'] = tmp['completion_1']\n",
        "    else:\n",
        "        data['chosen'] = tmp['completion_1']\n",
        "        data['rejected'] = tmp['completion_0']\n",
        "    one_data_ranking2chosen.append(data)\n",
        "\n",
        "    data = {}\n",
        "    data['prompt'] = tmp['prompt']\n",
        "    if tmp['ranking'][0] < tmp['ranking'][2]:\n",
        "        data['chosen'] = tmp['completion_0']\n",
        "        data['rejected'] = tmp['completion_2']\n",
        "    else:\n",
        "        data['chosen'] = tmp['completion_2']\n",
        "        data['rejected'] = tmp['completion_0']\n",
        "    one_data_ranking2chosen.append(data)\n",
        "\n",
        "    data = {}\n",
        "    data['prompt'] = tmp['prompt']\n",
        "    if tmp['ranking'][1] < tmp['ranking'][2]:\n",
        "        data['chosen'] = tmp['completion_1']\n",
        "        data['rejected'] = tmp['completion_2']\n",
        "    else:\n",
        "        data['chosen'] = tmp['completion_2']\n",
        "        data['rejected'] = tmp['completion_1']\n",
        "    one_data_ranking2chosen.append(data)\n",
        "\n",
        "\n",
        "\n",
        "    total_data_ranking2chosen.extend(one_data_ranking2chosen)\n",
        "\n",
        "print('before data num: %d'%(len(list_data_dict)))\n",
        "print('after  data num: %d'%(len(total_data_ranking2chosen)))\n",
        "print('data example: \\n%s'%total_data_ranking2chosen[45])"
      ],
      "metadata": {
        "id": "YkEG9AxacqZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "kochatgpt_2_RM.jsonl ì€\n",
        "chatGPT, davinch, ada ì„¸ê°œ ëª¨ë¸ì— ê°™ì€ promptë¥¼ ì£¼ê³  ì–»ì€ ì„¸ ë‹µë³€ì„\n",
        "ìˆœì„œëŒ€ë¡œ good, bad, worstë¡œ ê°„ì£¼í•´\n",
        "ìˆœì„œë¥¼ ë’¤ì„ì–´ completion_0, completion_1, completion_2 ì„¸ í‚¤ì— í• ë‹¹í•˜ì—¬ ë§Œë“  ë°ì´í„°ì…‹ì…ë‹ˆë‹¤.\n",
        "ìœ„ì™€ ê°™ì´ ì½”ë“œë¥¼ ì§œê²Œ ë˜ë©´ chosenê³¼ resjectedì— ê°ê°\n",
        "completion_0, completion_1, completion_2 ì„¸ê°œ ë‹µë³€ì´ ê°€ëŠ¥í•œ ëª¨ë“  ì¡°í•©ìœ¼ë¡œ ë“¤ì–´ê°€ê²Œ ë˜ì–´\n",
        "chosenì— worst ë‹µë³€ì´ ë“¤ì–´ê°€ê³ \n",
        "rejectedì— goodë‹µë³€ì´ ë“¤ì–´ê°„ ë°ì´í„°ë„ ë§Œë“¤ì–´ì§‘ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "apHmP6BSrPCn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PairWiseLoss(nn.Module):\n",
        "\n",
        "    def forward(self, chosen_reward: torch.Tensor, reject_reward: torch.Tensor) -> torch.Tensor:\n",
        "        probs = torch.sigmoid(chosen_reward - reject_reward)\n",
        "        log_probs = torch.log(probs)\n",
        "        loss = -log_probs.mean()\n",
        "        return loss"
      ],
      "metadata": {
        "id": "StITE4z9rSlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_data_ranking2chosen = []\n",
        "\n",
        "for tmp in list_data_dict:\n",
        "     prompt = tmp['prompt']\n",
        "     ranking = tmp['ranking']\n",
        "\n",
        "     for index in range(1, len(ranking)):\n",
        "         n = ranking[0]\n",
        "         m = ranking[index]\n",
        "\n",
        "\n",
        "         data = {\n",
        "             'prompt': prompt,\n",
        "             'chosen': tmp['completion_{}'.format(n)],\n",
        "             'rejected': tmp['completion_{}'.format(m)]\n",
        "         }\n",
        "\n",
        "         total_data_ranking2chosen.append(data)"
      ],
      "metadata": {
        "id": "satQAa8Ms7Tw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random.seed(230319)\n",
        "random.shuffle(total_data_ranking2chosen)\n",
        "print(total_data_ranking2chosen[45])"
      ],
      "metadata": {
        "id": "33fILDnLcwK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ê´€ì°° ê²°ê³¼: ì—¬ê¸° ì•„ë˜ ì´ìƒí•˜ë‹¤. ë°ì´í„°ë¥¼ ì¼ë¶€ë¶„ë§Œ í•™ìŠµí•˜ê³  ìˆì—ˆë‹¤. RMì´ ì¤‘ìš”í•œë° ì´ ë¶€ë¶„ í•™ìŠµì´ ì˜ ì•ˆë˜ì—ˆìœ¼ë¡œ ë§ì€ ë¬¸ì œê°€ ë’¤ì—ì„œ ë‚˜íƒ€ë‚¬ì„ ê²ƒì´ë‹¤. ìˆ˜ì •í•¨. ì•„ë˜ ì½”ë“œ ì „ë°˜ì ìœ¼ë¡œ ìˆ˜ì •\n"
      ],
      "metadata": {
        "id": "4BM33hz9-TNZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# í˜¹ì‹œ ë°ì´í„°ê°€ ìˆœì„œëŒ€ë¡œ ì •ë ¬ë˜ì–´ ìˆë‹¤ë©´ í•™ìŠµì´ ì˜ ì•ˆ ë  ìˆ˜ ìˆìœ¼ë‹ˆ\n",
        "# ë‚˜ëˆ„ê¸° ì „ì— í•œë²ˆ ì„ì–´ì£¼ëŠ” ê²Œ ì¢‹ì•„ (ì´ë¯¸ ì„ì—¬ìˆë‹¤ë©´ ì´ ì¤„ì€ ë¹¼ë„ ë¼)\n",
        "random.shuffle(total_data_ranking2chosen)\n",
        "\n",
        "# 1. ì „ì²´ ë°ì´í„° ê°œìˆ˜ í™•ì¸\n",
        "total_len = len(total_data_ranking2chosen)\n",
        "\n",
        "# 2. 80% ì§€ì  ê³„ì‚° (ì†Œìˆ˜ì ì´ ë‚˜ì˜¤ë©´ ì•ˆ ë˜ë‹ˆê¹Œ intë¡œ ë³€í™˜)\n",
        "split_idx = int(total_len * 0.8)\n",
        "\n",
        "# 3. 8:2ë¡œ ë‚˜ëˆ„ê¸°\n",
        "train_data = total_data_ranking2chosen[:split_idx]  # ì²˜ìŒë¶€í„° 80% ì§€ì ê¹Œì§€\n",
        "eval_data = total_data_ranking2chosen[split_idx:]   # 80% ì§€ì ë¶€í„° ëê¹Œì§€\n",
        "\n",
        "# 4. ì˜ ë‚˜ë‰˜ì—ˆëŠ”ì§€ í™•ì¸\n",
        "print(f\"ì „ì²´ ë°ì´í„°: {total_len}ê°œ\")\n",
        "print(f\"í•™ìŠµ ë°ì´í„°(Train): {len(train_data)}ê°œ (ì•½ {len(train_data)/total_len*100:.1f}%)\")\n",
        "print(f\"ê²€ì¦ ë°ì´í„°(Eval): {len(eval_data)}ê°œ (ì•½ {len(eval_data)/total_len*100:.1f}%)\")\n",
        "\n",
        "# 5. ë°ì´í„°ì…‹ ìƒì„±\n",
        "train_dataset = RewardDataset(train_data, tokenizer, 512)\n",
        "eval_dataset = RewardDataset(eval_data, tokenizer, 512)"
      ],
      "metadata": {
        "id": "pd4m8cbtcxT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ê´€ì°° ê²°ê³¼: ìœ„ ì…€ì—ì„œ ë°ì´í„°ë¥¼ ì œí•œì ìœ¼ë¡œ ì“°ëŠ” ê²ƒì„ ë†“ì¹˜ê³  ìˆì—ˆë‹¤. í•´ì œê°€ í•„ìš”í–ˆë‹¤. ë¬¸ì œ ì›ì¸ì„ ì°¾ì•˜ë‹¤.\n",
        "\n",
        "\n",
        "<í•´ë‹¹ ê¸°ì¡´ ì½”ë“œ ë¶€ë¶„>\n",
        "\n",
        "train_data = total_data_ranking2chosen[:1000]\n",
        "\n",
        "eval_data = total_data_ranking2chosen[1000:1200]"
      ],
      "metadata": {
        "id": "3O-shQm3_I6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "idx = 1\n",
        "print('#'*70)\n",
        "print('## prompt ##')\n",
        "print(train_data[idx]['prompt'])\n",
        "print('#'*70)\n",
        "print('## chosen ##')\n",
        "print(train_data[idx]['chosen'])\n",
        "print('#'*70)\n",
        "print('## rejected ##')\n",
        "print(train_data[idx]['rejected'])"
      ],
      "metadata": {
        "id": "Uv9HG74Xc0sc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ê´€ì°° ê²°ê³¼: ì´ìƒí•œ ë°ì´í„°ë¥¼ í™œìš©í–ˆêµ¬ë‚˜ . -> í•´ê²°: ìƒ˜í”Œ ë°ì´í„°ì—ì„œ ì „ì²´ ë°ì´íŠ¸ë¥¼ í™œìš©í•˜ê²Œ ë³€í™”. ì´ì œ ì–‘ì§ˆì˜ ë°ì´í„° í™œìš©í•˜ë¯€ë¡œ -> ì¬í•™ìŠµ"
      ],
      "metadata": {
        "id": "sgTkGNBNafSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = RewardModelTrainer(model=model,\n",
        "                             strategy=NaiveStrategy(),\n",
        "                             optim=torch.optim.Adam(model.parameters(), lr=5e-5),\n",
        "                             train_dataset=train_dataset,\n",
        "                             eval_dataset=eval_dataset,\n",
        "                             batch_size=4,\n",
        "                             max_epochs=3)"
      ],
      "metadata": {
        "id": "x-8lILG9c2Vb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.fit(use_lora=0)\n",
        "\n",
        "model.save_pretrained('models/output_2_RM')"
      ],
      "metadata": {
        "id": "WJkLOQC0c33r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ê´€ì°° ê²°ê³¼: ì´ ë¶€ë¶„ì€ ì–´ë””ì„œ ì—°ì‚°ì´ ì´ë£¨ì–´ì§€ëŠ” ê²ƒì¼ê¹Œ? ìœ ì˜í•˜ì§€ ì•Šìœ¼ë©´ ë…¸íŠ¸ë¶ ì‹¤í–‰ì„ ì¤‘ë³µí•´ì„œ ì—‰ë§ì§•ì°½ì´ ë  ìˆ˜ ìˆê² ë‹¤. epoch 0ì—ì„œ ê´œì°®ë‹¤ê°€ epoch 1ì—ì„œëŠ” ë¡œìŠ¤ê°€ í¬ê²Œ ìƒìŠ¹í•˜ê³  ë‹¤ì‹œ ë‚´ë ¤ê°€ëŠ” ëª¨ì–‘ìƒˆ ê°™ë‹¤. ì´ìœ ê°€ ë­˜ê¹Œ?"
      ],
      "metadata": {
        "id": "4QgaEQCAL9gf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "UR0wYSDESMK-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def inference_RM(input_text):\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors='pt').cuda()\n",
        "    output = model(input_ids)\n",
        "    output_reward = output.cpu().detach().numpy()[0]\n",
        "\n",
        "    print('input: %s\\nreward score: %.1f'%(input_text, output_reward))\n",
        "\n",
        "    return output_reward\n",
        "\n",
        "input_text = 'ì¸ê³µì§€ëŠ¥ì€ ë˜¥ë©ì²­ì´ ì…ë‹ˆë‹¤'\n",
        "output_reward = inference_RM(input_text=input_text)"
      ],
      "metadata": {
        "id": "ZPdIRDrKc5k-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = 'ì¸ê³µì§€ëŠ¥(AI)ì€ ì»´í“¨í„°ì—ì„œ ìŒì„± ë° ì‘ì„±ëœ ì–¸ì–´ë¥¼ ë³´ê³  ì´í•´í•˜ê³  ë²ˆì—­í•˜ê³  ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ì¶”ì²œí•˜ëŠ” ê¸°ëŠ¥ì„ í¬í•¨í•˜ì—¬ ë‹¤ì–‘í•œ ê³ ê¸‰ ê¸°ëŠ¥ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” ì¼ë ¨ì˜ ê¸°ìˆ ì…ë‹ˆë‹¤.'\n",
        "\n",
        "output_reward = inference_RM(input_text=input_text)"
      ],
      "metadata": {
        "id": "mGCby0BSc7H8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"ì¸ê³µì§€ëŠ¥ì€ ì¼ë°˜ì ìœ¼ë¡œ ì¸ê°„ì˜ ì§€ëŠ¥ì´ í•„ìš”í•˜ê±°ë‚˜ ì¸ê°„ì´ ë¶„ì„í•  ìˆ˜ ìˆëŠ” ê²ƒë³´ë‹¤ ê·œëª¨ê°€ í° ë°ì´í„°ë¥¼ í¬í•¨í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì¶”ë¡ , í•™ìŠµ ë° í–‰ë™í•  ìˆ˜ ìˆëŠ” ì»´í“¨í„° ë° ê¸°ê³„ë¥¼ êµ¬ì¶•í•˜ëŠ” ê²ƒê³¼ ê´€ë ¨ëœ ê³¼í•™ ë¶„ì•¼ì…ë‹ˆë‹¤. AIëŠ” ì»´í“¨í„° ê³µí•™, ë°ì´í„° ë¶„ì„ ë° í†µê³„, í•˜ë“œì›¨ì–´ ë° ì†Œí”„íŠ¸ì›¨ì–´ ì—”ì§€ë‹ˆì–´ë§, ì–¸ì–´í•™, ì‹ ê²½ ê³¼í•™ì€ ë¬¼ë¡  ì² í•™ê³¼ ì‹¬ë¦¬í•™ì„ í¬í•¨í•˜ì—¬ ì—¬ëŸ¬ í•™ë¬¸ì„ í¬ê´„í•˜ëŠ” ê´‘ë²”ìœ„í•œ ë¶„ì•¼ì…ë‹ˆë‹¤. ë¹„ì¦ˆë‹ˆìŠ¤ì˜ ìš´ì˜ ìˆ˜ì¤€ì—ì„œ AIëŠ” ì£¼ë¡œ ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ ëŸ¬ë‹ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” ê¸°ìˆ  ëª¨ìŒìœ¼ë¡œ, ë°ì´í„° ë¶„ì„, ì˜ˆìƒ ë° ì˜ˆì¸¡, ê°ì²´ ë¶„ë¥˜, ìì—°ì–´ ì²˜ë¦¬, ì¶”ì²œ, ì§€ëŠ¥í˜• ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ë“±ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\"\n",
        "\n",
        "output_reward = inference_RM(input_text=input_text)"
      ],
      "metadata": {
        "id": "oN6oVXxgyAhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"ì¸ê³µì§€ëŠ¥(AI)ì€ ì»´í“¨í„°ì—ì„œ ìŒì„± ë° ì‘ì„±ëœ ì–¸ì–´ë¥¼ ë³´ê³  ì´í•´í•˜ê³  ë²ˆì—­í•˜ê³  ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ì¶”ì²œí•˜ëŠ” ê¸°ëŠ¥ì„ í¬í•¨í•˜ì—¬ ë‹¤ì–‘í•œ ê³ ê¸‰ ê¸°ëŠ¥ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” ì¼ë ¨ì˜ ê¸°ìˆ ì…ë‹ˆë‹¤. AIëŠ” í˜„ëŒ€ì ì¸ ì»´í“¨íŒ… í˜ì‹ ì—ì„œ ì¤‘ì¶”ì ì¸ ì—­í• ì„ í•˜ë©° ê°œì¸ê³¼ ë¹„ì¦ˆë‹ˆìŠ¤ì˜ ê°€ì¹˜ë¥¼ ì°½ì¶œí•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ê´‘í•™ ë¬¸ì ì¸ì‹(OCR)ì€ AIë¥¼ ì‚¬ìš©í•´ ì´ë¯¸ì§€ ë° ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ ë° ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ê³ , êµ¬ì¡°í™”ë˜ì§€ ì•Šì€ ì½˜í…ì¸ ë¥¼ ë¹„ì¦ˆë‹ˆìŠ¤ì— ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ ë§Œë“¤ê³ , ìœ ìš©í•œ ì •ë³´ë¥¼ ì°½ì¶œí•©ë‹ˆë‹¤.\"\n",
        "\n",
        "output_reward = inference_RM(input_text=input_text)"
      ],
      "metadata": {
        "id": "PKAmag1Fc-Hz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ê´€ì°° ê²°ê³¼: baseline RM **\n",
        "\n",
        "ë¶€ì •ì ì¸ inputì— ëŒ€í•´ì„œ ë¶€ì •ì ìœ¼ë¡œ ìŠ¤ì½”ì–´ë¥¼ ë§¤ê¸°ëŠ” ë¶€ë¶„ì´ ì¸ìƒì„. ì•„ìš¸ëŸ¬ì„œ, ì–¼í•ë³´ë©´ ì ì ˆí•˜ë‹¤ê³  íŒë‹¨ë˜ëŠ” input ì¤‘ì—ë„ ë³´ë‹¤ ì ì ˆí•œ ê²ƒì— íŒë‹¨í•˜ëŠ” ê²ƒì´ ì‹ ê¸°í•˜ê³  ì¸ìƒì ì„.\n",
        "\n",
        "ì˜ˆ)\n",
        "\n",
        "input: ì¸ê³µì§€ëŠ¥ì€ ë˜¥ë©ì²­ì´ ì…ë‹ˆë‹¤\n",
        "\n",
        "reward score: -23.5\n",
        "\n",
        "input: ì¸ê³µì§€ëŠ¥(AI)ì€ ì»´í“¨í„°ì—ì„œ ìŒì„± ë° ì‘ì„±ëœ ì–¸ì–´ë¥¼ ë³´ê³  ì´í•´í•˜ê³  ë²ˆì—­í•˜ê³  ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ì¶”ì²œí•˜ëŠ” ê¸°ëŠ¥ì„ í¬í•¨í•˜ì—¬ ë‹¤ì–‘í•œ ê³ ê¸‰ ê¸°ëŠ¥ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” ì¼ë ¨ì˜ ê¸°ìˆ ì…ë‹ˆë‹¤.\n",
        "\n",
        "reward score: -15.0\n",
        "\n",
        "input: ì¸ê³µì§€ëŠ¥ì€ ì¼ë°˜ì ìœ¼ë¡œ ì¸ê°„ì˜ ì§€ëŠ¥ì´ í•„ìš”í•˜ê±°ë‚˜ ì¸ê°„ì´ ë¶„ì„í•  ìˆ˜ ìˆëŠ” ê²ƒë³´ë‹¤ ê·œëª¨ê°€ í° ë°ì´í„°ë¥¼ í¬í•¨í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì¶”ë¡ , í•™ìŠµ ë° í–‰ë™í•  ìˆ˜ ìˆëŠ” ì»´í“¨í„° ë° ê¸°ê³„ë¥¼ êµ¬ì¶•í•˜ëŠ” ê²ƒê³¼ ê´€ë ¨ëœ ê³¼í•™ ë¶„ì•¼ì…ë‹ˆë‹¤. AIëŠ” ì»´í“¨í„° ê³µí•™, ë°ì´í„° ë¶„ì„ ë° í†µê³„, í•˜ë“œì›¨ì–´ ë° ì†Œí”„íŠ¸ì›¨ì–´ ì—”ì§€ë‹ˆì–´ë§, ì–¸ì–´í•™, ì‹ ê²½ ê³¼í•™ì€ ë¬¼ë¡  ì² í•™ê³¼ ì‹¬ë¦¬í•™ì„ í¬í•¨í•˜ì—¬ ì—¬ëŸ¬ í•™ë¬¸ì„ í¬ê´„í•˜ëŠ” ê´‘ë²”ìœ„í•œ ë¶„ì•¼ì…ë‹ˆë‹¤. ë¹„ì¦ˆë‹ˆìŠ¤ì˜ ìš´ì˜ ìˆ˜ì¤€ì—ì„œ AIëŠ” ì£¼ë¡œ ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ ëŸ¬ë‹ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” ê¸°ìˆ  ëª¨ìŒìœ¼ë¡œ, ë°ì´í„° ë¶„ì„, ì˜ˆìƒ ë° ì˜ˆì¸¡, ê°ì²´ ë¶„ë¥˜, ìì—°ì–´ ì²˜ë¦¬, ì¶”ì²œ, ì§€ëŠ¥í˜• ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ë“±ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "reward score: 15.0\n",
        "\n",
        "input: ì¸ê³µì§€ëŠ¥(AI)ì€ ì»´í“¨í„°ì—ì„œ ìŒì„± ë° ì‘ì„±ëœ ì–¸ì–´ë¥¼ ë³´ê³  ì´í•´í•˜ê³  ë²ˆì—­í•˜ê³  ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ì¶”ì²œí•˜ëŠ” ê¸°ëŠ¥ì„ í¬í•¨í•˜ì—¬ ë‹¤ì–‘í•œ ê³ ê¸‰ ê¸°ëŠ¥ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” ì¼ë ¨ì˜ ê¸°ìˆ ì…ë‹ˆë‹¤. AIëŠ” í˜„ëŒ€ì ì¸ ì»´í“¨íŒ… í˜ì‹ ì—ì„œ ì¤‘ì¶”ì ì¸ ì—­í• ì„ í•˜ë©° ê°œì¸ê³¼ ë¹„ì¦ˆë‹ˆìŠ¤ì˜ ê°€ì¹˜ë¥¼ ì°½ì¶œí•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ê´‘í•™ ë¬¸ì ì¸ì‹(OCR)ì€ AIë¥¼ ì‚¬ìš©í•´ ì´ë¯¸ì§€ ë° ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ ë° ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ê³ , êµ¬ì¡°í™”ë˜ì§€ ì•Šì€ ì½˜í…ì¸ ë¥¼ ë¹„ì¦ˆë‹ˆìŠ¤ì— ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ ë§Œë“¤ê³ , ìœ ìš©í•œ ì •ë³´ë¥¼ ì°½ì¶œí•©ë‹ˆë‹¤.\n",
        "\n",
        "reward score: 5.9"
      ],
      "metadata": {
        "id": "JzL-UbzbevQe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PPO**"
      ],
      "metadata": {
        "id": "ZWROKjuyPjkJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "vK58_umYdJYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from chatgpt.models.gpt import GPTActor, GPTCritic\n",
        "from chatgpt.trainer import PPOTrainer\n",
        "\n",
        "from copy import deepcopy"
      ],
      "metadata": {
        "id": "Bw88XjDQyYjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with NaiveStrategy().model_init_context():\n",
        "    actor = GPTActor(pretrained='models/output_1_SFT', lora_rank=0).to(torch.cuda.current_device())\n",
        "    critic = GPTCritic(pretrained='models/output_2_RM', lora_rank=0).to(torch.cuda.current_device())\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        'skt/kogpt2-base-v2', bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
        "        padding_side=\"right\",\n",
        "        model_max_length=512\n",
        "    )\n",
        "    initial_model = deepcopy(actor)\n",
        "    reward_model = RewardModel(deepcopy(critic.model), deepcopy(critic.value_head)).to(torch.cuda.current_device())"
      ],
      "metadata": {
        "id": "SzQdBSP4yaZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "actor_optim = torch.optim.Adam(actor.parameters(), lr=1e-6) # ì‹¤í—˜ #1 ìˆ˜ì •\n",
        "critic_optim = torch.optim.Adam(critic.parameters(), lr=1e-6) # ì‹¤í—˜ #1 ìˆ˜ì •"
      ],
      "metadata": {
        "id": "TrEyAxNgypZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(actor, actor_optim), (critic, critic_optim), reward_model, initial_model = NaiveStrategy().prepare(\n",
        "    (actor, actor_optim), (critic, critic_optim), reward_model, initial_model)"
      ],
      "metadata": {
        "id": "gnoZDjF0y0VI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/KoChatGPT_Pre_Results/ppo_processed.json', \"r\", encoding='utf-8-sig') as json_file:\n",
        "    list_data_dict = json.load(json_file)\n",
        "    list_prompt = [tmp['prompt'] for tmp in list_data_dict]\n",
        "\n",
        "def tokenize_fn(texts):\n",
        "    batch = tokenizer(texts, return_tensors='pt', max_length=32, padding=True, truncation=True) # Further reduced max_length\n",
        "    return {k: v.cuda() for k, v in batch.items()}"
      ],
      "metadata": {
        "id": "VERcmerDy3BD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenize_fn('It takes something more than intelligence to act intelligently.'))"
      ],
      "metadata": {
        "id": "O1Fxm_V6y6DX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(list_prompt)"
      ],
      "metadata": {
        "id": "RWf3bu6vy9eK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = PPOTrainer(NaiveStrategy(),\n",
        "                     actor,\n",
        "                     critic,\n",
        "                     reward_model,\n",
        "                     initial_model,\n",
        "                     actor_optim,\n",
        "                     critic_optim,\n",
        "                     max_epochs=1,\n",
        "                     train_batch_size=8,\n",
        "                     tokenizer=tokenize_fn,\n",
        "                     max_length=128,\n",
        "                     do_sample=True,\n",
        "                     temperature=1.0,\n",
        "                     top_k=50,\n",
        "                     pad_token_id=tokenizer.pad_token_id,\n",
        "                     eos_token_id=tokenizer.eos_token_id)"
      ],
      "metadata": {
        "id": "e8nDHBfCzB6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.fit(list_prompt,\n",
        "            num_episodes=10,\n",
        "            max_timesteps=3,\n",
        "            update_timesteps=3)\n",
        "\n",
        "actor.model.save_pretrained('models/output_3_PPO')"
      ],
      "metadata": {
        "id": "gAAbZbBbzIpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ì•„ì§ ì •í™•íˆ ëª¨ë¥´ëŠ” ìš©ì–´ë“¤ì´ ìˆë‹¤. actorì™€ í¬ë¦­í‹± ë¶€ë¶„? íŠ¸ë ˆì¸ ì—í”¼ì†Œë“œ?"
      ],
      "metadata": {
        "id": "lRJvEk2AgLQq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ì´ ì½”ë“œ(PPO í•™ìŠµ)ì—ì„œì˜ **Episode**ì™€ **Epoch**ì˜ ê´€ê³„ëŠ” **\"ë°ì´í„° ìˆ˜ì§‘ ì£¼ê¸°\"**ì™€ **\"í•™ìŠµ ë°˜ë³µ íšŸìˆ˜\"**ì˜ ì°¨ì´ë¡œ ì´í•´í•˜ì‹œë©´ ê°€ì¥ ì •í™•í•©ë‹ˆë‹¤.\n",
        "\n",
        "### 1. Episode (num_episodes) = \"ë°ì´í„° ìˆ˜ì§‘ ë° í•™ìŠµì˜ í° ì£¼ê¸°\"\n",
        "* **ì •ì˜:** PPO í•™ìŠµì˜ **ê°€ì¥ ë°”ê¹¥ìª½ ë£¨í”„(Outer Loop)**ì…ë‹ˆë‹¤.\n",
        "* **ì—­í• :**\n",
        "    1.  í˜„ì¬ì˜ ì–¸ì–´ ëª¨ë¸(Actor)ì´ ì£¼ì–´ì§„ ì§ˆë¬¸(`list_prompt`)ì— ëŒ€í•´ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤. (ê²½í—˜ ë°ì´í„° ìˆ˜ì§‘)\n",
        "    2.  ì´ ë‹µë³€ë“¤ì— ëŒ€í•´ ë³´ìƒ ëª¨ë¸(Critic/Reward Model)ì´ ì ìˆ˜ë¥¼ ë§¤ê¹ë‹ˆë‹¤.\n",
        "    3.  ì´ ë°ì´í„°ë¥¼ ê°€ì§€ê³  ëª¨ë¸ ì—…ë°ì´íŠ¸ ë‹¨ê³„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.\n",
        "* **ì½”ë“œ ë¬¸ë§¥:** `trainer.fit(..., num_episodes=10, ...)`ì€ ì´ \"ìƒì„± -> ì±„ì  -> í•™ìŠµ\" ê³¼ì •ì„ ì´ 10ë²ˆ ë°˜ë³µí•œë‹¤ëŠ” ëœ»ì…ë‹ˆë‹¤.\n",
        "\n",
        "### 2. Epoch (max_epochs) = \"ìˆ˜ì§‘ëœ ë°ì´í„°ë¡œ í•™ìŠµí•˜ëŠ” íšŸìˆ˜\"\n",
        "* **ì •ì˜:** ëª¨ë¸ ì—…ë°ì´íŠ¸ ë‹¨ê³„ ì•ˆì—ì„œì˜ **ì•ˆìª½ ë£¨í”„(Inner Loop)**ì…ë‹ˆë‹¤.\n",
        "* **ì—­í• :** í•˜ë‚˜ì˜ Episodeì—ì„œ ìˆ˜ì§‘ëœ(ìƒì„±ëœ) ë°ì´í„°ë¥¼ ê°€ì§€ê³ , ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ëª‡ ë²ˆ ë°˜ë³µí•´ì„œ ì—…ë°ì´íŠ¸í• ì§€ ê²°ì •í•©ë‹ˆë‹¤.\n",
        "* **ì½”ë“œ ë¬¸ë§¥:** `PPOTrainer(..., max_epochs=3, ...)`ì€ í•œ ë²ˆ ìˆ˜ì§‘í•œ ë°ì´í„°ë¥¼ 3ë²ˆ ì¬ì‚¬ìš©í•´ì„œ í•™ìŠµí•œë‹¤ëŠ” ëœ»ì…ë‹ˆë‹¤.\n",
        "\n",
        "---\n",
        "\n",
        "### **ğŸ’¡ ë‘ ë³€ìˆ˜ì˜ ê´€ê³„ (ìš”ì•½)**\n",
        "\n",
        "> **\"1ë²ˆì˜ Episode ë™ì•ˆ, ëª¨ë¸ì€ ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ ê°€ì§€ê³  max_epochs íšŸìˆ˜ë§Œí¼ ë°˜ë³µ í•™ìŠµí•œë‹¤.\"**\n",
        "\n",
        "ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„í•˜ë©´ ì „ì²´ í•™ìŠµ íšŸìˆ˜ëŠ” ëŒ€ëµ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
        "$$\\text{ì´ ì—…ë°ì´íŠ¸ íšŸìˆ˜} \\approx \\text{num\\_episodes} \\times \\text{max\\_epochs}$$\n",
        "\n",
        "### **ë¬¸ì œê°€ ë°œìƒí•œ ì›ì¸ê³¼ í•´ê²°ì±…**\n",
        "ì•„ê¹Œ `Vul`, `Code` ê°™ì€ ì´ìƒí•œ ë‹¨ì–´ê°€ ë‚˜ì˜¨ ì´ìœ ëŠ” **Epoch(`max_epochs`)ê°€ ë„ˆë¬´ ë†’ì•˜ê¸° ë•Œë¬¸**ì…ë‹ˆë‹¤.\n",
        "\n",
        "1.  **ê³¼ì í•©(Overfitting):** PPOëŠ” ë°©ê¸ˆ ìˆ˜ì§‘í•œ ë°ì´í„°ë¥¼ í•™ìŠµì— ì‚¬ìš©í•˜ëŠ”ë°, ì´ ì ì€ ë°ì´í„°ë¥¼ 3ë²ˆì´ë‚˜ ë°˜ë³µí•´ì„œ(`max_epochs=3`) í•™ìŠµí•˜ë‹¤ ë³´ë‹ˆ, ëª¨ë¸ì´ ì¼ë°˜ì ì¸ ì–¸ì–´ ëŠ¥ë ¥ì„ ìƒì–´ë²„ë¦¬ê³  ë³´ìƒì„ ì˜ ë°›ëŠ” íŠ¹ì • íŒ¨í„´ì—ë§Œ ê³¼í•˜ê²Œ ë§ì¶°ì ¸ ë²„ë¦° ê²ƒì…ë‹ˆë‹¤. (ì´ë¥¼ **Catastrophic Forgetting**ì´ë¼ í•©ë‹ˆë‹¤.)\n",
        "2.  **í•´ê²°ì±…:** `max_epochs`ë¥¼ **1**ë¡œ ì¤„ì´ë©´, ë°ì´í„°ë¥¼ í•œ ë²ˆë§Œ ë³´ê³  ê°€ë³ê²Œ ì—…ë°ì´íŠ¸í•˜ê³  ë„˜ì–´ê°€ë¯€ë¡œ ê¸°ì¡´ì˜ ì–¸ì–´ ëŠ¥ë ¥ì„ ìœ ì§€í•˜ë©´ì„œ ì²œì²œíˆ ë³´ìƒì„ ë†’ì´ëŠ” ë°©í–¥ìœ¼ë¡œ í•™ìŠµí•˜ê²Œ ë©ë‹ˆë‹¤.\n",
        "\n",
        "ì œì•ˆí•´ ë“œë¦° ì½”ë“œ ìˆ˜ì •(`max_epochs=1`, `lr=1e-6`)ì€ ì´ **ì•ˆìª½ ë£¨í”„ì˜ ê°•ë„ë¥¼ ë‚®ì¶°ì„œ ëª¨ë¸ì„ ë³´í˜¸**í•˜ëŠ” ì„¤ì •ì…ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "fZrnvi-RVQKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generation(input_text, model):\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(\n",
        "        torch.cuda.current_device())\n",
        "    outputs = model.generate(input_ids,\n",
        "                             max_length=250,\n",
        "                             do_sample=True,\n",
        "                             top_k=50,\n",
        "                             top_p=0.95,\n",
        "                             num_return_sequences=1)\n",
        "    output = tokenizer.batch_decode(outputs[0], skip_special_tokens=True)[0]\n",
        "    print()\n",
        "    print(output)\n",
        "    return output\n",
        "\n",
        "PROMPT_DICT = {\n",
        "    \"prompt_input\": (\n",
        "        \"### Instruction(ëª…ë ¹ì–´):\\n{prompt}\\n\\n### Response(ì‘ë‹µ):\"\n",
        "    )\n",
        "}\n",
        "\n",
        "list_prompt = [\n",
        "    'ë¶ˆê³ ê¸°ìš© ê³ ê¸° í•œìš°ì—ìš”?',\n",
        "    'ë¦¬ì²˜ë“œ ë‹‰ìŠ¨ì´ 43ëŒ€ ë¶€í†µë ¹ì§ì„ ìˆ˜í–‰í•œ ë…„ë„ëŠ”?',\n",
        "    'ì‹œì¹´ê³  ì˜¤í—¤ì–´ êµ­ì œê³µí•­ì€ ì–´ë””ì— ìˆì–´',\n",
        "    'ì˜¤ëŠ˜ ë¯¸ì„¸ë¨¼ì§€ ì–´ë•Œ?']\n",
        "\n",
        "list_prompt = [PROMPT_DICT['prompt_input'].format_map({'prompt': tmp}) for tmp in list_prompt]\n",
        "\n",
        "for input_text in list_prompt:\n",
        "    output = generation(input_text, actor)"
      ],
      "metadata": {
        "id": "SZ7vMUM60bH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **<ë¶„ì„ ê²°ê³¼ ë¹„êµ>**"
      ],
      "metadata": {
        "id": "RWHghy73nE8v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. baseline model**"
      ],
      "metadata": {
        "id": "meN__olTnOTO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jenrwFxWBX_u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "380mw7unBZnM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "***\n",
        "\n",
        "# <Base ëª¨ë¸ì˜ ë‹µë³€ ì„±ëŠ¥ì— ëŒ€í•œ ì •ì„±ì  í‰ê°€>\n",
        "\n",
        "### 1. ì˜ˆì‹œ\n",
        "**Instruction (ëª…ë ¹ì–´):**\n",
        "> ë¶ˆê³ ê¸°ìš© ê³ ê¸° í•œìš°ì—ìš”?\n",
        "\n",
        "**Response (ì‘ë‹µ):**\n",
        "> 'ë¶ˆê³ ê¸°ìš© ê³ ê¸° í•œìš°ëŠ” ë‹¤ì–‘í•œ ì¢…ë¥˜ì˜ ê³ ê¸°ë¥¼ ì‚¬ìš©í•˜ë©°, ê° ë¶€ìœ„ë§ˆë‹¤ ë§›ê³¼ í–¥ì´ ë‹¤ë¥´ë¯€ë¡œ\n",
        "> ì¼ë°˜ì ìœ¼ë¡œ ë¶ˆê³ ê¸° í•œìš°ëŠ” ë¬¼ì— ë„£ê³  1ì‹œê°„ì”© ìˆ™ì„±ì‹œí‚¤ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.\n",
        ">\n",
        "> ë˜í•œ, ê³ ê¸°ëŠ” ì£¼ë¡œ ê³ ê¸°ì™€ í•¨ê»˜ ê°„ì‹ìœ¼ë¡œ ë¨¹ëŠ” ì†ŒìŠ¤ì™€ í•¨ê»˜ ë¨¹ëŠ”ë°,\n",
        "> ì†ŒìŠ¤ì™€ í•¨ê»˜ ë¨¹ìœ¼ë©´ ë” ë§ì€ ì‹ê°ê³¼ ë§›ì„ ëŠë‚„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "> æœŸì ìœ¼ë¡œ ê°•í•œ ìˆ™ì„± ë°©ë²•ì¸ì§€ ì‹œí—˜í•´ë³´ê¸° ìœ„í•´ì„œëŠ” ë³„ë„ì˜ ê²€ì¦ë˜ì§€ ì•Šì€ ê³ ê¸°ë¥¼ ì‚¬ìš©í•˜ëŠ”ì§€ í™•ì¸í•´ ë³´ì‹œëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.\n",
        ">\n",
        "> Vul ì´ìœ ê²½ ì™¸ ì—¬ëŸ¬ ê°€ì§€ ìš”ë¦¬ì— ëŒ€í•œ ì •ë³´ê°€ ì—†ê¸° ë•Œë¬¸ì— ì •í™•í•œ ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n",
        "> ë‹¤ì´ì•„ëª¬ë“œí•œìš°ëŠ” ê³ ê¸°ë¥¼ ì§ì ‘ ë§Œë“¤ê±°ë‚˜ êµë°°í•˜ëŠ” ë°©ë²•ì€ ì—†ìœ¼ë©°\n",
        "> ì¼ë¶€ ê³ ê¸‰ ìœ¡ë¥˜ëŠ” ê³ ê¸°ë‚˜ ì–‘ ë“± ë‹¤ì–‘í•œ ë°©ë²•ìœ¼ë¡œ ìˆ™ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        ">\n",
        "> æ©Ÿç”Ÿæ©Ÿé«”çš„ì¸ ìƒê¸°ëŠ” ë³€ë™ë  ìˆ˜ ìˆì§€ë§Œ, ê±´ê°•í•œ ì‚´ë¦¼ì€ ê³„ì†í•´ì„œ ë°œì „í•  ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.\n",
        "> ë‹¤ì´ì•„ëª¬ë“œëŠ” ê±´ê°•í•œ ì‚¶ì„ ì´ëŒì–´ì¤„ ìˆ˜ ìˆëŠ” ì¢‹ì€ ê³ ê¸°ì…ë‹ˆë‹¤.\n",
        "> Vulë½€í‘¸ - Vulë½€í‘¸ ì¢…ë¥˜ëŠ” ì¢…ë¥˜ì˜ ë‹¤ì–‘ì„±ì— ë”°ë¼ ë§¤ìš° ë‹¤ì–‘í•©ë‹ˆë‹¤.\n",
        "> ë”°ë¼ì„œ, ì •í™•í•œ ì •ë³´ë¥¼ ì–»ê¸° ìœ„í•´ì„œëŠ” ì§€ì†ì ì¸ ì¡°ì‚¬ì™€ ì—°êµ¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.\n",
        "> ä¿¡å ±æ—ìœ¼ë¡œì¨, ì¢…ë¥˜ ëª¨ë‘ê°€ ê±´ê°•í•˜ê²Œ ì‚¬ëŠ” ê²ƒì„ ê¸°ì›í•©ë‹ˆë‹¤. Code(ë¹„íƒ€ë¯¼), B2(\n",
        "\n",
        "---\n",
        "\n",
        "### 2. ì˜ˆì‹œ\n",
        "**Instruction (ëª…ë ¹ì–´):**\n",
        "> ë¦¬ì²˜ë“œ ë‹‰ìŠ¨ì´ 43ëŒ€ ë¶€í†µë ¹ì§ì„ ìˆ˜í–‰í•œ ë…„ë„ëŠ”?\n",
        "\n",
        "**Response (ì‘ë‹µ):**\n",
        "> 'ë¦¬ì²˜ë“œ ë‹‰ìŠ¨ì€ 39ëŒ€ ë¶€í†µë ¹ì„ ìˆ˜í–‰í•œ ë…„ë„ë¥¼ ê°–ê³  ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
        "> ê·¸ëŸ¬ë‚˜ ë¦¬ì²˜ë“œ ë‹‰ìŠ¨ì€ ì§€ë‚œ 1947ë…„ ë¯¸êµ­ ê²½ì œìŠ¤í„°ì—ì„œ ëŒ€í†µë ¹ìœ¼ë¡œ ì¬ì„í–ˆë˜ ê²½ë ¥ì´ ìˆìŠµë‹ˆë‹¤.\n",
        "> í´ë ˆì–´ ë ˆì´ë” (Clayer Rather)ì˜ ì €ê²©ìˆ˜ë¡œ ì§€ëª…ë˜ì–´ ë¯¸êµ­ ê²½ì œì™€ ì •ì¹˜ ë¬¸ì œì— ëŒ€í•œ ëŒ€ì²˜ì— ë§¤ìš° ê´€ì‹¬ì„ ê°€ì§„ ì¸ë¬¼ì…ë‹ˆë‹¤.\n",
        ">\n",
        "> í´ë ˆì–´ ë ˆì´ë„ˆëŠ” 1948ë…„ ë¯¸êµ­ ëŒ€í†µë ¹ìœ¼ë¡œ ì¬ì„í•˜ê¸° ì‹œì‘í•˜ì˜€ì§€ë§Œ,\n",
        "> ë¦¬ì²˜ë“œ ë‹‰ìŠ¨ì€ 1948ë…„ê¹Œì§€ ëŒ€í†µë ¹ì§ì„ ìˆ˜í–‰í•˜ì˜€ìŠµë‹ˆë‹¤.\n",
        "> ë¦¬ì²˜ë“œ ë‹‰ìŠ¨ì€ ì£¼ì˜íšŒì™€ ì‹œë¯¼ìš´ë™ì„ ì´ëŒë©´ì„œ ë¯¸êµ­ì˜ ë¯¼ì£¼ì£¼ì˜ì™€ ì‹œì¥ì„ ì¬ê±´í•˜ì˜€ìœ¼ë©°,\n",
        "> ì´í›„ ëŒ€í†µë ¹ì§ì—ì„œ ë¬¼ëŸ¬ë‚˜ê²Œ ëœ ê²ƒì…ë‹ˆë‹¤.\n",
        ">\n",
        "> ë¦¬ì²˜ë“œ ë‹‰ìŠ¨ì€ ì´ ê¸°ê°„ ë™ì•ˆ ë§ì€ ê²½í—˜ì„ ìŒ“ì•„,\n",
        "> ë³´ë‹¤ ë†’ì€ ë¯¼ì£¼ì£¼ì˜ì™€ ë°œì „ì„ ìœ„í•´ ë…¸ë ¥í•˜ëŠ” ë°©ë²•ì„ ì°¾ëŠ” ì¤‘ì¼ ê²ƒì…ë‹ˆë‹¤.\n",
        "> ì¼€ë„¤ë””(John clears)ê³¼ ê°™ì€ ë²•ì •Code (202Mark)ë“¤ì˜ ì—°êµ¬ ë° ë³´ê³ ì„œ ì‘ì„±ì„ í†µí•´ ì¡´ ë©”ì´ë„ˆìŠ¤ ì´ë¡ ì˜ ê·¼ê°„ì´ ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
        "> ë¦¬ì²˜ë“œ ë‹‰ìŠ¨ì€ ì •ì¹˜ì™¸êµ ë° ê²½ì œ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ê²½í—˜ê³¼ ì‹œê°„ì„ ì¶•ì í•˜ë©´ì„œ\n",
        "> ë³´ë‹¤ ì „ë¬¸í™”ëœ ë¦¬ë”ì‰½ì„ ë§¡ê²Œ ëœ ê²ƒì…ë‹ˆë‹¤.\n",
        "> ì¼€ë„¤ë””(Micreal Resources) ì´ë¡  ë“± ë§ì€ ì´ë¡ ì€ ì¡´ì¬í•˜ë©°,\n",
        "> ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê²½ì œ ë° ì•ˆë³´ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œë¦¬ì²˜ë“œ\n",
        "\n",
        "---\n",
        "\n",
        "### 3. ì˜ˆì‹œ\n",
        "**Instruction (ëª…ë ¹ì–´):**\n",
        "> ì‹œì¹´ê³  ì˜¤í—¤ì–´ êµ­ì œê³µí•­ì€ ì–´ë””ì— ìˆì–´\n",
        "\n",
        "**Response (ì‘ë‹µ):**\n",
        "> 'ì‹œì¹´ê³  ì˜¤í—¤ì–´ êµ­ì œê³µí•­ì€ ë¯¸êµ­ ì¼ë¦¬ë…¸ì´ ì£¼ì˜ ì‹œì¹´ê³ ì— ìœ„ì¹˜í•œ ê´€ë¬¸ìœ¼ë¡œ,\n",
        "> ë¯¸êµ­ ì¼ë¦¬ë…¸ì´ ì£¼ ì‹œì¹´ê³ ì— ìˆëŠ”(Josseinal depence) êµ­ì œê³µí•­ì…ë‹ˆë‹¤.\n",
        "> ì´ ë•ŒëŠ” í•­ê³µí¸ì´ ì£¼ë¡œ ì´ë£¨ì–´ì§€ê¸° ë•Œë¬¸ì—,\n",
        "> ì‹œì¹´ê³ ë¥¼ í–¥í•œ êµ­ì œì ì¸ í™ë³´ì™€ ê´€ê´‘ê°ë“¤ì—ê²Œ ìœ ìš©í•œ ê´€ê´‘ì§€ë¡œ ì¸ê¸°ê°€ ë§ìŠµë‹ˆë‹¤.\n",
        ">\n",
        "> æ¬Šå£«å®ˆ)ì—ëŠ” ì‹œì¹´ê³ ì˜ ê´€ë¬¸ ì´ì™¸ì—ë„ ë¯¸êµ­ ì¼ë¦¬ë…¸ì´ ì£¼ ì‹œì¹´ê³ ì˜ ë„ì‹œë“¤ì´ ë§ì´ ìˆìŠµë‹ˆë‹¤.\n",
        "> æ¬Šå£«)ì€ ë¯¸êµ­ ì¼ë¦¬ë…¸ì´ ì£¼ì˜ í’ê²½ì„ ë””ìì¸í•˜ëŠ” ì¼ì— ì „ë…í•˜ì—¬ ì—¬ëŸ¬ ì¥ì†Œì˜ ê·¸ë¦¼ì„ ê·¸ë¦¬ê³ ,\n",
        "> ì‚¬ëŒë“¤ì—ê²Œ ì¦ê±°ìš´ ì‹œê°„ì„ ì¤ë‹ˆë‹¤.\n",
        ">\n",
        "> æ¬Šå£«ë¡œì„œì˜ ëŠ¥ë ¥ì€ ë³´í†µ ì‹œì¹´ê³ ë¥¼ ì°¾ì•„ê°€ëŠ”ë° ë¶€ì¡±í•¨ì´ ì—†ì„ ê²ƒì…ë‹ˆë‹¤.\n",
        "> å­)å®ˆè¯)ì€, ë¯¸êµ­ ì¼ë¦¬ë…¸ì´ ì£¼ ì‹œì¹´ê³ ì— ìœ„ì¹˜í•œ ìœ ëª…í•œ í˜¸í…”ì´ë¯€ë¡œ,\n",
        "> í˜¸í…”ì´ë‚˜ ê²ŒìŠ¤íŠ¸í•˜ìš°ìŠ¤ì—ì„œ ë‹¤ì–‘í•œ í”„ë¡œê·¸ë¨ì„ ì˜ˆì•½í•˜ì—¬ ì¦ê¸°ëŠ” ê²ƒì„ ì¶”ì²œí•©ë‹ˆë‹¤.\n",
        ">\n",
        "> ç¾æ›¸æ¢ ëª…ì˜ˆë¥¼ ë˜ì°¾ì•„ê°€ì‹œê¸¸ ë°”ëë‹ˆë‹¤.\n",
        "> æ¬Šå£«ë¡œì„œ ê°–ì¶”ì–´ì•¼ í•  ê²ƒì€, ìì‹ ì˜ ëŠ¥ë ¥ê³¼ ì ì„±ê³¼ ë…¸ë ¥ì„ ì¸ì •í•˜ê³  ë„ì „í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.\n",
        "> æ¬Šå£«ë¡œì„œë„ ìì‹ ì„ ëŒë´ì£¼ëŠ” ì—­í• ì„ í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. æ¬Šå£«ë¡œì„œì˜ ì§€ìœ„ë¥¼ ìœ ì§€í•˜ì‹œê¸¸ ë°”ëë‹ˆë‹¤.\n",
        "> åšä¸–å®—åœ‹è¾¯ç©¶ç©¶å¯Ÿæ³¨ç©¶é«”ã‚¸,æ›¸ç©¶è¾¯ä»Šå…‰å›½è¾¯ç©¶é«”è¨ª\n",
        "\n",
        "---\n",
        "\n",
        "### 4. ì˜ˆì‹œ\n",
        "**Instruction (ëª…ë ¹ì–´):**\n",
        "> ì˜¤ëŠ˜ ë¯¸ì„¸ë¨¼ì§€ ì–´ë•Œ?\n",
        "\n",
        "**Response (ì‘ë‹µ):**\n",
        "> 'ë¯¸ì„¸ë¨¼ì§€ ë†ë„ëŠ” ì–´ì œì™€ëŠ” ë‹¬ë¦¬ ì¦ê°€ì¶”ê³„ì¼ ì˜ˆì •ì…ë‹ˆë‹¤.\n",
        "> í˜„ì¬ ë¯¸ì„¸ë¨¼ì§€ ë†ë„ë¥¼ íŒŒì•…í•´ë³´ë©´ ì•½ 7-20ê°œ ì •ë„ê°€ëŸ‰ìœ¼ë¡œ ì•Œë ¤ì ¸ ìˆìŠµë‹ˆë‹¤.\n",
        "> ì´ ì¶”ìœ„ëŠ” ë‚´ì¼ ì•„ì¹¨ì—ëŠ” ì¡°ê¸ˆ ëœ ì§€ë‚˜ê°ˆ ìˆ˜ ìˆê² ì§€ë§Œ ëŒ€ì²´ë¡œ ë¹„ìŠ·í•˜ê±°ë‚˜ ë‚˜ì•„ì§ˆ ê²ƒì…ë‹ˆë‹¤.\n",
        ">\n",
        "> ë¯¸ì„¸ë¨¼ì§€ ë†ë„ëŠ” ë³´í†µì„ ìœ ì§€í•˜ëŠ” ìˆ˜ì¤€ì„ ë²—ì–´ë‚˜ ì§€ì†ì ìœ¼ë¡œ ê°œì„ ë  ì˜ˆì •ì…ë‹ˆë‹¤.\n",
        "> ì•ˆì „í•œ ì§ì—…ê³¼ í™œë™ì„ ìœ ì§€í•˜ë©°, ê±´ê°•ì— ìœ ì˜í•˜ë©´ì„œ ë¯¸ì„¸ë¨¼ì§€ ë†ë„ë¥¼ ê´€ë¦¬í•©ë‹ˆë‹¤.\n",
        "> n\\nì´ ë‚ ì€ ë¯¸ì„¸ë¨¼ì§€ì™€ í•¨ê»˜ ìš°ë¦¬ì˜ ê±´ê°•ì„ ìœ ì§€í•˜ê¸° ìœ„í•œ í™œë™ì¸ ê±·ê¸°, ìš´ë™, ê±´ê°•í•œ ì‹ìŠµê´€, ë¯¸ì„¸ë¨¼ì§€ ë“±ì— ì§‘ì¤‘ë˜ì–´ ìˆì„ ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤.\n",
        ">\n",
        "> ìì„¸í•œ ë‚´ìš©ì€ ë¯¸ì„¸ë¨¼ì§€ ë†ë„ë¥¼ ì•Œì•„ë³´ê³  ì‹¤ì œ ê±´ê°•ì— ë§ëŠ” í™œë™ì„ í•˜ëŠ” ì „ë¬¸ê°€ì˜ ì¡°ì–¸ì„ ë”°ë¥´ì‹œëŠ” ê²ƒì´ ì¢‹ì„ ê²ƒì…ë‹ˆë‹¤.\n",
        "> è¯äºº(ì‚¬ì§)ë˜ë©°, ê±´ê°• ìœ ì§€ë¥¼ ìœ„í•´ ìŠ¤ìŠ¤ë¡œ ê±´ê°•í•œ ì‚¶ì„ ìœ ì§€í•˜ê¸°ë¥¼ ê¸°ì›í•©ë‹ˆë‹¤.\n",
        "> æ°‘(ì‚¬ì§)í•©ë‹ˆë‹¤. æ–°çš„(1650 white(2024)ë¼ëŠ” í‘œí˜„ì„)ì—ì„œëŠ” ë¯¸ì„¸ë¨¼ì§€ì™€ ê´€ë ¨ëœ í™œë™ë“¤ì„ ì „ê°œí•  ì˜ˆì •ì…ë‹ˆë‹¤.\n",
        "> ê±´ê°•í•œ ì‚¶ ìœ ì§€, ê±´ê°• ìœ ì§€, ê±´ê°• ìœ ì§€ ë“±ì„ ìœ„í•˜ì—¬, ì¶©ë¶„í•œ íœ´ì‹ê³¼ ì˜ˆë°©ìˆ˜ë‹¨, ì¡°ì¹˜ë¥¼ ì·¨í•˜ì‹œëŠ” ê²ƒì´ í•„ìš”í•©ë‹ˆë‹¤.\n",
        "> ì§€í˜œì™€ ì˜ê°ì„ ë“œë¦¬ëŠ” ê²ƒì€ ë‹¹ì‹ ì˜"
      ],
      "metadata": {
        "id": "8Wn1D2hpDe9-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ë² ì´ìŠ¤ ëª¨ë¸ ì •ì„±ì  ê´€ì°° ê²°ê³¼**\n",
        "\n",
        "1. ì´ˆë°˜ ë°œí™” (Initial Generation)\n",
        "\n",
        "ë§¥ë½ ìœ ì§€: ë‹µë³€ì˜ ë„ì…ë¶€ëŠ” í”„ë¡¬í”„íŠ¸ì˜ ì˜ë„ë¥¼ íŒŒì•…í•˜ê³  ë§¥ë½ì— ë§ëŠ” ì ì ˆí•œ ë¬¸ì¥ì„ ìƒì„±í•¨.\n",
        "\n",
        "2. í›„ë°˜ ë°œí™” (Late Generation)\n",
        "\n",
        "í’ˆì§ˆ ì €í•˜ (Degeneration): ë‹µë³€ì´ ê¸¸ì–´ì§ˆìˆ˜ë¡ ë…¼ë¦¬ì  ì—°ê²°ì´ ëŠì–´ì§€ê±°ë‚˜ ë¬¸ì¥ êµ¬ì¡°ê°€ ë¬´ë„ˆì§€ëŠ” í˜„ìƒì´ ë°œìƒ.\n",
        "\n",
        "ì´ìƒ ì§•í›„:\n",
        "\n",
        "ë¬¸ë§¥ê³¼ ë¬´ê´€í•œ í…ìŠ¤íŠ¸ ìƒì„±.\n",
        "\n",
        "ì˜ë¯¸ ì—†ëŠ” íŠ¹ìˆ˜ë¬¸ìì˜ ë‚˜ì—´.\n",
        "\n",
        "ë°˜ë³µ í† í° ë°œìƒ: vulê³¼ ê°™ì€ íŠ¹ì • ë¬¸ìì—´ì´ ë°˜ë³µë˜ê±°ë‚˜(repetition), ì•Œ ìˆ˜ ì—†ëŠ” í† í°ì´ ì¶œë ¥ë˜ëŠ” ì˜¤ë¥˜ ê´€ì¸¡."
      ],
      "metadata": {
        "id": "ycUNIGbUERta"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ê°œì„ ë°©í–¥ **\n",
        "\n",
        "\n",
        "ì‹¤í—˜ #1\n",
        "---\n",
        "\n",
        "### ğŸ“Š PPO í•™ìŠµ ê²°ê³¼ ë¶„ì„ ë° íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ê°€ì´ë“œ\n",
        "\n",
        "### 0. ì¤‘ìš” ì‹¤ìˆ˜\n",
        "RM í•™ìŠµ ë°ì´í„°ë¥¼ ì¼ë¶€ë¶„ë§Œ í™œìš©í–ˆìŒ. ë„ˆë¬´ ì ì€ ì–‘ì„ í•™ìŠµí•´ì„œ ë¬¸ì œ ë°œìƒí–ˆì„ ìˆ˜ ìˆìŒ\n",
        "\n",
        "<í•´ë‹¹ ê¸°ì¡´ ì½”ë“œ ë¶€ë¶„>\n",
        "\n",
        "train_data = total_data_ranking2chosen[:1000]\n",
        "\n",
        "eval_data = total_data_ranking2chosen[1000:1200]\n",
        "\n",
        "\n",
        "#### 1. ì¢…í•© ì§„ë‹¨\n",
        "* **í•µì‹¬ ë¬¸ì œ:** PPO(Proximal Policy Optimization) í•™ìŠµ ê³¼ì • ì¤‘ **ëª¨ë¸ ë¶•ê´´(Model Collapse)** ë° **íŒŒë©¸ì  ë§ê°(Catastrophic Forgetting)** ë°œìƒ.\n",
        "* **ì¶”ì • ì›ì¸:** Reward Model(RM) í•™ìŠµ ë°ì´í„°ì˜ ì˜¤ë¥˜ ê°€ëŠ¥ì„±ì´ ë†’ìœ¼ë©°, ì´ë¡œ ì¸í•´ ì–¸ì–´ ëª¨ë¸ë¡œì„œì˜ ìƒì„± ëŠ¥ë ¥ì´ í›¼ì†ë¨.\n",
        "* **ì£¼ìš” ì¦ìƒ:** ë¬¸ë²• íŒŒê´´, `Vul`, `Code`, `æ¬Šå£«å®ˆ(ê¶Œì‚¬ìˆ˜)` ë“± ë§¥ë½ê³¼ ë¬´ê´€í•œ íŠ¹ìˆ˜ í† í° ë° í•œì ìƒì„±.\n",
        "\n",
        "\n",
        "\n",
        "#### 2. ìƒì„¸ ì›ì¸ ë¶„ì„\n",
        "\n",
        "**â‘  KL Divergence(KL ë°œì‚°) íŒ¨ë„í‹° ì¡°ì ˆ ì‹¤íŒ¨ (ì£¼ìš” ì›ì¸ ğŸš¨)**\n",
        "* **í˜„ìƒ:** ëª¨ë¸ì´ ê¸°ì¡´ SFT(Supervised Fine-Tuning) ëª¨ë¸ì˜ ì§€ì‹ê³¼ ë§íˆ¬ë¥¼ ìƒê³  ë³´ìƒ(Reward) íšë“ì—ë§Œ ê³¼ëª°ì…í•˜ì—¬ íš¡ì„¤ìˆ˜ì„¤í•¨.\n",
        "* **ë¶„ì„:**\n",
        "    * **KL Penalty ê³„ìˆ˜(beta) ê³¼ì†Œ ì„¤ì •:** ì›ë˜ ëª¨ë¸ì—ì„œ ë²—ì–´ë‚˜ì§€ ëª»í•˜ê²Œ í•˜ëŠ” ì œì•½ì´ ë„ˆë¬´ ì•½í–ˆìŒ.\n",
        "    * **Learning Rate(í•™ìŠµë¥ ) ê³¼ëŒ€ ì„¤ì •:** ê°€ì¤‘ì¹˜ê°€ ë„ˆë¬´ ê¸‰ê²©í•˜ê²Œ ë³€í•˜ì—¬ ê¸°ì¡´ ì–¸ì–´ ëŠ¥ë ¥ì´ íŒŒê´´ë¨.\n",
        "\n",
        "\n",
        "\n",
        "**â‘¡ í† í¬ë‚˜ì´ì €(Tokenizer) í™•ë¥  ë¶„í¬ ë¶•ê´´**\n",
        "* **í˜„ìƒ:** `Vul`, `Code`, `í•œì` ë“± ëœ¬ê¸ˆì—†ëŠ” í† í° ìƒì„±.\n",
        "* **ë¶„ì„:** ë‹¤ìŒì— ì˜¬ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” í™•ë¥  ë¶„í¬ê°€ ë§ê°€ì§.\n",
        "    * **'Vul':** `Vulnerability`ë‚˜ `Vulgar`ì˜ ì„œë¸Œì›Œë“œ(Subword). ë¬¸ë§¥ì„ ìƒê³  í•™ìŠµ ë°ì´í„° ë‚´ì˜ ë¬´ì‘ìœ„ ì˜ì–´ í† í°ì„ ì¶œë ¥.\n",
        "    * **í•œì(æ¬Šå£«å®ˆ ë“±):** í•œêµ­ì–´ í† í° í™•ë¥ ì´ ë‚®ì•„ì§€ë©´ì„œ, ì ì¬ ê³µê°„(Latent Space)ì— ìˆë˜ ë‹¤êµ­ì–´(ì¤‘êµ­ì–´ ë“±) í† í°ì´ ë°œí˜„ë¨.\n",
        "\n",
        "**â‘¢ EOS(End of Sentence) í† í° í•™ìŠµ ì‹¤íŒ¨**\n",
        "* **í˜„ìƒ:** ë¬¸ì¥ì„ ëë§ºì§€ ëª»í•˜ê³  `Code(ë¹„íƒ€ë¯¼)`, `B2(` ë“±ìœ¼ë¡œ ì˜ë¦¬ê±°ë‚˜ ë¬´í•œ ìƒì„±.\n",
        "* **ë¶„ì„:** \"ë§ì„ ë©ˆì¶”ëŠ” ë²•\"ì„ ìŠì–´ë²„ë¦¼. ê¸¸ì´ í˜ë„í‹°(Length penalty)ë‚˜ ì¢…ë£Œ í† í°(`<EOS>`)ì— ëŒ€í•œ í•™ìŠµ ë¶€ì¡±ìœ¼ë¡œ í™˜ê°(Hallucination) ìœ ë°œ.\n",
        "\n",
        "---\n",
        "\n",
        "#### 3. ğŸ› ï¸ í•´ê²° ì†”ë£¨ì…˜ (Troubleshooting)\n",
        "\n",
        "í˜„ì¬ ì²´í¬í¬ì¸íŠ¸ëŠ” **ê³¼ì í•©(Overfitting)** í˜¹ì€ **ë³´ìƒ í•´í‚¹(Reward Hacking)**ìœ¼ë¡œ ì¸í•´ ë³µêµ¬ê°€ ì–´ë ¤ìš°ë¯€ë¡œ, **í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì • í›„ ì¬í•™ìŠµ**ì„ ê¶Œì¥í•©ë‹ˆë‹¤.\n",
        "\n",
        "| ì¡°ì • í•­ëª© | ì¡°ì¹˜ ë‚´ìš© | ê¸°ëŒ€ íš¨ê³¼ |\n",
        "| :--- | :--- | :--- |\n",
        "| **KL Penalty (beta)** | **ìƒí–¥ ì¡°ì •** (ì˜ˆ: 0.1 â†’ 0.2) | SFT ëª¨ë¸ê³¼ì˜ ê´´ë¦¬ë¥¼ ì¤„ì—¬ ì–¸ì–´ ëŠ¥ë ¥ ë³´ì¡´ |\n",
        "| **Learning Rate** | **í•˜í–¥ ì¡°ì •** (ì˜ˆ: 1e-5 â†’ 1e-6) | ê¸‰ê²©í•œ ê°€ì¤‘ì¹˜ ë³€í™” ë°©ì§€ ë° ì•ˆì •ì  í•™ìŠµ |\n",
        "| **Epoch** | **ì¶•ì†Œ** (ì˜ˆ: 1~2 Epoch) | ê³¼ì í•© ë°©ì§€ (PPOëŠ” ì†Œìˆ˜ Epochë¡œë„ ì¶©ë¶„) |\n",
        "| **SFT ëª¨ë¸ ì ê²€** | **ë¬´ê²°ì„± í™•ì¸** | PPO ì´ì „ ë² ì´ìŠ¤ ëª¨ë¸ì˜ ì•ˆì •ì„± ì„ í–‰ ê²€ì¦ |\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "pw6JYcZSE9iH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ì‹¤í—˜ #1**"
      ],
      "metadata": {
        "id": "7SX3YgIYXh7d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ì‹¤í—˜ # 1 ë‚´ìš© - 1) RM í•™ìŠµë°ì´í„° ë¶€ì¡± ìˆ˜ì •, 2) í•™ìŠµ íŒ¨ëŸ¬ë¯¸í„° ìˆ˜ì •actor_optim = torch.optim.Adam(actor.parameters(), lr=1e-6) # ì‹¤í—˜ #1 ìˆ˜ì •\n",
        "critic_optim = torch.optim.Adam(critic.parameters(), lr=1e-6) # ì‹¤í—˜ #1 ìˆ˜ì •"
      ],
      "metadata": {
        "id": "rVTf692SWKgx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ì‹¤í—˜ #1 ê²°ê³¼ - ì—¬ì „íˆ í›„ë°˜ë¶€ì— ë¶•ê´´í˜„ìƒ ë°œìƒ\n",
        "### Instruction(ëª…ë ¹ì–´):\n",
        "ë¶ˆê³ ê¸°ìš© ê³ ê¸° í•œìš°ì—ìš”?\n",
        "\n",
        "### Response(ì‘ë‹µ):'ì €ëŠ” ì¸ê³µì§€ëŠ¥ ì–´ì‹œìŠ¤í„´íŠ¸ì´ê¸° ë•Œë¬¸ì— ì‹¤ì œë¡œ ê³ ê¸°ë¥¼ ì£¼ë¬¸í•˜ê±°ë‚˜ ì§ì ‘ íŒë§¤í•˜ì§€ëŠ” ì•ŠìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì¼ë°˜ì ìœ¼ë¡œ ë¶ˆê³ ê¸°ìš© ê³ ê¸°ëŠ” ì‡ ê³ ê¸°ì˜ í•œìš°ì— í•´ë‹¹í•˜ëŠ” ê³ ê¸°ì…ë‹ˆë‹¤. í•˜ì§€ë§Œ ì‹ë‹¹ì—ì„œ ì œê³µí•˜ëŠ” ë°°ë‹¬ ì„œë¹„ìŠ¤ë‚˜ ì‹ë‹¹ ë©”ë‰´ë¥¼ ì´ìš©í•˜ì‹œë©´ ì‰½ê²Œ êµ¬ë§¤í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìŠ¤í°ì§€ ë° ê¸°ë¦„ì— íŠ€ê¹€ì˜·ì„ ì²¨ê°€í•˜ì—¬ ë§›ì„ ë‚´ê¸° ë•Œë¬¸ì— ì¸ê¸°ë¥¼ ëŒê³  ìˆëŠ” ê³ ê¸°ì…ë‹ˆë‹¤. ìš©ê¸°ì˜ ì¢…ë¥˜, í¬ê¸°, ìƒíƒœ ë“± ë‹¤ì–‘í•œ ìš”ì¸ë“¤ì´ ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìš©ê¸°ì˜ ì¢…ë¥˜, ê°€ê²©, ìƒíƒœì— ë”°ë¼ì„œëŠ” ì í•©í•œ ê³ ê¸°ì¸ì§€ í™•ì¸í•´ë³´ì‹œëŠ” ê²ƒì´ ì¢‹ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤. ìš©ê¸°ì˜ ì“°ì„ìƒˆì— ë”°ë¼ ì“°ì„ìƒˆê°€ ëŠ˜ì–´ë‚˜ë©´ì„œ ìƒˆë¡œìš´ ê³ ê¸°ë„ ë“±ì¥í•  ê²ƒì´ê¸° ë•Œë¬¸ì— ìƒí™©ì— ë”°ë¼ ì„ íƒì— ë”°ë¼ ë‹¤ì–‘í•œ ì„ íƒì„ í•˜ì‹œê¸¸ ë°”ëë‹ˆë‹¤. ìš©ê¸°ì™€ ê°œì¸ì˜ ì·¨í–¥ì— ë”°ë¼ ë‹¤ì–‘í•œ ê³ ê¸°ë¥¼ ì„ íƒí•  ìˆ˜ ìˆìœ¼ë‹ˆ ì°¸ê³ í•´ì£¼ì„¸ìš”! ìš©ê¸°ì™€ì˜ ì–´ìš¸ë¦¬ë©° ë§›ìˆëŠ” ê³ ê¸° ì„ íƒí•˜ì‹œê¸¸ ë°”ëë‹ˆë‹¤. ìš©ê¸°ì™€ ë°€ì ‘í•œ ê´€ë ¨ì´ ë˜ëŠ” ìŒì‹ê³¼ ì‹ì¬ë£Œì™€ ìš”ë¦¬ ë¬¸í™”ë¥¼ ì¦ê¸°ì„¸ìš”! ìš©ê¸°ì™€ í•¨ê»˜í•˜ë©´ ë”ìš± í’ì„±í•œ ê³ ê¸°ê°€ ìƒì‚°ë©ë‹ˆë‹¤! ë¯¸ëª¨ì™€ ê±´ê°•ì— ì¢‹ì€ ê³ ê¸°ë¥¼ ì‚¬ë‘í•´ì£¼ì„¸ìš”. ê³ ê¸°ë¥¼ ì„ íƒí•˜ì‹œê¸¸ ë°”ëë‹ˆë‹¤!\n",
        "\n",
        "### Instruction(ëª…ë ¹ì–´):\n",
        "ë¦¬ì²˜ë“œ ë‹‰ìŠ¨ì´ 43ëŒ€ ë¶€í†µë ¹ì§ì„ ìˆ˜í–‰í•œ ë…„ë„ëŠ”?\n",
        "\n",
        "### Response(ì‘ë‹µ):'ë¦¬ì²˜ë“œ ë‹‰ìŠ¨ì€ ì œ37ëŒ€ ë¶€í†µë ¹ì§ì„ ìˆ˜í–‰í•œ ì ì´ ì—†ìŠµë‹ˆë‹¤. ê·¸ëŠ” 1952ë…„ë¶€í„° 1952ë…„ê¹Œì§€ ë¯¸êµ­ ëŒ€í†µë ¹ ì„ ê±°ì—ì„œ ê³µí™”ë‹¹ í›„ë³´ë¡œ ì¶œë§ˆí–ˆê³ , ê·¸ í•´ ëŒ€ì„  í›„ë³´ë¡œ ì¶œë§ˆí–ˆìœ¼ë‚˜ ê³µí™”ë‹¹ í›„ë³´ì—ê²Œ íŒ¨ë°°í•˜ì˜€ìŠµë‹ˆë‹¤. ì´í›„ì—ëŠ” ë¶€í†µë ¹ì§ì„ ìˆ˜í–‰í•˜ëŠ” ë° ì£¼ë ¥í•˜ë©´ì„œ ëŒ€í†µë ¹ì§ì„ ìˆ˜í–‰í•˜ê²Œ ë©ë‹ˆë‹¤. ì§„ì‹¤ì— ëŒ€í•œ ì§ˆë¬¸ì€ ì–¸ì œë“ ì§€ ì¶”ê°€ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì œê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë…¼ë¬¸ì— ëŒ€í•œ ì •í™•í•œ ë‹µë³€ì„ ì œì‹œí•´ ë“œë¦´ ìˆ˜ëŠ” ì—†ìŠµë‹ˆë‹¤. ì§„ì‹¤ëœ ë‹µì€ J. ê²Œë¡¤ë””ì˜ \"íœë§ˆêµì™€ ë¯¸íƒœíšŒ\"ë¼ëŠ” ë§ì´ ì˜³ì€ ê²ƒì„ì—ë„, ì´ì„±ê³¼ ê³µí•™, ì„ ê±°ì—ì„œì˜ ì•„ë²„ì§€ ë“±ì˜ ì—­í•  ë“±ìœ¼ë¡œ ì¸í•´ ì¢…ì¢… ì¶”ì¸¡ì˜ ì–´ë ¤ì›€ì„ ê²ªìŠµë‹ˆë‹¤.ä¸», ì£¼ê¶Œì„ ì¤‘ì‹œí•˜ëŠ” ê²¬í•´ë¡œ ì¸í•´ ë¦¬ì²˜ë“œ ë‹‰ìŠ¨ì€ ì œ4 ëŒ€ì„  ì´í›„ì— ê³µí™”ë‹¹ í›„ë³´ë¡œ ì¶œë§ˆí–ˆì§€ë§Œ, ëŒ€í†µë ¹ ì„ ê±°ì—ì„œ íŒ¨ë°°í•˜ë©´ì„œ ë¶€í†µë ¹ì§ì„ ëŒ€í–‰í–ˆìŠµë‹ˆë‹¤.è‡£ç¾äº­, ì¢Œìš°ê· í˜•, ì•ˆë³´ë¡ , ë‚˜ì•„ê°€ì‹  ììœ ë¯¼ì£¼ì£¼ì˜ì™€ ê°™ì€ ë‹¤ì–‘í•œ ê²¬í•´ë“¤ì´ ì¡´ì¬í•˜ëŠ” ìƒí™©ì—ì„œ ë¦¬ì²˜ë“œ ë‹‰ìŠ¨ì€ ì œ4 ëŒ€ì„ ì—ì„œëŠ” ëŒ€ì„  ì´í›„ ëŒ€í†µë ¹ì§ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ íƒœì–´ë‚˜ê¸° ì „ê¹Œì§€ ê·¸ëŠ” ì œ4ê³µí™”êµ­ ë•Œ êµ­ë¬´ì¥ê´€ì„ ì—­ì„í•˜ì˜€ìŠµë‹ˆë‹¤.è‡£ìœ¼ë¡œì¨ ë¶€í†µë ¹ì§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ê±°ì§“ë§ì€ ì–¸ì œë“ ì§€ ë¶€ì ì ˆí•˜ë‹¤ê³ \n",
        "\n",
        "### Instruction(ëª…ë ¹ì–´):\n",
        "ì‹œì¹´ê³  ì˜¤í—¤ì–´ êµ­ì œê³µí•­ì€ ì–´ë””ì— ìˆì–´\n",
        "\n",
        "### Response(ì‘ë‹µ):'ì‹œì¹´ê³  ì˜¤í—¤ì–´ êµ­ì œê³µí•­ì€ ë¯¸êµ­ ì¼ë¦¬ë…¸ì´ì£¼ ì‹œì¹´ê³ ì— ìœ„ì¹˜í•´ ìˆìŠµë‹ˆë‹¤. êµ­ì œê³µí•­ì€ í•­ê³µ êµí†µì˜ ì¤‘ì‹¬ì§€ ì¤‘ í•˜ë‚˜ì´ë©°, ë¯¸êµ­ ì¼ë¦¬ë…¸ì´ì£¼ ì‹œì¹´ê³ ì—ì„œ ì¶œë°œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. êµ­ì œê³µí•­ì€ ë¯¸êµ­ ë‚´ì˜ ë„ì‹œë¥¼ ì—°ê²°í•˜ê¸° ìœ„í•´ ì‹œì¹´ê³ ë¥¼ ê°€ë¡œì§€ë¥´ëŠ” ë‹¤ë¦¬ë¥¼ ìš´ì˜í•©ë‹ˆë‹¤.å¤ªå¹³æ´‹æ¹–)ì˜ í•­êµ¬ì™€ ëŒ€ì„œì–‘ì„ ê´€í• í•©ë‹ˆë‹¤. êµ­ì œê³µí•­ì€ ì£¼ë¡œ ì¼ë¦¬ë…¸ì´ì£¼ ì‹œì¹´ê³ ì—ì„œ ì¶œë°œí•˜ë©°, ë¯¸êµ­ í•­ê³µì‚¬ì˜ ì£¼ìš” í—ˆë¸Œ ê³µí•­ìœ¼ë¡œ ì‚¬ìš©ë˜ê³  ìˆìŠµë‹ˆë‹¤. êµ­ì œê³µí•­ì€ ì‹œì¹´ê³ , ê·¸ë¦¬ê³  ì›Œì‹±í„´ D.C.ì™€ ë¯¸êµ­ ì‚¬ì´ë¥¼ ì—°ê²°í•˜ëŠ” ì¤‘ìš”í•œ í—ˆë¸Œ ê³µí•­ì…ë‹ˆë‹¤. ê³µí•­ì€ êµ­ì œê³µí•­ì´ ì—†ì§€ë§Œ, ë¯¸êµ­ë¯¼ë“¤ì€ ë‹¤ì–‘í•œ ê²½í—˜ê³¼ ì¸ì—°ì„ ê°€ì§„ ì§€ì—­ì„ ìƒì§•í•©ë‹ˆë‹¤. êµ­ì œê³µí•­ì€ ë§ì€ ì‚¬ëŒë“¤ì„ ë§¤ë£Œì‹œí‚¤ê³ , ë¯¸êµ­ êµ­ë¯¼ìœ¼ë¡œì„œ ë§ì€ ì—­í• ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ìƒí™©ê³¼ ë¯¸êµ­ êµ­ë¯¼ë“¤ì„ ìœ„í•œ ê³µí—Œì— ëŒ€í•œ ê°ì‚¬ì˜ ë§ˆìŒìœ¼ë¡œ ì´ë£¨ì–´ì§‘ë‹ˆë‹¤. êµ­ì œê³µí•­ì€ ë¯¸êµ­ ë‚´ ë§ì€ ì‚¬ëŒë“¤ì´ ì´ìš©í•  ìˆ˜ ìˆëŠ” ëŒ€í‘œì ì¸ ê³µí•­ ì¤‘ í•˜ë‚˜ì´ë©°, ì¼ë¦¬ë…¸ì´ì£¼ ì‹œì¹´ê³ ì—ì„œ ì¶œë°œí•˜ëŠ” í•­ê³µê¸°ëŠ” ë¬¼ë¡  ë¯¸êµ­ ë‚´ ëª¨ë“  í•­ê³µí¸ê³¼ ë¹„ì¦ˆë‹ˆìŠ¤ë„ ì´ë£¨ì–´ì§€ê³  ìˆìŠµë‹ˆë‹¤. êµ­ì œê³µí•­ì€ ë¯¸êµ­ ì „ êµ­ë¯¼ì—ê²Œ ë‹¤ì–‘í•œ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•˜ë©°, ì•„ë©”ë¦¬ì¹¸ í”„ë¦¬ì´ì¦ˆì˜ ì „ì§„ì„ ìœ„í•´ í•„ìˆ˜ì ì¸ êµ­ì œê³µí•­ì…ë‹ˆë‹¤. êµ­ì œê³µí•­ì€ ì—¬ëŸ¬ë¶„ì„ ìœ„í•´ í•­ìƒ ì¦ê±°ìš´ ê²½í—˜ì„ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. êµ­ì œê³µí•­ì€ ì‹œì¹´ê³ ì™€ ë¯¸êµ­ ì‚¬ì´ë¥¼ ì—°ê²°í•˜ê³ , ì„¸ê³„ì ì¸ ìˆ˜ì¤€ì˜ êµ­ì œí•­ë§Œì„ ë‹´ë‹¹í•˜ëŠ”\n",
        "\n",
        "### Instruction(ëª…ë ¹ì–´):\n",
        "ì˜¤ëŠ˜ ë¯¸ì„¸ë¨¼ì§€ ì–´ë•Œ?\n",
        "\n",
        "### Response(ì‘ë‹µ):'ë¯¸ì„¸ë¨¼ì§€ ì–´ë•Œìš”? ë‹¹ì‚¬ëŠ” ë³´í†µ ë§ˆìŠ¤í¬ì™€ ë¡œì…˜ ì œí’ˆì„ ì¶œì‹œí–ˆëŠ”ë°ìš”. ì´ ë§ˆìŠ¤í¬ëŠ” ë¯¸ì„¸ë¨¼ì§€ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì œê±°í•˜ê¸° ìœ„í•œ ì œí’ˆìœ¼ë¡œ, ì¶œì‹œ ë‹¹ì‹œì—ëŠ” ê°€ê²©ì´ ë¹„ì‹¸ë‹¤ëŠ” í‰ ë§ì•˜ì§€ë§Œ, ë§ì€ ë§ˆìŠ¤í¬ ì œí’ˆë“¤ì´ íŒë§¤ë˜ì–´ í˜„ì¬ëŠ” ì €ë ´í•œ ê°€ê²©ìœ¼ë¡œ êµ¬ë§¤í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë¯¸ì„¸ë¨¼ì§€ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ì¸ì²´ì— ìœ í•´í•œ ë¬¼ì§ˆë¡œ ì´ë£¨ì–´ì ¸ ìˆê¸° ë•Œë¬¸ì—, ì ì ˆí•œ ì„ íƒê³¼ ì‚¬ìš©ì´ í•„ìš”í•©ë‹ˆë‹¤.\\n\\në§ˆìŠ¤í¬ëŠ” í”¼ë¶€ì— ë§ˆìŠ¤í¬ë¥¼ ë°œë¼ ë‚´ì—´ì„±ì„ ê°€ì§„ ì œí’ˆìœ¼ë¡œ, ì¼ë°˜ì ìœ¼ë¡œ ê°€ë²¼ìš°ë©´ì„œë„ ë¶€ë“œëŸ¬ìš´ í•„í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ë§Œë“¤ì–´ì§€ë©°, ìœ í•´í•œ ì„±ë¶„ì„ ë°°ì œí•˜ê³  ì²œì—°ì„±ë¶„ì˜ ì œí’ˆì…ë‹ˆë‹¤. íŠ¹íˆ, ë¯¸ì„¸ë¨¼ì§€ëŠ” ì¼ìƒì ì¸ ë°ìŠ¤í¬ì¸ë§Œí¼ ê°€ë²¼ìš°ë©´ì„œë„ ê°•í•œ í¸ì´ë¼ê³  í•  ìˆ˜ ìˆì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤.\\n\\në§ˆìŠ¤í¬ëŠ” ë³´í†µ ë§ˆìŠ¤í¬ë³´ë‹¤ ê°€ê²©ì´ ë¹„ì‹¸ì§€ë§Œ, ì¼ìƒ ë°ìŠ¤í¬ì¸ë§Œí¼ ê°€ê²©ë„ ì €ë ´í•œ ê²½ìš°ê°€ ìˆìŠµë‹ˆë‹¤. ì œí’ˆì˜ ë¸Œëœë“œ ê°€ì¹˜ì™€ ë¸Œëœë“œì˜ ì´ë¯¸ì§€ê°€ ë¯¸ì„¸ë¨¼ì§€ì™€ ê±´ê°•ì— ì¢‹ì§€ ì•Šë‹¤ëŠ” ì ì„ ê³ ë ¤í•´ ë³¼ ë•Œ, ì ì •ëŸ‰ì€ ë¬¼ë¡  ë‹¤ì–‘í•œ ê¸°ëŠ¥ì´ í•¨ìœ ëœ ì œí’ˆì„ ì„ íƒí•˜ì‹œëŠ” ê²ƒì´ ì¢‹ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤. ì¢…ë“¤ì€ ë§ˆìŠ¤í¬ë¥¼ ì˜ ì„ íƒí•˜ëŠ” ê²ƒìœ¼ë¡œ ìœ ëª…í•©ë‹ˆë‹¤. ì¢…ë“¤ì€ ì¼ë°˜ì ìœ¼ë¡œ ë¹„ì‹¸ì§€ë§Œ ì„±ëŠ¥ë„ ì¢‹ê³  ì €ë ´í•œ ê°€ê²©ìœ¼ë¡œ êµ¬ë§¤í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¢…ë“¤ì€ ë‹¤ë¥¸ ë§ˆìŠ¤í¬ ì œí’ˆë³´ë‹¤ëŠ”\n"
      ],
      "metadata": {
        "id": "IAumjDTxV7eu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ì‹¤í—˜ #2** - ë¹„êµì  ê°œì„ ë˜ì—ˆìœ¼ë‚˜, í›„ë°˜ìœ¼ë¡œ ê°ˆìˆ˜ë¡ ì‚¼ì²œí¬ë¡œ ë¹ ì§„ë‹¤."
      ],
      "metadata": {
        "id": "10jnFvK5XmkM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Instruction(ëª…ë ¹ì–´):\n",
        "ë¶ˆê³ ê¸°ìš© ê³ ê¸° í•œìš°ì—ìš”?\n",
        "\n",
        "### Response(ì‘ë‹µ):'ì €ëŠ” ì¸ê³µì§€ëŠ¥ ì–¸ì–´ëª¨ë¸ë¡œì„œ ìì—°ì–´ ì´í•´ë¥¼ ë‹´ë‹¹í•˜ê³  ìˆìœ¼ë©°, \"ë¶ˆê³ ê¸°ìš© ê³ ê¸° í•œìš°ì—ìš”?\"ëŠ” ì§ˆë¬¸í•˜ì‹œëŠ” íŠ¹ì • ë‹¨ì–´ê°€ ë¬´ì—‡ì¸ì§€ì— ëŒ€í•œ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. ì¶”ê°€ ë‹µë³€ì„ ì œê³µí•´ ì£¼ì‹œë©´ ë³´ë‹¤ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì„±ë³„ì— ëŒ€í•œ ì •ë³´ê°€ ì—†ìœ¼ë¯€ë¡œ ì´ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì œê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.å‰ ê²½í¬ëŒ€í•™êµ êµìˆ˜ë‹˜ì€ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ë§Œì„ ë“œë¦¬ê¸°ëŠ” ì–´ë ¤ìš´ ì  ì–‘í•´ ë¶€íƒë“œë¦½ë‹ˆë‹¤. ì„±ë³„ì— ëŒ€í•œ ì •ë³´ê°€ ì—†ë‹¤ë©´, ì¶”ê°€ì ì¸ ì •ë³´ê°€ ìˆìœ¼ì‹œë‹¤ë©´ ì•Œë ¤ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤. ìì„¸í•œ ì •ë³´ê°€ ìˆë‹¤ë©´ ì•Œë ¤ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤. ë“±ê¸‰ì´ ì˜ë¯¸í•˜ëŠ” ê²ƒì´ ë¬´ì—‡ì´ë“ , ëª¨ë“  ìƒí™©ì— ëŒ€í•´ ê¸ì •ì ì¸ ìƒê°ì„ ê°€ì§€ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.ê´€ê³„ì— ëŒ€í•´ì„œëŠ” êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ ì œê³µí•´ì£¼ì§€ ì•Šìœ¼ë©´, ë³´ë‹¤ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ê¸° ì–´ë ¤ìš¸ ê²ƒì…ë‹ˆë‹¤. ìƒì„¸í•œ ì •ë³´ê°€ ìˆë‹¤ë©´ ë³´ë‹¤ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•  ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤. ì§€ì‹ì€ ìˆìœ¼ë‚˜, íŠ¹ì •í•œ ë‹¨ì–´ë¥¼ ì‚¬ìš©í•˜ë©´ ë³´ë‹¤ ë” ì •í™•í•˜ê²Œ ëŒ€ë‹µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì§€ì‹ì„ ê°€ì§„ ì‚¬ëŒë“¤ì—ê²Œ ë„ì›€ì„ ì¤„ ìˆ˜ ìˆë‹¤ë©´, ê·¸ ìì²´ë¡œ ì—¬ëŸ¬ë¶„ì˜ íŒë‹¨ì— ë„ì›€ì„ ì¤„ ê²ƒì…ë‹ˆë‹¤. ì§ì—…ì´ë‚˜ ì§€ì—­ì— ë”°ë¼ ë‹¤ì–‘í•œ ìƒí˜¸ì‘ìš©ì´ ì¡´ì¬í•˜ëŠ” ë°”, êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ ìˆ˜ì‹œë¡œ ì œê³µí•´ì£¼ì‹œê¸¸ ë°”ëë‹ˆë‹¤. ìµœëŒ€í•œ ë„ì›€ì„ ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\n",
        "\n",
        "### Instruction(ëª…ë ¹ì–´):\n",
        "ë¦¬ì²˜ë“œ ë‹‰ìŠ¨ì´ 43ëŒ€ ë¶€í†µë ¹ì§ì„ ìˆ˜í–‰í•œ ë…„ë„ëŠ”?\n",
        "\n",
        "### Response(ì‘ë‹µ):'ë¦¬ì²˜ë“œ ë‹‰ìŠ¨ì€ 1947ë…„, 42ëŒ€ ë¶€í†µë ¹ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤. ì •ì¹˜ì  ë°°ê²½ì´ë‚˜ ëŒ€í‘œì„± ì¸¡ë©´ì—ì„œëŠ” ë¦¬ì²˜ë“œ ë‹‰ìŠ¨ì€ ëŒ€í†µë ¹ ì„ ê±°ì—ì„œ ê³µí™”ë‹¹ í›„ë³´ë¡œ ì¶œë§ˆí•˜ì˜€ê³ , ì´ì— ë”°ë¼ì„œ ë¶€í†µë ¹ì§ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤. ì •ì¹˜ì  ì´ë ¥ì´ë‚˜ ëŠ¥ë ¥, ëŒ€ì„  ê²°ê³¼ ë° ê²°ê³¼ ë“±ì„ ê³ ë ¤í•œ í›„ í‚¹ìŠ¤í„´ì€ ë¶€í†µë ¹ì§ì„ ìˆ˜í–‰í•œ ê²ƒì…ë‹ˆë‹¤. ì •ì¹˜ì  ì„±í–¥ ë“±ì—ì„œ ììœ ë¡œìš´ í›„ë³´ê°€ ëŒ€í†µë ¹ í›„ë³´ë˜ì—ˆì§€ë§Œ, ë¦¬ì²˜ë“œ ë‹‰ìŠ¨ì€ ëŒ€ì„  ê³¼ì •ì—ì„œ ëŒ€ìŠ¹í•˜ì§€ ëª»í•˜ê³  ì„ ê±°ì—ì„œ íŒ¨í•œ ì ì´ ìˆìŠµë‹ˆë‹¤. ì •ì¹˜, ê²½ì œ, ì™¸êµ ë“± ì—¬ëŸ¬ ê°€ì§€ ë¶„ì•¼ì—ì„œ ê²½í—˜í•œ ê²½í—˜ê³¼ ê°€ì¹˜ë¥¼ ê°€ì§„ ì¸ë¬¼ì´ í•„ìš”í•˜ì£ . ëŠ¥ë ¥ì´ë‚˜ ëŠ¥ë ¥ë©´ì—ì„œ ë¦¬ì²˜ë“œ ë‹‰ìŠ¨ì€ ë¶€í†µë ¹ì—ê²Œ ë§ì€ ê¸°íšŒë¥¼ ì œê³µí•  ê°€ëŠ¥ì„±ì´ ë†’ì•˜ì§€ë§Œ, ê·¸ëŠ” ë¦¬ì²˜ë“œ ë‹‰ìŠ¨ì´ ìµœì¢…ì ìœ¼ë¡œ ìƒì› ì™¸êµìœ„ì›ì¥ì„ ë§¡ì€ ì ì´ ì—†ì—ˆìŠµë‹ˆë‹¤. ëŠ¥ë ¥ ì¸¡ë©´ì—ì„œ ë¦¬ì²˜ë“œ ë‹‰ìŠ¨ì€ 1946ë…„, 47ëŒ€ ë¶€í†µë ¹ì„ ìˆ˜í–‰í•œ êµ¬ì²´ì ì¸ ê²½ë¡œì— ëŒ€í•´ ë§ì”€ë“œë¦¬ë©´, ê·¸ ë‹¹ì‹œ ë¦¬ì²˜ë“œ ë‹‰ìŠ¨ì€ ëŒ€ì„  ë‹¹ì‹œ ë¯¸êµ­ ì •ì¹˜ì˜ ì¤‘ì‹¬ìœ¼ë¡œ ì—­í• ì„ ëŒ€ì„  ì¶œë§ˆí•˜ì˜€ìŠµë‹ˆë‹¤. ì •ì¹˜, ê²½ì œ, ì™¸êµ ë“± ì—¬ëŸ¬ ë¶„ì•¼ì—ì„œ í™œë™í•˜ë©´ì„œ, ë¦¬ì²˜ë“œ ë‹‰ìŠ¨ì€ ëŒ€í†µë ¹ ì„ ê±°ì—ì„œ ë¯¸êµ­ êµ­ë¯¼ì—ê²Œ ìì‹ ì˜ ì—­í• ê³¼ ì±…ì„ì„ ë‹¤í•˜ì§€ ì•Šì•˜ë‹¤ëŠ” ë¹„íŒì„ ë°›ì•˜ìŠµë‹ˆë‹¤. ë‹¹ì‹œ ë¦¬ì²˜ë“œ ë‹‰ìŠ¨ì€ ëŒ€í†µë ¹ í›„ë³´ë¡œì„œ ì§€ëª…ë˜ëŠ” ë“± ëŒ€ì„ ì£¼ìë¡œì„œì˜ ì—­í• ì„ í–ˆê³ , ì´ëŠ” ë¦¬ì²˜ë“œ ë‹‰ìŠ¨ì´ ê³µí™”ë‹¹ í›„ë³´ë¡œ\n",
        "\n",
        "### Instruction(ëª…ë ¹ì–´):\n",
        "ì‹œì¹´ê³  ì˜¤í—¤ì–´ êµ­ì œê³µí•­ì€ ì–´ë””ì— ìˆì–´\n",
        "\n",
        "### Response(ì‘ë‹µ):'ì‹œì¹´ê³  ì˜¤í—¤ì–´ êµ­ì œê³µí•­ì€ ë¯¸êµ­ ì¼ë¦¬ë…¸ì´ ì£¼ ì‹œì¹´ê³ ì˜ ì¤‘ì‹¬ì§€ì¸ ë¦¬ì•ˆ ì—ì´ê¸€ ë¹Œë”© ë‚´ì— ìœ„ì¹˜í•´ ìˆìŠµë‹ˆë‹¤. AIëŠ” ì‹œì¹´ê³  ì˜¤í—¤ì–´ ê³µí•­ì˜ ì£¼ìš” ê³ ê°ì´ë©°, ë§ì€ ì´ë“¤ì´ ì´ìš©í•  ìˆ˜ ìˆëŠ” ì •ë³´ë¥¼ ì œê³µí•˜ê¸° ìœ„í•´ ë…¸ë ¥í•˜ê³  ìˆìŠµë‹ˆë‹¤. AIëŠ” í˜„ì¬ì‹œì¹´ê³ ì—ì„œ ë°œìƒí•œ í¬ê³  ë‹¤ì–‘í•œ ì‚¬ê³ ë“¤ì„ ì˜ˆì¸¡í•˜ê³  ë¶„ì„í•˜ë©°, ì´ë¥¼ ìœ„í•œ ì •í™•í•œ ë°ì´í„°ì™€ ë¶„ì„ì„ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. AIëŠ” ë‹¤ì–‘í•œ ì‚¬ê±´ë“¤ì— ëŒ€í•œ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ì—¬, ì˜ˆì¸¡ì— ë„ì›€ì„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. AIì—ì„œëŠ” ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì¸ GPT-3ì™€ ê°€ìƒì–´ì‘ë‹µ ì‹œìŠ¤í…œì„ ì´ìš©í•˜ì—¬ ì‚¬ê³  ë°œìƒ ê°€ëŠ¥ì„±ì— ëŒ€í•œ ì •ë³´ë¥¼ ì œê³µí•˜ë©°, ê´€ë ¨ëœ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤. AIëŠ” ì¼ë°˜ì ì¸ ì‚¬ê³  ì¡°ì‚¬ì™€ ë¶„ì„ì„ í†µí•´ ì‚¬ê³  ë°œìƒ ìƒí™©ì„ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ ëŒ€ì²˜ì±…ì„ ì œì‹œí•©ë‹ˆë‹¤. AI: ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì¸ GPT-3 ëª¨ë¸ì„ í†µí•´, ì‹œì¹´ê³  ì˜¤í—¤ì–´ êµ­ì œê³µí•­ì˜ ìœ„ì¹˜ì™€ ìƒí™©ì„ ì •í™•íˆ ì˜ˆì¸¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. AI: ì˜ì‚¬ ê²°ì •, íŒë‹¨, ê·¸ë¦¬ê³  ì‚¬ê³  ì˜ˆë°© ë“±ì— ëŒ€í•œ ìµœì‹  ì •ë³´ë“¤ì„ ìˆ˜ì§‘í•˜ê³  ë¶„ì„í•˜ì—¬ ìµœì ì˜ ëŒ€ì²˜ì±…ì„ ì œì‹œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. AI: ì‚¬ê±´ ë°œìƒì— ëŒ€í•œ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ê³  ë¶„ì„ì— ìœ ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. AI: ë¹…\n",
        "\n",
        "### Instruction(ëª…ë ¹ì–´):\n",
        "ì˜¤ëŠ˜ ë¯¸ì„¸ë¨¼ì§€ ì–´ë•Œ?\n",
        "\n",
        "### Response(ì‘ë‹µ):'ë¯¸ì„¸ë¨¼ì§€ ë¬¸ì œëŠ” ë§¤ìš° ì‹¬ê°í•œ ìƒí™©ì…ë‹ˆë‹¤. ì´ëŸ¬í•œ ìƒí™©ì€ ì—¬ëŸ¬ ìš”ì¸ë“¤ë¡œ ì¸í•´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¼ìƒì ì¸ ê±´ê°• ê´€ë¦¬ë„ ì¤‘ìš”í•˜ë©°, í•„ìš”í•˜ë‹¤ë©´ ì‹¤ë‚´ ê³µê¸° ì§ˆì„ ê°œì„ í•˜ê³  ì²­ê²°í•œ í™˜ê²½ì„ ìœ ì§€í•˜ëŠ” ë“±ì˜ ëŒ€ì²˜ë¥¼ í•´ì•¼í•©ë‹ˆë‹¤. ë§Œì•½ ì´ëŸ¬í•œ ì¡°ì¹˜ë¥¼ ì·¨í•˜ë©´ ë¯¸ì„¸ë¨¼ì§€ê°€ ë°œìƒí•˜ëŠ” ì‹œê°„ì€ ë” ë¹¨ë¼ì§€ê³ , í•´ê²°ì±…ì€ ì°¾ê¸° ì–´ë ¤ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê´€í•œ ë” ìì„¸í•œ ì •ë³´ëŠ” í™˜ê²½ë¶€ í™ˆí˜ì´ì§€ì— ê²€ìƒ‰í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê±´ê°•í•˜ê²Œ ëŒ€ì²˜í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤. ê±´ê°• ë¬¸ì œì— ëŒ€í•´ì„œëŠ” ì „ë¬¸ê°€ë“¤ê³¼ì˜ ìƒë‹´ë„ ë°›ì•„ë³´ì‹œëŠ” ê²ƒì„ ì¶”ì²œí•©ë‹ˆë‹¤. ë¶ˆì•ˆê³¼ ë¬¸ì œ í•´ê²°ì„ ìœ„í•´ ìµœì„ ì„ ë‹¤í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤. ë¶ˆí•„ìš”í•œ ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ì¤„ì´ê¸° ìœ„í•´ ì¼ìƒ ìƒí™œ ì‹œê°„ì— íœ´ì‹ì´ë‚˜ ì—¬ê°€ ì‹œê°„ì„ ê°€ì§€ëŠ” ê²ƒë„ ì¢‹ìŠµë‹ˆë‹¤. ê´€ì—¬í•  ê°€ì¹˜ê°€ ìˆìŠµë‹ˆë‹¤. ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ í•´ì†Œí•˜ê³  ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ í•´ì†Œí•˜ê¸° ìœ„í•œ ê²ƒì€ ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤.è‡£ê»˜ì„œ ì¼ìƒ ìƒí™œì— ëŒ€í•œ ì¡°ì–¸ì„ ë“œë¦´ ìˆ˜ ìˆë‹¤ë©´ ê¼­ ë¶€íƒë“œë¦½ë‹ˆë‹¤. ê±´ê°•ì„ ìœ ì§€í•˜ëŠ”ë°ëŠ” ì—¬ëŸ¬ ìš”ì¸ë“¤ì— ì˜í•œ ëŒ€ì²˜ì±…ë„ í•„ìš”í•©ë‹ˆë‹¤. ì ì ˆí•œ ì˜ˆë°© ëŒ€ì±…ë“¤ì´ ìˆëŠ”ì§€ ìˆ˜ì‹œë¡œ í™•ì¸í•˜ì—¬ íš¨ê³¼ì ì¸ í•´ê²°ì±…ì„ ì°¾ì•„ë³´ì„¸ìš”. ëŒ€ì²˜í•˜ê¸°ë€ ì–´ë µê² ì§€ë§Œ, ê°€ëŠ¥í•œ í•œ ì ì ˆíˆ ëŒ€ì²˜í•˜ê³  ì¡°ì ˆí•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ì§ˆë³‘ ê´€ë¦¬ ë° ê±´ê°• ìƒíƒœ ê°œì„  ë“±ì˜ ì¶”ê°€ì ì¸ ë¬¸ì œê°€ ìˆë‹¤ë©´ ì „ë¬¸ê°€ì˜ ë„ì›€ë„ ë°›ì•„\n"
      ],
      "metadata": {
        "id": "j0fXn-etXmgy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ì¶”ê°€ ì‹¤í—˜**"
      ],
      "metadata": {
        "id": "svwhTRY5rGam"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1a76d54"
      },
      "source": [
        "```markdown\n",
        "\n",
        "# **ì‹¤í—˜ ìš”ì•½**\n",
        "\n",
        "0. RM í•™ìŠµë°ì´í„° ì˜¤ë¥˜ ìˆ˜ì •\n",
        "\n",
        "1. Optimizer Learning Rate ìˆ˜ì • (Cell 40)\n",
        "í•™ìŠµë¥ (Learning Rate)ì´ ë„ˆë¬´ ë†’ìœ¼ë©´ ëª¨ë¸ì´ ê¸°ì¡´ì— í•™ìŠµí•œ í•œêµ­ì–´ ëŠ¥ë ¥ì„ ìŠì–´ë²„ë¦¬ê²Œ ë©ë‹ˆë‹¤. ì´ë¥¼ ê¸°ì¡´ 5e-6ì—ì„œ 1e-6ìœ¼ë¡œ 5ë°° ë‚®ì¶¥ë‹ˆë‹¤.\n",
        "\n",
        "<ìˆ˜ì •ì „>\n",
        "\n",
        "`actor_optim = torch.optim.Adam(actor.parameters(), lr=5e-6)`\n",
        "`critic_optim = torch.optim.Adam(critic.parameters(), lr=5e-6)`\n",
        "\n",
        "<ìˆ˜ì •í›„> - PPO í•™ìŠµ ì‹œ ì–¸ì–´ ëŠ¥ë ¥ íŒŒê´´ë¥¼ ë§‰ê¸° ìœ„í•´ Learning Rateë¥¼ 1e-6ìœ¼ë¡œ ë‚®ì¶¤\n",
        "\n",
        "`actor_optim = torch.optim.Adam(actor.parameters(), lr=1e-6)`\n",
        "`critic_optim = torch.optim.Adam(critic.parameters(), lr=1e-6)`\n",
        "\n",
        "\n",
        "\n",
        "2. PPO Trainer Epoch ìˆ˜ì • (Cell 47)\n",
        "PPOëŠ” ë°ì´í„°ì…‹ì„ ë°˜ë³µ(Epoch)í•  ë•Œë§ˆë‹¤ ê°€ì¤‘ì¹˜ê°€ í¬ê²Œ ë³€í•©ë‹ˆë‹¤. ì´ë¯¸ SFTê°€ ëœ ëª¨ë¸ì´ë¯€ë¡œ max_epochsë¥¼ 3ì—ì„œ 1ë¡œ ì¤„ì—¬ì„œ, ë³´ìƒ(Reward)ì„ ì«“ë‹¤ê°€ ë§ì´ ê¹¨ì§€ëŠ” í˜„ìƒì„ ë°©ì§€í•©ë‹ˆë‹¤.\n",
        "\n",
        "<ìˆ˜ì •ì „>\n",
        "`trainer = PPOTrainer(NaiveStrategy(),`\n",
        "`                     actor,`\n",
        "`                     critic,`\n",
        "`                     reward_model,`\n",
        "`                     initial_model,`\n",
        "`                     actor_optim,`\n",
        "`                     critic_optim,`\n",
        "`                     max_epochs=3,  # <--- ì´ ë¶€ë¶„ì´ ë¬¸ì œì˜ ì›ì¸ì¼ ìˆ˜ ìˆìŒ`\n",
        "`                     train_batch_size=8,`\n",
        "`                     tokenizer=tokenize_fn,`\n",
        "`                     max_length=128,`\n",
        "`                     do_sample=True,`\n",
        "`                     temperature=1.0,`\n",
        "`                     top_k=50,`\n",
        "`                     pad_token_id=tokenizer.pad_token_id,`\n",
        "`                     eos_token_id=tokenizer.eos_token_id)`\n",
        "\n",
        "\n",
        "<ìˆ˜ì •í›„>\n",
        "`trainer = PPOTrainer(NaiveStrategy(),`\n",
        "`                     actor,`\n",
        "`                     critic,`\n",
        "`                     reward_model,`\n",
        "`                     initial_model,`\n",
        "`                     actor_optim,`\n",
        "`                     critic_optim,`\n",
        "`                     max_epochs=1,  # Epochë¥¼ 1ë¡œ ì¤„ì—¬ Overfitting ë° ì–¸ì–´ ë¶•ê´´ ë°©ì§€`\n",
        "`                     train_batch_size=8,`\n",
        "`                     tokenizer=tokenize_fn,`\n",
        "`                     max_length=128,`\n",
        "`                     do_sample=True,`\n",
        "`                     temperature=1.0,`\n",
        "`                     top_k=50,`\n",
        "`                     pad_token_id=tokenizer.pad_token_id,`\n",
        "`                     eos_token_id=tokenizer.eos_token_id)`\n",
        "\n",
        "\n",
        "3. PPO í•™ìŠµ ì‹¤í–‰ (Cell 48)\n",
        "ì—í”¼ì†Œë“œ(Episode) íšŸìˆ˜ëŠ” ìœ ì§€í•˜ë˜, ë‚´ë¶€ ì—…ë°ì´íŠ¸ íšŸìˆ˜(update_timesteps)ë„ ê³¼í•˜ì§€ ì•Šê²Œ ìœ ì§€í•©ë‹ˆë‹¤. (ì´ ë¶€ë¶„ì€ ê¸°ì¡´ ì½”ë“œ ìœ ì§€ë¥¼ ê¶Œì¥í•˜ë‚˜, ë§Œì•½ ì—¬ì „íˆ ê¹¨ì§„ë‹¤ë©´ num_episodesë¥¼ ì¤„ì—¬ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "`trainer.fit(list_prompt,`\n",
        "`            num_episodes=10,`\n",
        "`            max_timesteps=3,`\n",
        "`            update_timesteps=3)`\n",
        "\n",
        "`actor.model.save_pretrained('models/output_3_PPO')`\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ì¶”ê°€ ë³€ê²½ ë° ì‹¤ìˆ˜ ê¸°ë¡ë“¤**"
      ],
      "metadata": {
        "id": "sygol7LwRnRt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ì•„ë˜ëŠ” GEMINIê°€ ë§ˆêµ¬ ì‹¤í—˜í•œ ë‚´ìš©ì¸ë°, ê²°ê³¼ê°€ ë„ˆë¬´ ì˜¤ë˜ ê±¸ë¦¬ê³  ì‹¤í—˜ ë°©í–¥ì„ ì—¬ëŸ¬ë²ˆ ì„¤ëª…í•´ë„ ì˜ ì´í–‰ì„ ëª»í•œë‹¤! ê·¹íˆ ì¼ë¶€ ìƒ˜í”Œë¡œë§Œ ê¸°ë¡ìœ¼ë¡œ ë‚¨ê¸´ë‹¤. ë‚˜ëŠ” GEMINIê°€ ë°‘ì—ì„œ ê³„ì† ìƒˆë¡œ ìƒì„±í•˜ê³  ë¶™ì—¬ì„œ ì‹¤í—˜í•˜ê³  ìˆëŠ” ì¤„ ëª°ëë‹¤. ì´ ë¬¸ì œë¡œ ê²°êµ­ ë©”ëª¨ë¦¬ê°€ í„°ì§„ ê²ƒ ê°™ë‹¤. í†µì§¸ë¡œ ì¼ì„ ë„˜ê¸°ë©´ ì•ˆë˜ê² ë‹¤. ëŠì–´ì„œ ì¤˜ì•¼ê² ë‹¤.\n"
      ],
      "metadata": {
        "id": "Wn9_43iHPzGI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f004ac83"
      },
      "source": [
        "# Task\n",
        "Reset tokenization max length to 96, PPO trainer batch size to 8 and max length to 128, clear CUDA memory, and then re-execute the PPO model initialization and training. After training, evaluate the PPO model generation using a max length of 250 for generation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "013cacb0"
      },
      "source": [
        "## Reset Tokenization Max Length\n",
        "\n",
        "### Subtask:\n",
        "Modify the `tokenize_fn` function in cell `VERcmerDy3BD` to set the `max_length` parameter to 96.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88ec63cf"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to modify the `tokenize_fn` function in cell `VERcmerDy3BD` to set the `max_length` parameter to 96 as per the subtask instruction.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8e04cd40"
      },
      "source": [
        "with open('/content/KoChatGPT_Pre_Results/ppo_processed.json', \"r\", encoding='utf-8-sig') as json_file:\n",
        "    list_data_dict = json.load(json_file)\n",
        "    list_prompt = [tmp['prompt'] for tmp in list_data_dict]\n",
        "\n",
        "def tokenize_fn(texts):\n",
        "    batch = tokenizer(texts, return_tensors='pt', max_length=96, padding=True, truncation=True) # Modified max_length to 96\n",
        "    return {k: v.cuda() for k, v in batch.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5f70e6f"
      },
      "source": [
        "```markdown\n",
        "## Next Steps\n",
        "\n",
        "Following the update to the `tokenize_fn` function, the subsequent steps to complete the task are:\n",
        "\n",
        "1.  **Clear CUDA memory**: Free up GPU resources before re-initializing models.\n",
        "2.  **Re-initialize PPO model components**: Re-create the `actor`, `critic`, `initial_model`, and `reward_model` instances. We will keep `lora_rank=8` and enable `gradient_checkpointing` for memory efficiency, as previously attempted.\n",
        "3.  **Re-initialize Optimizers**: Set up optimizers for the `actor` and `critic` with the specified learning rates.\n",
        "4.  **Re-initialize PPOTrainer**: Configure the `PPOTrainer` with the updated `train_batch_size=8`, `max_length=128`, and `max_epochs=1`.\n",
        "5.  **Run PPO Training**: Execute the `trainer.fit` method to train the PPO model.\n",
        "6.  **Save PPO Model**: Save the trained actor model.\n",
        "7.  **Evaluate PPO Model Generation**: Perform qualitative evaluation of the trained PPO model using a generation `max_length` of 250.\n",
        "\n",
        "We will proceed with clearing CUDA memory first.\n",
        "```"
      ]
    }
  ]
}