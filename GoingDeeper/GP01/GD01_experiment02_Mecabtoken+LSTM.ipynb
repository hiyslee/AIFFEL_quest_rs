{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "base modelê³¼ ë¹„êµê°€ ì‰½ë„ë¡, ì–´ë–¤ ë¶€ë¶„ì„ ì§€ì› ëŠ”ì§€ ë³„ë„ë¡œ í‘œì‹œí–ˆìŒ. (ë” ë³µì¡í•˜ë ¤ë‚˜ìš”~ã…œ)"
      ],
      "metadata": {
        "id": "xB4r8apchSoz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================================\n",
        "# ğŸ”§ MECAB í† í¬ë‚˜ì´ì € ì‚¬ìš© ë²„ì „ (SentencePiece ì œê±°)\n",
        "# ================================================================================\n",
        "# ì›ë³¸ ë…¸íŠ¸ë¶ê³¼ ë™ì¼í•œ ì½”ë“œ ë¸”ë¡ êµ¬ì¡°ë¡œ ì‘ì„±\n",
        "# âŒ ì œê±°, âš ï¸ ìˆ˜ì •, âœ… ì¶”ê°€/ìœ ì§€\n",
        "\n",
        "# ================================================================================\n",
        "# âœ… Cell 0: Mecab ì„¤ì¹˜ (ë™ì¼ - ìœ ì§€)\n",
        "# ================================================================================\n",
        "!pip install konlpy\n",
        "!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\n",
        "%cd Mecab-ko-for-Google-Colab/\n",
        "!bash install_mecab-ko_on_colab_light_220429.sh\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZpKspZz-B67B",
        "outputId": "686cdf6a-1dad-482c-c33d-f75f2067dc6b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading jpype1-1.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from konlpy) (5.4.0)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.12/dist-packages (from konlpy) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from JPype1>=0.7.0->konlpy) (25.0)\n",
            "Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m100.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jpype1-1.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (495 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m495.9/495.9 kB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.6.0 konlpy-0.6.0\n",
            "Cloning into 'Mecab-ko-for-Google-Colab'...\n",
            "remote: Enumerating objects: 138, done.\u001b[K\n",
            "remote: Counting objects: 100% (47/47), done.\u001b[K\n",
            "remote: Compressing objects: 100% (38/38), done.\u001b[K\n",
            "remote: Total 138 (delta 26), reused 22 (delta 8), pack-reused 91 (from 1)\u001b[K\n",
            "Receiving objects: 100% (138/138), 1.72 MiB | 27.02 MiB/s, done.\n",
            "Resolving deltas: 100% (65/65), done.\n",
            "/content/Mecab-ko-for-Google-Colab\n",
            "Installing konlpy.....\n",
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.12/dist-packages (0.6.0)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from konlpy) (1.6.0)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from konlpy) (5.4.0)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.12/dist-packages (from konlpy) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from JPype1>=0.7.0->konlpy) (25.0)\n",
            "Done\n",
            "Installing mecab-0.996-ko-0.9.2.tar.gz.....\n",
            "Downloading mecab-0.996-ko-0.9.2.tar.gz.......\n",
            "from https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz\n",
            "--2025-11-13 12:05:14--  https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz\n",
            "Resolving bitbucket.org (bitbucket.org)... 104.192.142.25, 104.192.142.24, 104.192.142.26, ...\n",
            "Connecting to bitbucket.org (bitbucket.org)|104.192.142.25|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://bbuseruploads.s3.amazonaws.com/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz?response-content-disposition=attachment%3B%20filename%3D%22mecab-0.996-ko-0.9.2.tar.gz%22&response-content-encoding=None&AWSAccessKeyId=ASIA6KOSE3BNOY72U2UL&Signature=ZYuHU%2FNF3VSSKtK42pEOxGTEzF0%3D&x-amz-security-token=IQoJb3JpZ2luX2VjEIT%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJGMEQCIFfBAFSpBeuJmqajCq%2BC7%2Bm0AhnIATz7Ve4mDNajz2thAiBba9M28aErKyHjwBdc5jSlSbWfk6GcAkmvUJKloP37FyqnAghNEAAaDDk4NDUyNTEwMTE0NiIMqpH%2BkV8DC5m99gzzKoQCnvjegd7LuGckIEKmXVIUI5MoFsphJdqnXjEVozaI%2FOlJVPPP%2BtzkJqAtSZ35MsM0aJ3pj%2BNzFWXIJl%2FHRyD6uct7FNyMs6QigUv9638p8wphQmifyGw4QPEVrByqj69hFXZ7DGZdkTjaz22%2BWVBdcK7xnVyZe4Cd%2BBX6xT%2FysxWBkKTAO2KkzruVMQS0UX%2FcCywgnOX0DVslpOkObFbRd0%2F6vfMSbXkDOz7kdqknEFKzODI54VmfmmDfpbgSAYAgwNR%2F4d5xHUjDqccE5uZPoYDQ0a3Z%2Bw6nqAeJUbQSN0ybxG33OKTc41nhf5QnZx0NTMxLbEAZpaHMO8nUVYOfkz5OOe4wuZHXyAY6ngHNxEtLrzKScDi2tyBxoxHn2S8jjA0RwTHgP%2FhCIBQCctFP3c3yvxb1LRdoszeFn05E3Ib3Xlfx0Wzf8ScL29fz8cc6ILLwmi%2B56lkPQ%2F6Yq4LYx%2Fe8Xp3k9hpTOSQk2Z5D5vXVTAAP69l6FEiZglzhV%2Fqgq2cj562T8ol9H2X2Qj%2BAVxqMR7OeuVZBu8HrXe8a7a4LLVUkvrpRnNAE7g%3D%3D&Expires=1763037121 [following]\n",
            "--2025-11-13 12:05:14--  https://bbuseruploads.s3.amazonaws.com/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz?response-content-disposition=attachment%3B%20filename%3D%22mecab-0.996-ko-0.9.2.tar.gz%22&response-content-encoding=None&AWSAccessKeyId=ASIA6KOSE3BNOY72U2UL&Signature=ZYuHU%2FNF3VSSKtK42pEOxGTEzF0%3D&x-amz-security-token=IQoJb3JpZ2luX2VjEIT%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJGMEQCIFfBAFSpBeuJmqajCq%2BC7%2Bm0AhnIATz7Ve4mDNajz2thAiBba9M28aErKyHjwBdc5jSlSbWfk6GcAkmvUJKloP37FyqnAghNEAAaDDk4NDUyNTEwMTE0NiIMqpH%2BkV8DC5m99gzzKoQCnvjegd7LuGckIEKmXVIUI5MoFsphJdqnXjEVozaI%2FOlJVPPP%2BtzkJqAtSZ35MsM0aJ3pj%2BNzFWXIJl%2FHRyD6uct7FNyMs6QigUv9638p8wphQmifyGw4QPEVrByqj69hFXZ7DGZdkTjaz22%2BWVBdcK7xnVyZe4Cd%2BBX6xT%2FysxWBkKTAO2KkzruVMQS0UX%2FcCywgnOX0DVslpOkObFbRd0%2F6vfMSbXkDOz7kdqknEFKzODI54VmfmmDfpbgSAYAgwNR%2F4d5xHUjDqccE5uZPoYDQ0a3Z%2Bw6nqAeJUbQSN0ybxG33OKTc41nhf5QnZx0NTMxLbEAZpaHMO8nUVYOfkz5OOe4wuZHXyAY6ngHNxEtLrzKScDi2tyBxoxHn2S8jjA0RwTHgP%2FhCIBQCctFP3c3yvxb1LRdoszeFn05E3Ib3Xlfx0Wzf8ScL29fz8cc6ILLwmi%2B56lkPQ%2F6Yq4LYx%2Fe8Xp3k9hpTOSQk2Z5D5vXVTAAP69l6FEiZglzhV%2Fqgq2cj562T8ol9H2X2Qj%2BAVxqMR7OeuVZBu8HrXe8a7a4LLVUkvrpRnNAE7g%3D%3D&Expires=1763037121\n",
            "Resolving bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)... 3.5.29.103, 16.15.176.16, 16.15.178.255, ...\n",
            "Connecting to bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)|3.5.29.103|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1414979 (1.3M) [application/x-tar]\n",
            "Saving to: â€˜mecab-0.996-ko-0.9.2.tar.gzâ€™\n",
            "\n",
            "mecab-0.996-ko-0.9. 100%[===================>]   1.35M  3.68MB/s    in 0.4s    \n",
            "\n",
            "2025-11-13 12:05:15 (3.68 MB/s) - â€˜mecab-0.996-ko-0.9.2.tar.gzâ€™ saved [1414979/1414979]\n",
            "\n",
            "Done\n",
            "Unpacking mecab-0.996-ko-0.9.2.tar.gz.......\n",
            "Done\n",
            "Change Directory to mecab-0.996-ko-0.9.2.......\n",
            "installing mecab-0.996-ko-0.9.2.tar.gz........\n",
            "configure\n",
            "make\n",
            "make check\n",
            "make install\n",
            "ldconfig\n",
            "Done\n",
            "Change Directory to /content\n",
            "Downloading mecab-ko-dic-2.1.1-20180720.tar.gz.......\n",
            "from https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz\n",
            "--2025-11-13 12:06:50--  https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz\n",
            "Resolving bitbucket.org (bitbucket.org)... 104.192.142.26, 104.192.142.25, 104.192.142.24, ...\n",
            "Connecting to bitbucket.org (bitbucket.org)|104.192.142.26|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://bbuseruploads.s3.amazonaws.com/a4fcd83e-34f1-454e-a6ac-c242c7d434d3/downloads/b5a0c703-7b64-45ed-a2d7-180e962710b6/mecab-ko-dic-2.1.1-20180720.tar.gz?response-content-disposition=attachment%3B%20filename%3D%22mecab-ko-dic-2.1.1-20180720.tar.gz%22&response-content-encoding=None&AWSAccessKeyId=ASIA6KOSE3BNIRVAZIWW&Signature=8nbzcDFW4rykid5oDpSh3bIgohU%3D&x-amz-security-token=IQoJb3JpZ2luX2VjEIT%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIGQ%2F7TbS4ygx%2BkPa9HK6Hd15PeTR40%2BTzR%2BT7DJBEm4hAiEApUX4hNf4jkHGOoo87SCnYvle9ErSoXqScwOqVAdoo8kqpwIITRAAGgw5ODQ1MjUxMDExNDYiDLWgM7HL8Do%2F1D%2FYxSqEAiKq3jtg6kycvK1AWpER%2F0CpUeOTfC15s3SZlE4IZrpRsEvM7kmgEKQeZpz9SE4VR%2FLWXwFotUBqRl0awGz7gfbCgJ4T%2B5fVB7iK4byMwJ%2BVO%2FkOCy6iwO3YMzZaB6ozkRnodK%2FuLi%2FEivvhuXOVZLfCmSyStw8V%2FMOVyAE%2BOaA2zJP5IFjuFazSkntxSX8QL906nz0YA5q3zNkgN91izRDas58da7rq8luOM2q8%2Blk4HzCR0DrhZPZxbjrWz7EN2llAENz2ZTN34CdpdvU1sWwqJEx1L4H6oqPlAbDYcy1zb2XXaND2pw40W5ctJhUOSFH0skd%2BoRuaELpVpxfFW%2Bf8Cgx8MIKS18gGOp0BzA0BhpRAvfb3xLCuQPMrnzM%2F%2Faq5UYUdgF8Yd5U0fNUUkQgpFWiGhj%2BkOz453M5nb8wsvxVuonnHd5pPfLb15lA%2FGZWjaB6ylEECMnlyoJzxRthLWjB0ymMr8heXU10fhZOqz9zEe2ctVlT2U2wk2DYDQMuu7jzrWI4NIPK6iA1lvTVTyusWg1rmz%2F8jjTdNvb1YppBeTPuleWn3%2Bg%3D%3D&Expires=1763037194 [following]\n",
            "--2025-11-13 12:06:51--  https://bbuseruploads.s3.amazonaws.com/a4fcd83e-34f1-454e-a6ac-c242c7d434d3/downloads/b5a0c703-7b64-45ed-a2d7-180e962710b6/mecab-ko-dic-2.1.1-20180720.tar.gz?response-content-disposition=attachment%3B%20filename%3D%22mecab-ko-dic-2.1.1-20180720.tar.gz%22&response-content-encoding=None&AWSAccessKeyId=ASIA6KOSE3BNIRVAZIWW&Signature=8nbzcDFW4rykid5oDpSh3bIgohU%3D&x-amz-security-token=IQoJb3JpZ2luX2VjEIT%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIGQ%2F7TbS4ygx%2BkPa9HK6Hd15PeTR40%2BTzR%2BT7DJBEm4hAiEApUX4hNf4jkHGOoo87SCnYvle9ErSoXqScwOqVAdoo8kqpwIITRAAGgw5ODQ1MjUxMDExNDYiDLWgM7HL8Do%2F1D%2FYxSqEAiKq3jtg6kycvK1AWpER%2F0CpUeOTfC15s3SZlE4IZrpRsEvM7kmgEKQeZpz9SE4VR%2FLWXwFotUBqRl0awGz7gfbCgJ4T%2B5fVB7iK4byMwJ%2BVO%2FkOCy6iwO3YMzZaB6ozkRnodK%2FuLi%2FEivvhuXOVZLfCmSyStw8V%2FMOVyAE%2BOaA2zJP5IFjuFazSkntxSX8QL906nz0YA5q3zNkgN91izRDas58da7rq8luOM2q8%2Blk4HzCR0DrhZPZxbjrWz7EN2llAENz2ZTN34CdpdvU1sWwqJEx1L4H6oqPlAbDYcy1zb2XXaND2pw40W5ctJhUOSFH0skd%2BoRuaELpVpxfFW%2Bf8Cgx8MIKS18gGOp0BzA0BhpRAvfb3xLCuQPMrnzM%2F%2Faq5UYUdgF8Yd5U0fNUUkQgpFWiGhj%2BkOz453M5nb8wsvxVuonnHd5pPfLb15lA%2FGZWjaB6ylEECMnlyoJzxRthLWjB0ymMr8heXU10fhZOqz9zEe2ctVlT2U2wk2DYDQMuu7jzrWI4NIPK6iA1lvTVTyusWg1rmz%2F8jjTdNvb1YppBeTPuleWn3%2Bg%3D%3D&Expires=1763037194\n",
            "Resolving bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)... 52.216.53.145, 16.15.184.63, 52.217.171.145, ...\n",
            "Connecting to bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)|52.216.53.145|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 49775061 (47M) [application/x-tar]\n",
            "Saving to: â€˜mecab-ko-dic-2.1.1-20180720.tar.gzâ€™\n",
            "\n",
            "mecab-ko-dic-2.1.1- 100%[===================>]  47.47M  39.3MB/s    in 1.2s    \n",
            "\n",
            "2025-11-13 12:06:52 (39.3 MB/s) - â€˜mecab-ko-dic-2.1.1-20180720.tar.gzâ€™ saved [49775061/49775061]\n",
            "\n",
            "Done\n",
            "Unpacking  mecab-ko-dic-2.1.1-20180720.tar.gz.......\n",
            "Done\n",
            "Change Directory to mecab-ko-dic-2.1.1-20180720\n",
            "Done\n",
            "installing........\n",
            "configure\n",
            "make\n",
            "make install\n",
            "bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/v0.6.0/scripts/mecab.sh)\n",
            "https://github.com/konlpy/konlpy/issues/395#issue-1099168405 - 2022.01.11\n",
            "Done\n",
            "Install mecab-python\n",
            "Successfully Installed\n",
            "Now you can use Mecab\n",
            "from konlpy.tag import Mecab\n",
            "mecab = Mecab()\n",
            "ì‚¬ìš©ì ì‚¬ì „ ì¶”ê°€ ë°©ë²• : https://bit.ly/3k0ZH53\n",
            "NameError: name 'Tagger' is not defined ì˜¤ë¥˜ ë°œìƒ ì‹œ ëŸ°íƒ€ì„ì„ ì¬ì‹¤í–‰ í•´ì£¼ì„¸ìš”\n",
            "ë¸”ë¡œê·¸ì— í•´ê²° ë°©ë²•ì„ ë‚¨ê²¨ì£¼ì‹  tanaë‹˜ ê°ì‚¬í•©ë‹ˆë‹¤.\n",
            "light ë²„ì „ ì‘ì„± : Dogdriipë‹˜ ( https://github.com/Dogdriip )\n",
            "ë¬¸ì œë¥¼ í•´ê²°í•´ì£¼ì‹  combacsaë‹˜ ê°ì‚¬í•©ë‹ˆë‹¤.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ì½”ë©ì— ìˆ˜ë™ìœ¼ë¡œ ë°ì´í„° ì—…ë¡œë”© !!!\n",
        "\n",
        "rating_test.txtì™€ rating_train.txtë¥¼ ì½”ë©ì— ë³µì‚¬í•´ì£¼ì„¸ìš”. ì´ê±° ìŠìœ¼ë©´ ì•ˆë¼ìš”. í•œ ì½”ë“œë‚´ì—ì„œ ê¹ƒì—…ì—ì„œ ì¤‘ë³µ í´ë¡ í•´ë„ ë¬¸ì œê°€ ì—†ëŠ”ì§€, í•´ê²°ì±…ì„ ì°¾ì§€ ëª»í•´ì„œ ... ì¼ë‹¨ ìˆ˜ë™ìœ¼ë¡œ ë°ì´í„°ë¥¼ ë„£ì—ˆì–´ìš”."
      ],
      "metadata": {
        "id": "R-qA7OTsB_Zs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================================\n",
        "# âœ… Cell 1: ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ (ë™ì¼ - ìœ ì§€)\n",
        "# ================================================================================\n",
        "# ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‹¹ ë‹¤ ëª¨ì•„ë´¤ìŠµë‹ˆë‹¤.\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import konlpy\n",
        "\n",
        "print(torch.__version__)\n",
        "print(np.__version__)\n",
        "print(pd.__version__)\n",
        "print(matplotlib.__version__)\n",
        "print(konlpy.__version__)\n",
        "\n",
        "# âœ… Mecab ë¡œë“œ (ì¶”ê°€)\n",
        "from konlpy.tag import Mecab\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "\n",
        "mecab = Mecab()\n",
        "print(\"\\nâœ… Mecab í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ\")\n",
        "\n",
        "\n",
        "# ================================================================================\n",
        "# âœ… Cell 2: ë°ì´í„° ë¡œë“œ (ë™ì¼ - ìœ ì§€)\n",
        "# ================================================================================\n",
        "train = pd.read_table(\"/content/ratings_train.txt\")\n",
        "test = pd.read_table(\"/content/ratings_test.txt\")\n",
        "\n",
        "print(train.info())\n",
        "train.head()\n",
        "\n",
        "\n",
        "# ================================================================================\n",
        "# âœ… Cell 3: ì „ì²˜ë¦¬ (ë™ì¼ - ìœ ì§€)\n",
        "# ================================================================================\n",
        "# íŒ€ ì „ì²˜ë¦¬ ì½”ë“œ\n",
        "import re\n",
        "\n",
        "# ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸\n",
        "STOPWORDS = ['ì˜','ê°€','ì´','ì€','ë“¤','ëŠ”','ì¢€','ì˜','ê±','ê³¼','ë„','ë¥¼','ìœ¼ë¡œ','ì','ì—','ì™€','í•œ','í•˜ë‹¤']\n",
        "\n",
        "# ë§ì¶¤ë²• ë³€í˜• ì‚¬ì „\n",
        "SPELLING_DICT = {\n",
        "    'êµ³': ['ê¶…', 'êµ³', 'êµ¿'],\n",
        "    'ë¯¸ì³¤': ['ë¯¸ì²«', 'ë¯¸ì³£', 'ë¯¸ì²¬', 'ë¯¸ì³¤', 'ã…ã…Š'],\n",
        "    'ê´œì°®': ['ê´œì°®', 'ê´œì¶˜', 'ê´œì°¬', 'ã„±ã…Š', 'ê° ì°¬', 'ê° ì°®', 'ê´¸ì°¬', 'ê´¸ì°®'],\n",
        "    'ë´¤': ['ë´£'],\n",
        "    'ê² ': ['ê²Ÿ']\n",
        "}\n",
        "\n",
        "# í…ìŠ¤íŠ¸ ì»¬ëŸ¼ëª…\n",
        "TEXT_COL = \"document\"\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•¨ìˆ˜\n",
        "    \"\"\"\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "\n",
        "    text = str(text)\n",
        "\n",
        "    # 1. ë°˜ë³µë˜ëŠ” ë¬¸ì¥ ë¶€í˜¸ ì œê±° (2ê°œ ì´ìƒ â†’ 1ê°œ)\n",
        "    text = re.sub(r'([.!?â€¦;])\\1+', r'\\1', text)\n",
        "\n",
        "    # 2. ë§ì¶¤ë²• ë³€í˜• í†µì¼\n",
        "    for correct, variations in SPELLING_DICT.items():\n",
        "        for variant in variations:\n",
        "            text = text.replace(variant, correct)\n",
        "\n",
        "    # 3. ë°˜ë³µ ë¬¸ì ì œê±° (3ë²ˆ ì´ìƒ ë°˜ë³µ â†’ 2ë²ˆ)\n",
        "    text = re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n",
        "\n",
        "    # 4. ììŒ/ëª¨ìŒ ë‹¨ë… ì œê±° (ì™„ì„±í˜• í•œê¸€ í•„í„°ë§ ì „ì— ë¨¼ì € ì œê±°)\n",
        "    # í•œê¸€ ììŒ: ã„±-ã…, í•œê¸€ ëª¨ìŒ: ã…-ã…£\n",
        "    text = re.sub(r'[ã„±-ã…ã…-ã…£]+', ' ', text)\n",
        "\n",
        "    # 5. ì˜ì–´/ìˆ«ì/í•œê¸€/ë¬¸ì¥ë¶€í˜¸ë§Œ ë‚¨ê¸°ê³  ëª¨ë‘ ì‚­ì œ\n",
        "    # ì´ëª¨ì§€, ì´ëª¨í‹°ì½˜, íŠ¹ìˆ˜ë¬¸ì ìë™ ì œê±°\n",
        "    text = re.sub(r'[^ê°€-í£a-zA-Z0-9\\s.!?,]', ' ', text)\n",
        "\n",
        "    # 6. ë¬¸ì¥ë¶€í˜¸ ì•ë’¤ë¡œ ê³µë°± ì¶”ê°€\n",
        "    text = re.sub(r'([.!?,])', r' \\1 ', text)\n",
        "\n",
        "    # 7. ì˜ì–´ ì†Œë¬¸ì ë³€í™˜\n",
        "    text = text.lower()\n",
        "\n",
        "    # 9. ë¶ˆìš©ì–´ ì œê±° (ì¡°ì‚¬ê°€ ë¶™ì€ ê²½ìš°ë„ ì²˜ë¦¬)\n",
        "    words = text.split()\n",
        "\n",
        "    # ë‹¨ì–´ ëì— ë¶ˆìš©ì–´ê°€ ë¶™ì–´ìˆìœ¼ë©´ ì œê±°\n",
        "    filtered_words = []\n",
        "    for word in words:\n",
        "        # ë‹¨ì–´ ì „ì²´ê°€ ë¶ˆìš©ì–´ì¸ ê²½ìš°\n",
        "        if word in STOPWORDS:\n",
        "            continue\n",
        "        # ë‹¨ì–´ ëì—ì„œ ë¶ˆìš©ì–´ ì œê±° (ê°€ì¥ ê¸´ ê²ƒë¶€í„° ì²´í¬)\n",
        "        # ì˜ˆ: \"í•™êµì—ì„œ\" -> \"í•™êµ\", \"ì˜í™”ëŠ”\" -> \"ì˜í™”\"\n",
        "        found = False\n",
        "        for stopword in sorted(STOPWORDS, key=len, reverse=True):\n",
        "            if len(word) > len(stopword) and word.endswith(stopword):\n",
        "                cleaned = word[:-len(stopword)]\n",
        "                # ë‚¨ì€ ë¶€ë¶„ì´ 2ê¸€ì ì´ìƒì¼ ë•Œë§Œ ì œê±°\n",
        "                # êµ³ì´ -> êµ³, ê°™ì´ -> ê°™  ì´ë ‡ê²Œ ë°”ë€Œì–´ì„œ ê¸¸ì´ ì œí•œ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.\n",
        "                # \"êµ³ì´\" -> \"êµ³\" (X), \"ì˜í™”ëŠ”\" -> \"ì˜í™”\" (O)\n",
        "                if len(cleaned) >= 2:\n",
        "                    filtered_words.append(cleaned)\n",
        "                    found = True\n",
        "                    break\n",
        "        if not found:\n",
        "            filtered_words.append(word)\n",
        "\n",
        "    text = ' '.join(filtered_words)\n",
        "\n",
        "    # 10. ì—°ì† ê³µë°±ì„ í•˜ë‚˜ì˜ ê³µë°±ìœ¼ë¡œ êµì²´\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # 11. ì•ë’¤ ê³µë°± ì œê±°\n",
        "    text = text.strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def preprocess_dataframe(df, text_col=TEXT_COL):\n",
        "    \"\"\"\n",
        "    ë°ì´í„°í”„ë ˆì„ ì „ì²˜ë¦¬ í•¨ìˆ˜\n",
        "    \"\"\"\n",
        "    print(f\"ì „ì²˜ë¦¬ ì „ ë°ì´í„° í¬ê¸°: {len(df)}\")\n",
        "\n",
        "    # 1. ê²°ì¸¡ì¹˜ ì œê±°\n",
        "    df = df.dropna(subset=[text_col])\n",
        "    print(f\"ê²°ì¸¡ì¹˜ ì œê±° í›„: {len(df)}\")\n",
        "\n",
        "    # 2. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì ìš©\n",
        "    df[text_col] = df[text_col].apply(preprocess_text)\n",
        "\n",
        "    # 3. ì „ì²˜ë¦¬ í›„ ë¹ˆ ë¬¸ìì—´ ì œê±°\n",
        "    df = df[df[text_col].str.strip() != '']\n",
        "    print(f\"ë¹ˆ ë¬¸ìì—´ ì œê±° í›„: {len(df)}\")\n",
        "\n",
        "    # 4. ì¤‘ë³µ í–‰ ì œê±°\n",
        "    df = df.drop_duplicates(subset=[text_col])\n",
        "    print(f\"ì¤‘ë³µ ì œê±° í›„: {len(df)}\")\n",
        "\n",
        "    # ì¸ë±ìŠ¤ ì¬ì„¤ì •\n",
        "    df = df.reset_index(drop=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# ìƒ˜í”Œ í…ìŠ¤íŠ¸ë¡œ í…ŒìŠ¤íŠ¸\n",
        "sample_texts = [\n",
        "    \"ì–´ì œ ë³¸ ì˜í™” ì§„ì§œ ì¬ë°Œì—ˆìŒ!!! ë˜ ë³´ê³  ì‹¶ì–´ ğŸ˜‚\",\n",
        "    \"ë‚˜ëŠ” ì˜¤ëŠ˜ ì•„ì¹¨ì— í•™êµì— ê°”ë‹¤. ê·¼ë° ë„ˆë¬´ ì¡¸ë ¸ìŒã…‹ã…‹ã…‹ã…‹\",\n",
        "    \"ë°¥ì€ ë¨¹ì—ˆë‹ˆ?? ì•„ì§ì´ì•¼... ì ì‹¬ì— ê°™ì´ ë¨¹ì!!!\",\n",
        "    \"ë©”ìº… í˜•íƒœì†Œ ë¶„ì„ì€ í•œêµ­ì–´ ì²˜ë¦¬ì—ì„œ ë§ì´ ì‚¬ìš©ë¼ ğŸ‘\",\n",
        "    \"íŒŒì´ì¬ìœ¼ë¡œ í† í° ë¹ˆë„ì™€ í’ˆì‚¬ ë¶„í¬ë¥¼ ì‹œê°í™”í•´ ë³´ì!!!\",\n",
        "    \"ìš”ì¦˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ RG êµ¬ì¶•ì„ ë§ì´ í•´!!\",\n",
        "    \"ì—ì´ì „íŠ¸ëŠ” ì™¸ë¶€ ë„êµ¬ë¥¼ í˜¸ì¶œí•´ ì‘ì—…ì„ ìë™í™”í•  ìˆ˜ ìˆì–´. êµ³!\",\n",
        "    \"ì´ ì˜í™” ì§„ì§œ ë¯¸ì³£ë‹¤!!! ë„ˆë¬´ ì¬ë°ŒìŒã…‹ã…‹ã…‹ã…‹\",\n",
        "    \"ë°°ìš° ì—°ê¸° êµ³ì´ í›Œë¥­í–ˆìŒ, ìŠ¤í† ë¦¬ëŠ” ë´£ì§€ë§Œ...\",\n",
        "    \"ì´ê±´ ã„±ã…Š ì˜í™”ë„¤, êµ¿êµ¿!\"\n",
        "]\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"ìƒ˜í”Œ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ê²°ê³¼\")\n",
        "print(\"=\" * 80)\n",
        "for i, text in enumerate(sample_texts, 1):\n",
        "    processed = preprocess_text(text)\n",
        "    print(f\"\\n[{i}] ì›ë³¸: {text}\")\n",
        "    print(f\"    ê²°ê³¼: {processed}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"ì‹¤ì œ ë°ì´í„° ì ìš© ì˜ˆì‹œ\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "train_processed = preprocess_dataframe(train.copy())\n",
        "test_processed = preprocess_dataframe(test.copy())\n",
        "\n",
        "print(\"\\nì „ì²˜ë¦¬ ì™„ë£Œ!\")\n",
        "print(f\"Train ë°ì´í„°: {len(train_processed)}ê°œ\")\n",
        "print(f\"Test ë°ì´í„°: {len(test_processed)}ê°œ\")\n",
        "\n",
        "print(\"\\nì „ì²˜ë¦¬ ê²°ê³¼ ìƒ˜í”Œ:\")\n",
        "print(train_processed.head(10))\n",
        "\n",
        "\n",
        "# ================================================================================\n",
        "# âŒ Cell 4: SentencePiece ì„¤ì¹˜ [ì œê±°]\n",
        "# ================================================================================\n",
        "# âŒ ì œê±°ë¨: !pip install sentencepiece\n",
        "\n",
        "\n",
        "# ================================================================================\n",
        "# âŒ Cell 5: SentencePiece í•™ìŠµ [ì œê±°]\n",
        "# ================================================================================\n",
        "# âŒ ì œê±°ë¨: SentencePiece í•™ìŠµ ì½”ë“œ\n",
        "\n",
        "\n",
        "# ================================================================================\n",
        "# âŒ Cell 6: SentencePiece ëª¨ë¸ í™œìš© ì˜ˆì‹œ [ì œê±°]\n",
        "# ================================================================================\n",
        "# âŒ ì œê±°ë¨: SentencePiece í…ŒìŠ¤íŠ¸ ì½”ë“œ\n",
        "\n",
        "\n",
        "# ================================================================================\n",
        "# âš ï¸ Cell 7: í† í¬ë‚˜ì´ì§• í•¨ìˆ˜ [ìˆ˜ì •] - Vocab Size 8000 ê³ ì •\n",
        "# ================================================================================\n",
        "\n",
        "# âš ï¸ ìˆ˜ì •: Vocab Size 8000ìœ¼ë¡œ ì œí•œ\n",
        "def mecab_tokenize(mecab, corpus, max_vocab_size=8000):\n",
        "    \"\"\"\n",
        "    Mecab í˜•íƒœì†Œ ë¶„ì„ê¸°ë¡œ í† í¬ë‚˜ì´ì œì´ì…˜\n",
        "\n",
        "    âš ï¸ ìˆ˜ì •: vocab_sizeë¥¼ 8000ìœ¼ë¡œ ê³ ì •í•˜ì—¬ ë©”ëª¨ë¦¬ ìµœì í™”\n",
        "\n",
        "    Args:\n",
        "        mecab: Mecab í† í¬ë‚˜ì´ì € ê°ì²´\n",
        "        corpus: ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸\n",
        "        max_vocab_size: ìµœëŒ€ vocab í¬ê¸° (8000 ê³ ì •)\n",
        "\n",
        "    Returns:\n",
        "        tensor: íŒ¨ë”©ëœ í† í° ID í…ì„œ\n",
        "        word_index: ë‹¨ì–´ -> ì¸ë±ìŠ¤ (ìƒìœ„ 8000ê°œ)\n",
        "        index_word: ì¸ë±ìŠ¤ -> ë‹¨ì–´\n",
        "    \"\"\"\n",
        "    # Step 1: Mecab í˜•íƒœì†Œ ë¶„ì„\n",
        "    all_tokens = []\n",
        "    tokenized_texts = []\n",
        "\n",
        "    print(\"ğŸ”„ Mecab í˜•íƒœì†Œ ë¶„ì„ ì§„í–‰ ì¤‘...\")\n",
        "    for text in tqdm(corpus, desc=\"Mecab tokenizing\"):\n",
        "        tokens = mecab.morphs(text)  # í˜•íƒœì†Œ ë¦¬ìŠ¤íŠ¸\n",
        "        tokenized_texts.append(tokens)\n",
        "        all_tokens.extend(tokens)\n",
        "\n",
        "    # Step 2: ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°\n",
        "    token_counter = Counter(all_tokens)\n",
        "\n",
        "    print(f\"\\nğŸ“Š ì „ì²´ ê³ ìœ  í˜•íƒœì†Œ ìˆ˜: {len(token_counter):,}\")\n",
        "    print(f\"   ìƒìœ„ 10ê°œ: {token_counter.most_common(10)}\\n\")\n",
        "\n",
        "    # âš ï¸ ìˆ˜ì •: ìƒìœ„ 8000ê°œ í˜•íƒœì†Œë§Œ ì„ íƒ\n",
        "    print(f\"âš¡ Vocab Sizeë¥¼ {max_vocab_size:,}ë¡œ ì œí•œ ì¤‘...\")\n",
        "    top_tokens = dict(token_counter.most_common(max_vocab_size - 1))  # <PAD> ì˜ˆì•½\n",
        "\n",
        "    # Step 3: ë‹¨ì–´ ì¸ë±ìŠ¤ ìƒì„± (ìƒìœ„ 8000ê°œë§Œ)\n",
        "    word_index = {}\n",
        "    word_index['<PAD>'] = 0  # ì¸ë±ìŠ¤ 0: íŒ¨ë”©\n",
        "\n",
        "    for idx, (word, count) in enumerate(top_tokens.items(), start=1):\n",
        "        word_index[word] = idx\n",
        "\n",
        "    # Step 4: ì—­ ë§¤í•‘\n",
        "    index_word = {idx: word for word, idx in word_index.items()}\n",
        "\n",
        "    print(f\"âœ… ìµœì¢… Vocab í¬ê¸°: {len(word_index):,}\\n\")\n",
        "\n",
        "    # Step 5: ID ë³€í™˜ (ë¯¸ì§€ì˜ í† í°ì€ 0(PAD)ìœ¼ë¡œ ì²˜ë¦¬)\n",
        "    tensor_list = []\n",
        "    unknown_count = 0\n",
        "    total_tokens = 0\n",
        "\n",
        "    for tokens in tqdm(tokenized_texts, desc=\"Converting to tensor\"):\n",
        "        token_ids = []\n",
        "        for token in tokens:\n",
        "            if token in word_index:\n",
        "                token_ids.append(word_index[token])\n",
        "            else:\n",
        "                # âš ï¸ ë¯¸ì§€ì˜ í† í°ì€ 0(PAD)ìœ¼ë¡œ ì²˜ë¦¬\n",
        "                token_ids.append(0)\n",
        "                unknown_count += 1\n",
        "            total_tokens += 1\n",
        "\n",
        "        tensor_list.append(torch.tensor(token_ids, dtype=torch.long))\n",
        "\n",
        "    # Step 6: íŒ¨ë”©\n",
        "    tensor = pad_sequence(tensor_list, batch_first=True, padding_value=0)\n",
        "\n",
        "    print(f\"\\nâœ… í† í¬ë‚˜ì´ì œì´ì…˜ ì™„ë£Œ!\")\n",
        "    print(f\"   ìµœì¢… í…ì„œ shape: {tensor.shape}\")\n",
        "    print(f\"   OOV (ë¯¸ì§€ í† í°) ë¹„ìœ¨: {unknown_count/total_tokens*100:.2f}% ({unknown_count:,}/{total_tokens:,})\\n\")\n",
        "\n",
        "    return tensor, word_index, index_word\n",
        "\n",
        "\n",
        "# ================================================================================\n",
        "# âš ï¸ Cell 8: DataLoader ìƒì„± í•¨ìˆ˜ [ìˆ˜ì •]\n",
        "# ================================================================================\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "\n",
        "# âš ï¸ ìˆ˜ì •: attention_masks ì œê±°\n",
        "def create_dataloaders(train_input_ids, train_labels,\n",
        "                       test_input_ids, test_labels,\n",
        "                       batch_size=32, val_ratio=0.2):\n",
        "    \"\"\"\n",
        "    Train/Val/Test DataLoader ìƒì„± (Mecab ë²„ì „)\n",
        "\n",
        "    Args:\n",
        "        train_input_ids: í›ˆë ¨ ë°ì´í„° ì…ë ¥ ID í…ì„œ\n",
        "        train_labels: í›ˆë ¨ ë ˆì´ë¸”\n",
        "        test_input_ids: í…ŒìŠ¤íŠ¸ ë°ì´í„° ì…ë ¥ ID í…ì„œ\n",
        "        test_labels: í…ŒìŠ¤íŠ¸ ë ˆì´ë¸”\n",
        "        batch_size: ë°°ì¹˜ í¬ê¸°\n",
        "        val_ratio: validation ë¹„ìœ¨\n",
        "\n",
        "    Returns:\n",
        "        train_loader, val_loader, test_loader\n",
        "    \"\"\"\n",
        "    # Train ë°ì´í„°ì—ì„œ train/val split\n",
        "    train_dataset = TensorDataset(train_input_ids, train_labels)  # âœ… attention_masks ì œê±°\n",
        "    val_size = int(len(train_dataset) * val_ratio)\n",
        "    train_size = len(train_dataset) - val_size\n",
        "    train_split, val_split = random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "    # DataLoader ìƒì„±\n",
        "    train_loader = DataLoader(train_split, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_split, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Test DataLoader\n",
        "    test_dataset = TensorDataset(test_input_ids, test_labels)  # âœ… attention_masks ì œê±°\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    print(f\"âœ… DataLoader ìƒì„± ì™„ë£Œ!\")\n",
        "    print(f\"   Train samples: {train_size:,} â†’ {len(train_loader)} batches\")\n",
        "    print(f\"   Val samples: {val_size:,} â†’ {len(val_loader)} batches\")\n",
        "    print(f\"   Test samples: {len(test_dataset):,} â†’ {len(test_loader)} batches\")\n",
        "    print(f\"   Batch size: {batch_size}\")\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "\n",
        "# âœ… ì¶”ê°€: ê¸¸ì´ ì œí•œ í•¨ìˆ˜\n",
        "def truncate_corpus(corpus, max_length=130):\n",
        "    \"\"\"í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ\"\"\"\n",
        "    truncated_corpus = []\n",
        "    for text in tqdm(corpus, desc=\"Truncating corpus\"):\n",
        "        if len(text) > max_length * 5:\n",
        "            truncated_corpus.append(text[:max_length * 5])\n",
        "        else:\n",
        "            truncated_corpus.append(text)\n",
        "    return truncated_corpus\n",
        "\n",
        "\n",
        "# ë°ì´í„° ì¤€ë¹„\n",
        "corpus = train_processed['document'].tolist()\n",
        "labels = torch.tensor(train_processed['label'].values, dtype=torch.long)\n",
        "\n",
        "print(f\"ğŸ“Š ì›ë³¸ ë°ì´í„°:\")\n",
        "print(f\"   Corpus í¬ê¸°: {len(corpus):,}\")\n",
        "print(f\"   Labels í¬ê¸°: {len(labels):,}\\n\")\n",
        "\n",
        "corpus_truncated = truncate_corpus(corpus, 130)\n",
        "print()\n",
        "\n",
        "# âœ… Mecab í† í¬ë‚˜ì´ì§• (vocab_size=8000 ê³ ì •)\n",
        "train_input_ids, train_word_index, train_index_word = mecab_tokenize(mecab, corpus_truncated, max_vocab_size=8000)\n",
        "print()\n",
        "\n",
        "# Test ë°ì´í„°ë„ ì²˜ë¦¬\n",
        "test_corpus = test_processed['document'].tolist()\n",
        "test_labels = torch.tensor(test_processed['label'].values, dtype=torch.long)\n",
        "\n",
        "test_corpus_truncated = truncate_corpus(test_corpus, 130)\n",
        "test_input_ids, _, _ = mecab_tokenize(mecab, test_corpus_truncated, max_vocab_size=8000)\n",
        "print()\n",
        "\n",
        "# DataLoader ìƒì„±\n",
        "train_loader, val_loader, test_loader = create_dataloaders(\n",
        "    train_input_ids,\n",
        "    labels,\n",
        "    test_input_ids,\n",
        "    test_labels,\n",
        "    batch_size=32,\n",
        "    val_ratio=0.2\n",
        ")\n",
        "\n",
        "# í™•ì¸\n",
        "print(\"=\" * 60)\n",
        "print(\"ğŸ“Š DataLoader ìƒ˜í”Œ í™•ì¸\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# âœ… 2ê°œë§Œ ì–¸íŒ©\n",
        "for batch in train_loader:\n",
        "    x, y = batch  # âœ… 2ê°œë§Œ ì–¸íŒ©\n",
        "    print(f\"Train - ì…ë ¥ í…ì„œ shape: {x.shape}\")\n",
        "    print(f\"Train - ë ˆì´ë¸” shape: {y.shape}\")\n",
        "    break\n",
        "\n",
        "for batch in val_loader:\n",
        "    x, y = batch  # âœ… 2ê°œë§Œ ì–¸íŒ©\n",
        "    print(f\"Val - ì…ë ¥ í…ì„œ shape: {x.shape}\")\n",
        "    print(f\"Val - ë ˆì´ë¸” shape: {y.shape}\")\n",
        "    break\n",
        "\n",
        "for batch in test_loader:\n",
        "    x, y = batch  # âœ… 2ê°œë§Œ ì–¸íŒ©\n",
        "    print(f\"Test - ì…ë ¥ í…ì„œ shape: {x.shape}\")\n",
        "    print(f\"Test - ë ˆì´ë¸” shape: {y.shape}\")\n",
        "    break\n",
        "\n",
        "\n",
        "# ================================================================================\n",
        "# âœ… Cell 9: (ë¹ˆ ì…€ ìœ ì§€)\n",
        "# ================================================================================\n",
        "\n",
        "\n",
        "# ================================================================================\n",
        "# âœ… Cell 10: í•˜ì´í¼íŒŒë¼ë¯¸í„° ì •ì˜ [ìˆ˜ì •]\n",
        "# ================================================================================\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# ê³µí†µ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì •ì˜\n",
        "# âœ… ìˆ˜ì •: VOCAB_SIZE 8000ìœ¼ë¡œ ê³ ì • (ë™ì í™” ì•ˆ í•¨)\n",
        "VOCAB_SIZE = 8000  # âœ… 8000 ê³ ì • (ë©”ëª¨ë¦¬ ìµœì í™”)\n",
        "EMBEDDING_DIM = 100                # ì„ë² ë”© ë²¡í„°ì˜ ì°¨ì›\n",
        "HIDDEN_DIM = 128                   # LSTMì˜ ì€ë‹‰ ìƒíƒœ ì°¨ì›\n",
        "OUTPUT_DIM = 1                     # ì¶œë ¥ ì°¨ì› (ê¸ì •=1, ë¶€ì •=0 -> 1ê°œ)\n",
        "N_LAYERS = 2                       # LSTM ë ˆì´ì–´ ê°œìˆ˜\n",
        "BIDIRECTIONAL = True               # ì–‘ë°©í–¥ RNN/LSTM ì—¬ë¶€\n",
        "DROPOUT_RATE = 0.2                 # ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨\n",
        "PAD_IDX = 0                        # íŒ¨ë”© ì¸ë±ìŠ¤ (0)\n",
        "\n",
        "print(f\"ğŸ“Š ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°:\")\n",
        "print(f\"   VOCAB_SIZE: {VOCAB_SIZE:,} (ê³ ì •)\")\n",
        "\n",
        "\n",
        "# ================================================================================\n",
        "# âœ… Cell 11: LSTM ëª¨ë¸ [ë™ì¼ - ìœ ì§€]\n",
        "# ================================================================================\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers,\n",
        "                 bidirectional, dropout, pad_idx):\n",
        "        super().__init__()\n",
        "\n",
        "        # 1. ì„ë² ë”© ë ˆì´ì–´\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "\n",
        "        # 2. LSTM ë ˆì´ì–´\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim,\n",
        "                           hidden_size=hidden_dim,\n",
        "                           num_layers=n_layers,\n",
        "                           bidirectional=bidirectional,\n",
        "                           batch_first=True,\n",
        "                           dropout=dropout)\n",
        "\n",
        "        # 3. FC ë ˆì´ì–´\n",
        "        fc_input_dim = hidden_dim * 2\n",
        "        self.fc = nn.Linear(fc_input_dim, output_dim)\n",
        "\n",
        "        # 4. ë“œë¡­ì•„ì›ƒ\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text):\n",
        "\n",
        "        embedded = self.embedding(text)\n",
        "\n",
        "        # 2. LSTM\n",
        "        _output, (hidden, cell) = self.lstm(embedded)\n",
        "\n",
        "        # 3. ë§ˆì§€ë§‰ ë ˆì´ì–´ì˜ ì€ë‹‰ ìƒíƒœ ê²°í•© (ì–‘ë°©í–¥ ì²˜ë¦¬)\n",
        "        if self.lstm.bidirectional:\n",
        "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
        "        else:\n",
        "            hidden = self.dropout(hidden[-1,:,:])\n",
        "\n",
        "        # 4. FC ë ˆì´ì–´ í†µê³¼\n",
        "        prediction = self.fc(hidden)\n",
        "\n",
        "        return prediction.squeeze(1)\n",
        "\n",
        "\n",
        "# ================================================================================\n",
        "# âœ… Cell 12: í›ˆë ¨ í•¨ìˆ˜ [ìˆ˜ì •]\n",
        "# ================================================================================\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import time\n",
        "\n",
        "# 0. GPU ì¥ì¹˜ ì„¤ì •\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# 1. í—¬í¼ í•¨ìˆ˜ ì •ì˜\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "def binary_accuracy(preds, y):\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    correct = (rounded_preds == y).float()\n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n",
        "\n",
        "# 2. í›ˆë ¨ í•¨ìˆ˜ ì •ì˜\n",
        "def train(model, iterator, optimizer, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    model.train()\n",
        "    # âœ… ìˆ˜ì •: attention_masks ì œê±°\n",
        "    for texts, labels in iterator:  # âœ… 2ê°œë§Œ ì–¸íŒ©\n",
        "        texts = texts.to(device)\n",
        "        labels = labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(texts)\n",
        "        loss = criterion(predictions, labels.float())\n",
        "        acc = binary_accuracy(predictions, labels)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "# 3. í‰ê°€ í•¨ìˆ˜ ì •ì˜\n",
        "def evaluate(model, iterator, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # âœ… ìˆ˜ì •: attention_masks ì œê±°\n",
        "        for texts, labels in iterator:  # âœ… 2ê°œë§Œ ì–¸íŒ©\n",
        "            texts = texts.to(device)\n",
        "            labels = labels.to(device)\n",
        "            predictions = model(texts)\n",
        "            loss = criterion(predictions, labels.float())\n",
        "            acc = binary_accuracy(predictions, labels)\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "\n",
        "# ================================================================================\n",
        "# âœ… Cell 13: ëª¨ë¸ í›ˆë ¨ [ë™ì¼ - ìœ ì§€]\n",
        "# ================================================================================\n",
        "# ëª¨ë¸ ì„¤ì •\n",
        "lstm_model = LSTMModel(\n",
        "    VOCAB_SIZE,\n",
        "    EMBEDDING_DIM,\n",
        "    HIDDEN_DIM,\n",
        "    OUTPUT_DIM,\n",
        "    N_LAYERS,\n",
        "    BIDIRECTIONAL,\n",
        "    DROPOUT_RATE,\n",
        "    PAD_IDX\n",
        ").to(device)\n",
        "\n",
        "save_path = 'best_model_lstm.pt'\n",
        "N_EPOCHS = 20\n",
        "patience = 5\n",
        "criterion = nn.BCEWithLogitsLoss().to(device)\n",
        "optimizer = optim.Adam(filter(lambda p: p.requires_grad, lstm_model.parameters()), lr=0.0001)\n",
        "\n",
        "# Early stopping ë³€ìˆ˜\n",
        "best_valid_loss = float('inf')\n",
        "patience_counter = 0\n",
        "\n",
        "train_losses, train_accs, valid_losses, valid_accs = [], [], [], []\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"--- LSTM Model Training starts ---\")\n",
        "print(f\"{'='*60}\\n\")\n",
        "\n",
        "# í•™ìŠµ ë£¨í”„\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss, train_acc = train(lstm_model, train_loader, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(lstm_model, val_loader, criterion)\n",
        "\n",
        "    end_time = time.time()\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
        "\n",
        "    # ê¸°ë¡\n",
        "    train_losses.append(train_loss)\n",
        "    train_accs.append(train_acc)\n",
        "    valid_losses.append(valid_loss)\n",
        "    valid_accs.append(valid_acc)\n",
        "\n",
        "    # Early Stopping\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(lstm_model.state_dict(), save_path)\n",
        "        patience_counter = 0\n",
        "        print(f'\\t>> Validation loss improved ({best_valid_loss:.3f}). Model saved.')\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        print(f'\\t>> Validation loss did not improve. Counter: {patience_counter}/{patience}')\n",
        "        if patience_counter >= patience:\n",
        "            print(f'--- Early stopping triggered after {epoch+1} epochs ---')\n",
        "            break\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸ í‰ê°€\n",
        "print(f\"\\n--- Loading best LSTM model for test evaluation ---\")\n",
        "lstm_model.load_state_dict(torch.load(save_path))\n",
        "test_loss, test_acc = evaluate(lstm_model, test_loader, criterion)\n",
        "\n",
        "print(f\"\\n--- LSTM Model Test Results (Best Model) ---\")\n",
        "print(f'\\tTest Loss: {test_loss:.3f}')\n",
        "print(f'\\tTest Acc:  {test_acc*100:.2f}%')\n",
        "\n",
        "# ================================================================================\n",
        "# ğŸ‰ ì½”ë“œ ë\n",
        "# ================================================================================"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zvnQJ4IGEuR",
        "outputId": "e2e815e6-b395-4bd4-f632-1715b0419c91"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.0+cu126\n",
            "2.0.2\n",
            "2.2.2\n",
            "3.10.0\n",
            "0.6.0\n",
            "\n",
            "âœ… Mecab í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 150000 entries, 0 to 149999\n",
            "Data columns (total 3 columns):\n",
            " #   Column    Non-Null Count   Dtype \n",
            "---  ------    --------------   ----- \n",
            " 0   id        150000 non-null  int64 \n",
            " 1   document  149995 non-null  object\n",
            " 2   label     150000 non-null  int64 \n",
            "dtypes: int64(2), object(1)\n",
            "memory usage: 3.4+ MB\n",
            "None\n",
            "================================================================================\n",
            "ìƒ˜í”Œ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ê²°ê³¼\n",
            "================================================================================\n",
            "\n",
            "[1] ì›ë³¸: ì–´ì œ ë³¸ ì˜í™” ì§„ì§œ ì¬ë°Œì—ˆìŒ!!! ë˜ ë³´ê³  ì‹¶ì–´ ğŸ˜‚\n",
            "    ê²°ê³¼: ì–´ì œ ë³¸ ì˜í™” ì§„ì§œ ì¬ë°Œì—ˆìŒ ! ë˜ ë³´ê³  ì‹¶ì–´\n",
            "\n",
            "[2] ì›ë³¸: ë‚˜ëŠ” ì˜¤ëŠ˜ ì•„ì¹¨ì— í•™êµì— ê°”ë‹¤. ê·¼ë° ë„ˆë¬´ ì¡¸ë ¸ìŒã…‹ã…‹ã…‹ã…‹\n",
            "    ê²°ê³¼: ë‚˜ëŠ” ì˜¤ëŠ˜ ì•„ì¹¨ í•™êµ ê°”ë‹¤ . ê·¼ë° ë„ˆë¬´ ì¡¸ë ¸ìŒ\n",
            "\n",
            "[3] ì›ë³¸: ë°¥ì€ ë¨¹ì—ˆë‹ˆ?? ì•„ì§ì´ì•¼... ì ì‹¬ì— ê°™ì´ ë¨¹ì!!!\n",
            "    ê²°ê³¼: ë°¥ì€ ë¨¹ì—ˆë‹ˆ ? ì•„ì§ì´ì•¼ . ì ì‹¬ ê°™ì´ ë¨¹ì !\n",
            "\n",
            "[4] ì›ë³¸: ë©”ìº… í˜•íƒœì†Œ ë¶„ì„ì€ í•œêµ­ì–´ ì²˜ë¦¬ì—ì„œ ë§ì´ ì‚¬ìš©ë¼ ğŸ‘\n",
            "    ê²°ê³¼: ë©”ìº… í˜•íƒœì†Œ ë¶„ì„ í•œêµ­ì–´ ì²˜ë¦¬ì—ì„œ ë§ì´ ì‚¬ìš©ë¼\n",
            "\n",
            "[5] ì›ë³¸: íŒŒì´ì¬ìœ¼ë¡œ í† í° ë¹ˆë„ì™€ í’ˆì‚¬ ë¶„í¬ë¥¼ ì‹œê°í™”í•´ ë³´ì!!!\n",
            "    ê²°ê³¼: íŒŒì´ì¬ í† í° ë¹ˆë„ í’ˆì‚¬ ë¶„í¬ ì‹œê°í™”í•´ ë³´ì !\n",
            "\n",
            "[6] ì›ë³¸: ìš”ì¦˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ RG êµ¬ì¶•ì„ ë§ì´ í•´!!\n",
            "    ê²°ê³¼: ìš”ì¦˜ ì½”ì‚¬ì¸ ìœ ì‚¬ ê¸°ë°˜ ë²¡í„° ê²€ìƒ‰ rg êµ¬ì¶•ì„ ë§ì´ í•´ !\n",
            "\n",
            "[7] ì›ë³¸: ì—ì´ì „íŠ¸ëŠ” ì™¸ë¶€ ë„êµ¬ë¥¼ í˜¸ì¶œí•´ ì‘ì—…ì„ ìë™í™”í•  ìˆ˜ ìˆì–´. êµ³!\n",
            "    ê²°ê³¼: ì—ì´ì „íŠ¸ ì™¸ë¶€ ë„êµ¬ í˜¸ì¶œí•´ ì‘ì—…ì„ ìë™í™”í•  ìˆ˜ ìˆì–´ . êµ³ !\n",
            "\n",
            "[8] ì›ë³¸: ì´ ì˜í™” ì§„ì§œ ë¯¸ì³£ë‹¤!!! ë„ˆë¬´ ì¬ë°ŒìŒã…‹ã…‹ã…‹ã…‹\n",
            "    ê²°ê³¼: ì˜í™” ì§„ì§œ ë¯¸ì³¤ë‹¤ ! ë„ˆë¬´ ì¬ë°ŒìŒ\n",
            "\n",
            "[9] ì›ë³¸: ë°°ìš° ì—°ê¸° êµ³ì´ í›Œë¥­í–ˆìŒ, ìŠ¤í† ë¦¬ëŠ” ë´£ì§€ë§Œ...\n",
            "    ê²°ê³¼: ë°°ìš° ì—°ê¸° êµ³ì´ í›Œë¥­í–ˆìŒ , ìŠ¤í† ë¦¬ ë´¤ì§€ë§Œ .\n",
            "\n",
            "[10] ì›ë³¸: ì´ê±´ ã„±ã…Š ì˜í™”ë„¤, êµ¿êµ¿!\n",
            "    ê²°ê³¼: ì´ê±´ ê´œì°® ì˜í™”ë„¤ , êµ³êµ³ !\n",
            "\n",
            "================================================================================\n",
            "ì‹¤ì œ ë°ì´í„° ì ìš© ì˜ˆì‹œ\n",
            "================================================================================\n",
            "ì „ì²˜ë¦¬ ì „ ë°ì´í„° í¬ê¸°: 150000\n",
            "ê²°ì¸¡ì¹˜ ì œê±° í›„: 149995\n",
            "ë¹ˆ ë¬¸ìì—´ ì œê±° í›„: 149607\n",
            "ì¤‘ë³µ ì œê±° í›„: 144478\n",
            "ì „ì²˜ë¦¬ ì „ ë°ì´í„° í¬ê¸°: 50000\n",
            "ê²°ì¸¡ì¹˜ ì œê±° í›„: 49997\n",
            "ë¹ˆ ë¬¸ìì—´ ì œê±° í›„: 49843\n",
            "ì¤‘ë³µ ì œê±° í›„: 48700\n",
            "\n",
            "ì „ì²˜ë¦¬ ì™„ë£Œ!\n",
            "Train ë°ì´í„°: 144478ê°œ\n",
            "Test ë°ì´í„°: 48700ê°œ\n",
            "\n",
            "ì „ì²˜ë¦¬ ê²°ê³¼ ìƒ˜í”Œ:\n",
            "         id                                           document  label\n",
            "0   9976970                                ì•„ ë”ë¹™ . ì§„ì§œ ì§œì¦ë‚˜ë„¤ìš” ëª©ì†Œë¦¬      0\n",
            "1   3819312                   í  . í¬ìŠ¤í„°ë³´ê³  ì´ˆë”©ì˜í™”ì¤„ . ì˜¤ë²„ì—°ê¸°ì¡°ì°¨ ê°€ë³ì§€ ì•Šêµ¬ë‚˜      1\n",
            "2  10265843                                  ë„ˆë¬´ì¬ë°“ì—ˆë‹¤ê·¸ë˜ì„œë³´ëŠ”ê²ƒì„ì¶”ì²œí•œë‹¤      0\n",
            "3   9045019                      êµë„ì†Œ ì´ì•¼ê¸°êµ¬ë¨¼ . ì†”ì§íˆ ì¬ë¯¸ ì—†ë‹¤ . í‰ì  ì¡°ì •      0\n",
            "4   6483659  ì‚¬ì´ëª¬í˜ê·¸ ìµì‚´ìŠ¤ëŸ° ì—°ê¸° ë‹ë³´ì˜€ë˜ ì˜í™” ! ìŠ¤íŒŒì´ë”ë§¨ì—ì„œ ëŠ™ì–´ë³´ì´ê¸°ë§Œ í–ˆë˜ ì»¤ìŠ¤í‹´ ...      1\n",
            "5   5403919        ë§‰ ê±¸ìŒë§ˆ ë—€ 3ì„¸ë¶€í„° ì´ˆë“±í•™êµ 1í•™ë…„ìƒì¸ 8ì‚´ìš©ì˜í™” . . ë³„ë°˜ê°œ ì•„ê¹Œì›€ .      0\n",
            "6   7797314                              ì›ì‘ ê¸´ì¥ê°ì„ ì œëŒ€ë¡œ ì‚´ë ¤ë‚´ì§€ëª»í–ˆë‹¤ .      0\n",
            "7   9443947  ë³„ ë°˜ê°œ ì•„ê¹ë‹¤ ìš•ë‚˜ì˜¨ë‹¤ ì´ì‘ê²½ ê¸¸ìš©ìš° ì—°ê¸°ìƒí™œì´ëª‡ë…„ì¸ì§€ . ì •ë§ ë°œë¡œí•´ ê·¸ê²ƒë³´ë‹¨ ...      0\n",
            "8   7156791                                ì•¡ì…˜ ì—†ëŠ”ë° ì¬ë¯¸ ìˆëŠ” ëª‡ì•ˆë˜ ì˜í™”      1\n",
            "9   5912145     ì™œì¼€ í‰ì  ë‚®ì€ê±´ë° ? ê½¤ ë³¼ë§Œí•œë° . í—ë¦¬ìš°ë“œì‹ í™”ë ¤í•¨ì—ë§Œ ë„ˆë¬´ ê¸¸ë“¤ì—¬ì ¸ ìˆë‚˜ ?      1\n",
            "ğŸ“Š ì›ë³¸ ë°ì´í„°:\n",
            "   Corpus í¬ê¸°: 144,478\n",
            "   Labels í¬ê¸°: 144,478\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncating corpus: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 144478/144478 [00:00<00:00, 3990942.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ”„ Mecab í˜•íƒœì†Œ ë¶„ì„ ì§„í–‰ ì¤‘...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Mecab tokenizing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 144478/144478 [00:08<00:00, 17929.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ“Š ì „ì²´ ê³ ìœ  í˜•íƒœì†Œ ìˆ˜: 48,645\n",
            "   ìƒìœ„ 10ê°œ: [('.', 130475), ('ì˜í™”', 57208), ('ë‹¤', 53641), ('ê³ ', 47096), ('í•˜', 40099), ('ì´', 35465), ('ì„', 29883), ('ëŠ”', 26938), ('ë³´', 24138), ('ê²Œ', 22199)]\n",
            "\n",
            "âš¡ Vocab Sizeë¥¼ 8,000ë¡œ ì œí•œ ì¤‘...\n",
            "âœ… ìµœì¢… Vocab í¬ê¸°: 8,000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Converting to tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 144478/144478 [00:02<00:00, 70248.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ… í† í¬ë‚˜ì´ì œì´ì…˜ ì™„ë£Œ!\n",
            "   ìµœì¢… í…ì„œ shape: torch.Size([144478, 96])\n",
            "   OOV (ë¯¸ì§€ í† í°) ë¹„ìœ¨: 4.76% (111,719/2,345,133)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncating corpus: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48700/48700 [00:00<00:00, 3568466.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”„ Mecab í˜•íƒœì†Œ ë¶„ì„ ì§„í–‰ ì¤‘...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Mecab tokenizing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48700/48700 [00:02<00:00, 19687.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ“Š ì „ì²´ ê³ ìœ  í˜•íƒœì†Œ ìˆ˜: 29,887\n",
            "   ìƒìœ„ 10ê°œ: [('.', 43474), ('ì˜í™”', 19538), ('ë‹¤', 18184), ('ê³ ', 15643), ('í•˜', 13439), ('ì´', 11872), ('ì„', 9922), ('ëŠ”', 9100), ('ë³´', 8073), ('ê²Œ', 7613)]\n",
            "\n",
            "âš¡ Vocab Sizeë¥¼ 8,000ë¡œ ì œí•œ ì¤‘...\n",
            "âœ… ìµœì¢… Vocab í¬ê¸°: 8,000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Converting to tensor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48700/48700 [00:00<00:00, 97268.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ… í† í¬ë‚˜ì´ì œì´ì…˜ ì™„ë£Œ!\n",
            "   ìµœì¢… í…ì„œ shape: torch.Size([48700, 105])\n",
            "   OOV (ë¯¸ì§€ í† í°) ë¹„ìœ¨: 4.48% (35,264/787,110)\n",
            "\n",
            "\n",
            "âœ… DataLoader ìƒì„± ì™„ë£Œ!\n",
            "   Train samples: 115,583 â†’ 3612 batches\n",
            "   Val samples: 28,895 â†’ 903 batches\n",
            "   Test samples: 48,700 â†’ 1522 batches\n",
            "   Batch size: 32\n",
            "============================================================\n",
            "ğŸ“Š DataLoader ìƒ˜í”Œ í™•ì¸\n",
            "============================================================\n",
            "Train - ì…ë ¥ í…ì„œ shape: torch.Size([32, 96])\n",
            "Train - ë ˆì´ë¸” shape: torch.Size([32])\n",
            "Val - ì…ë ¥ í…ì„œ shape: torch.Size([32, 96])\n",
            "Val - ë ˆì´ë¸” shape: torch.Size([32])\n",
            "Test - ì…ë ¥ í…ì„œ shape: torch.Size([32, 105])\n",
            "Test - ë ˆì´ë¸” shape: torch.Size([32])\n",
            "ğŸ“Š ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°:\n",
            "   VOCAB_SIZE: 8,000 (ê³ ì •)\n",
            "Using device: cuda\n",
            "\n",
            "============================================================\n",
            "--- LSTM Model Training starts ---\n",
            "============================================================\n",
            "\n",
            "Epoch: 01 | Time: 0m 28s\n",
            "\tTrain Loss: 0.501 | Train Acc: 74.74%\n",
            "\t Val. Loss: 0.428 |  Val. Acc: 80.09%\n",
            "\t>> Validation loss improved (0.428). Model saved.\n",
            "Epoch: 02 | Time: 0m 27s\n",
            "\tTrain Loss: 0.400 | Train Acc: 81.89%\n",
            "\t Val. Loss: 0.392 |  Val. Acc: 82.54%\n",
            "\t>> Validation loss improved (0.392). Model saved.\n",
            "Epoch: 03 | Time: 0m 28s\n",
            "\tTrain Loss: 0.366 | Train Acc: 83.85%\n",
            "\t Val. Loss: 0.375 |  Val. Acc: 83.07%\n",
            "\t>> Validation loss improved (0.375). Model saved.\n",
            "Epoch: 04 | Time: 0m 28s\n",
            "\tTrain Loss: 0.344 | Train Acc: 85.08%\n",
            "\t Val. Loss: 0.367 |  Val. Acc: 84.01%\n",
            "\t>> Validation loss improved (0.367). Model saved.\n",
            "Epoch: 05 | Time: 0m 28s\n",
            "\tTrain Loss: 0.324 | Train Acc: 86.09%\n",
            "\t Val. Loss: 0.365 |  Val. Acc: 84.36%\n",
            "\t>> Validation loss improved (0.365). Model saved.\n",
            "Epoch: 06 | Time: 0m 29s\n",
            "\tTrain Loss: 0.309 | Train Acc: 86.99%\n",
            "\t Val. Loss: 0.355 |  Val. Acc: 84.79%\n",
            "\t>> Validation loss improved (0.355). Model saved.\n",
            "Epoch: 07 | Time: 0m 29s\n",
            "\tTrain Loss: 0.295 | Train Acc: 87.61%\n",
            "\t Val. Loss: 0.357 |  Val. Acc: 84.86%\n",
            "\t>> Validation loss did not improve. Counter: 1/5\n",
            "Epoch: 08 | Time: 0m 29s\n",
            "\tTrain Loss: 0.282 | Train Acc: 88.33%\n",
            "\t Val. Loss: 0.355 |  Val. Acc: 84.92%\n",
            "\t>> Validation loss did not improve. Counter: 2/5\n",
            "Epoch: 09 | Time: 0m 29s\n",
            "\tTrain Loss: 0.270 | Train Acc: 88.79%\n",
            "\t Val. Loss: 0.362 |  Val. Acc: 84.78%\n",
            "\t>> Validation loss did not improve. Counter: 3/5\n",
            "Epoch: 10 | Time: 0m 29s\n",
            "\tTrain Loss: 0.258 | Train Acc: 89.52%\n",
            "\t Val. Loss: 0.360 |  Val. Acc: 84.94%\n",
            "\t>> Validation loss did not improve. Counter: 4/5\n",
            "Epoch: 11 | Time: 0m 29s\n",
            "\tTrain Loss: 0.247 | Train Acc: 90.01%\n",
            "\t Val. Loss: 0.373 |  Val. Acc: 85.37%\n",
            "\t>> Validation loss did not improve. Counter: 5/5\n",
            "--- Early stopping triggered after 11 epochs ---\n",
            "\n",
            "--- Loading best LSTM model for test evaluation ---\n",
            "\n",
            "--- LSTM Model Test Results (Best Model) ---\n",
            "\tTest Loss: 1.021\n",
            "\tTest Acc:  55.50%\n"
          ]
        }
      ]
    }
  ]
}