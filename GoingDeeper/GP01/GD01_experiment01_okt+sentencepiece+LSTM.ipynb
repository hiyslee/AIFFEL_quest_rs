{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ì½”ë©ì—ì„œ ì•ˆì •ì ìœ¼ë¡œ ì„¤ì¹˜ë˜ëŠ” í˜•íƒœì†Œ ë¶„ì„ê¸° ì°¾ì•„ì„œ ì‚¼ë§Œë¦¬! - ETRI OPEN API, Kkma, Kiwi, KHAIII -> OKT.\n",
        "# ì°¸ë‚´! ì¸ê³µì§€ëŠ¥ì´ ê³ ì§‘ë¶€ë¦°ë‹¤ê³  ë­ë¼ê³  í•˜ë„¤ìš”.. ê²°êµ­ OKTë¡œ ì‹¤í—˜ ã…œã…œ\n",
        "# ================================================================================\n",
        "# âœ… Cell 1: ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ë° ì„¤ì¹˜\n",
        "# ================================================================================\n",
        "# ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ë° ì„í¬íŠ¸\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"ğŸ“¦ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ë° ì„í¬íŠ¸\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Step 0: OpenJDK ì„¤ì¹˜ ë° JAVA_HOME ì„¤ì • (konlpy ìš”êµ¬ì‚¬í•­)\n",
        "print(\"0ï¸âƒ£ OpenJDK ì„¤ì¹˜ ë° JAVA_HOME ì„¤ì •...\")\n",
        "# ê¸°ì¡´ openjdk-install ëŒ€ì‹  apt-getì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
        "subprocess.check_call(['apt-get', 'update'])\n",
        "subprocess.check_call(['apt-get', 'install', '-y', 'openjdk-11-jdk-headless'])\n",
        "os.environ['JAVA_HOME'] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "print(f\"   âœ… JAVA_HOME ì„¤ì • ì™„ë£Œ: {os.environ['JAVA_HOME']}\")\n",
        "\n",
        "# Step 1: í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
        "print(\"\\n1ï¸âƒ£ í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜...\")\n",
        "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"konlpy\", \"sentencepiece\"])\n",
        "print(\"   âœ… ì™„ë£Œ\")\n",
        "\n",
        "# Step 2: ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
        "print(\"\\n2ï¸âƒ£ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸...\")\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from konlpy.tag import Okt  # â­ OKT ì‚¬ìš©\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import sentencepiece as spm\n",
        "import re\n",
        "\n",
        "print(\"   âœ… ì™„ë£Œ\")\n",
        "\n",
        "# Step 3: OKT ì´ˆê¸°í™”\n",
        "print(\"\\n3ï¸âƒ£ OKT í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™”...\")\n",
        "okt = Okt()\n",
        "print(\"   âœ… OKT ë¡œë“œ ì™„ë£Œ\")\n",
        "\n",
        "# Step 4: ë²„ì „ í™•ì¸\n",
        "print(\"\\n4ï¸âƒ£ ë²„ì „ í™•ì¸...\")\n",
        "print(f\"   PyTorch: {torch.__version__}\")\n",
        "print(f\"   NumPy: {np.__version__}\")\n",
        "print(f\"   Pandas: {pd.__version__}\")\n",
        "\n",
        "# Step 5: OKT í…ŒìŠ¤íŠ¸\n",
        "print(\"\\n5ï¸âƒ£ OKT ë™ì‘ í…ŒìŠ¤íŠ¸...\")\n",
        "test_result = okt.morphs(\"ì´ ì˜í™” ì •ë§ ì¬ë¯¸ìˆì—ˆì–´\")\n",
        "print(f\"   âœ… OKT ì •ìƒ ì‘ë™\")\n",
        "print(f\"   ë¶„ì„ ê²°ê³¼: {test_result}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"ğŸ‰ í™˜ê²½ì…‹íŒ… ì™„ë£Œ!\")\n",
        "print(\"=\" * 80)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBCK4_0xgFYH",
        "outputId": "1250b439-27e3-4950-a84f-983de9a995ef"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "ğŸ“¦ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ë° ì„í¬íŠ¸\n",
            "================================================================================\n",
            "0ï¸âƒ£ OpenJDK ì„¤ì¹˜ ë° JAVA_HOME ì„¤ì •...\n",
            "   âœ… JAVA_HOME ì„¤ì • ì™„ë£Œ: /usr/lib/jvm/java-11-openjdk-amd64\n",
            "\n",
            "1ï¸âƒ£ í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜...\n",
            "   âœ… ì™„ë£Œ\n",
            "\n",
            "2ï¸âƒ£ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸...\n",
            "   âœ… ì™„ë£Œ\n",
            "\n",
            "3ï¸âƒ£ OKT í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™”...\n",
            "   âœ… OKT ë¡œë“œ ì™„ë£Œ\n",
            "\n",
            "4ï¸âƒ£ ë²„ì „ í™•ì¸...\n",
            "   PyTorch: 2.8.0+cu126\n",
            "   NumPy: 2.0.2\n",
            "   Pandas: 2.2.2\n",
            "\n",
            "5ï¸âƒ£ OKT ë™ì‘ í…ŒìŠ¤íŠ¸...\n",
            "   âœ… OKT ì •ìƒ ì‘ë™\n",
            "   ë¶„ì„ ê²°ê³¼: ['ì´', 'ì˜í™”', 'ì •ë§', 'ì¬ë¯¸ìˆì—ˆì–´']\n",
            "\n",
            "================================================================================\n",
            "ğŸ‰ í™˜ê²½ì…‹íŒ… ì™„ë£Œ!\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ë°ì´í„° ì—…ë¡œë”©! ìˆ˜ë™ìœ¼ë¡œ! ë‹¤ìŒ ì½”ë“œë¸”ëŸ­ ì‹¤í–‰í•˜ê¸° ì „ì—!!"
      ],
      "metadata": {
        "id": "7ssuDnY0WTfu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================================\n",
        "# âœ… Cell 2: ë°ì´í„° ë¡œë“œ\n",
        "# ================================================================================\n",
        "train = pd.read_table(\"/content/ratings_train.txt\")\n",
        "test = pd.read_table(\"/content/ratings_test.txt\")\n",
        "\n",
        "print(\"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ\")\n",
        "print(f\"Train shape: {train.shape}\")\n",
        "print(f\"Test shape: {test.shape}\")\n",
        "print(\"\\nì „ì²˜ë¦¬ ê²°ê³¼ ìƒ˜í”Œ:\")\n",
        "print(train.head())\n",
        "\n",
        "\n",
        "# ================================================================================\n",
        "# âœ… Cell 3: í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬\n",
        "# ================================================================================\n",
        "# ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸\n",
        "STOPWORDS = ['ì˜','ê°€','ì´','ì€','ë“¤','ëŠ”','ì¢€','ì˜','ê±','ê³¼','ë„','ë¥¼','ìœ¼ë¡œ','ì','ì—','ì™€','í•œ','í•˜ë‹¤']\n",
        "\n",
        "# ë§ì¶¤ë²• ë³€í˜• ì‚¬ì „\n",
        "SPELLING_DICT = {\n",
        "    'êµ³': ['ê¶…', 'êµ³', 'êµ¿'],\n",
        "    'ë¯¸ì³¤': ['ë¯¸ì²«', 'ë¯¸ì³£', 'ë¯¸ì²¬', 'ë¯¸ì³¤', 'ã…ã…Š'],\n",
        "    'ê´œì°®': ['ê´œì°®', 'ê´œì¶˜', 'ê´œì°¬', 'ã„±ã…Š', 'ê° ì°¬', 'ê° ì°®', 'ê´¸ì°¬', 'ê´¸ì°®'],\n",
        "    'ë´¤': ['ë´£'],\n",
        "    'ê² ': ['ê²Ÿ']\n",
        "}\n",
        "\n",
        "TEXT_COL = \"document\"\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•¨ìˆ˜\"\"\"\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "\n",
        "    text = str(text)\n",
        "\n",
        "    # 1. ë°˜ë³µë˜ëŠ” ë¬¸ì¥ ë¶€í˜¸ ì œê±° (2ê°œ ì´ìƒ â†’ 1ê°œ)\n",
        "    text = re.sub(r'([.!?â€¦;])\\1+', r'\\1', text)\n",
        "\n",
        "    # 2. ë§ì¶¤ë²• ë³€í˜• í†µì¼\n",
        "    for correct, variations in SPELLING_DICT.items():\n",
        "        for variant in variations:\n",
        "            text = text.replace(variant, correct)\n",
        "\n",
        "    # 3. ë°˜ë³µ ë¬¸ì ì œê±° (3ë²ˆ ì´ìƒ ë°˜ë³µ â†’ 2ë²ˆ)\n",
        "    text = re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n",
        "\n",
        "    # 4. ììŒ/ëª¨ìŒ ë‹¨ë… ì œê±°\n",
        "    text = re.sub(r'[ã„±-ã…ã…-ã…£]+', ' ', text)\n",
        "\n",
        "    # 5. ì˜ì–´/ìˆ«ì/í•œê¸€/ë¬¸ì¥ë¶€í˜¸ë§Œ ë‚¨ê¸°ê¸°\n",
        "    text = re.sub(r'[^ê°€-í£a-zA-Z0-9\\s.!?,]', ' ', text)\n",
        "\n",
        "    # 6. ë¬¸ì¥ë¶€í˜¸ ì•ë’¤ë¡œ ê³µë°± ì¶”ê°€\n",
        "    text = re.sub(r'([.!?,])', r' \\1 ', text)\n",
        "\n",
        "    # 7. ì˜ì–´ ì†Œë¬¸ì ë³€í™˜\n",
        "    text = text.lower()\n",
        "\n",
        "    # 8. ë¶ˆìš©ì–´ ì œê±°\n",
        "    words = text.split()\n",
        "    filtered_words = []\n",
        "    for word in words:\n",
        "        if word in STOPWORDS:\n",
        "            continue\n",
        "        found = False\n",
        "        for stopword in sorted(STOPWORDS, key=len, reverse=True):\n",
        "            if len(word) > len(stopword) and word.endswith(stopword):\n",
        "                cleaned = word[:-len(stopword)]\n",
        "                if len(cleaned) >= 2:\n",
        "                    filtered_words.append(cleaned)\n",
        "                    found = True\n",
        "                    break\n",
        "        if not found:\n",
        "            filtered_words.append(word)\n",
        "\n",
        "    text = ' '.join(filtered_words)\n",
        "\n",
        "    # 9. ì—°ì† ê³µë°±ì„ í•˜ë‚˜ë¡œ\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # 10. ì•ë’¤ ê³µë°± ì œê±°\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "def preprocess_dataframe(df, text_col=TEXT_COL):\n",
        "    \"\"\"ë°ì´í„°í”„ë ˆì„ ì „ì²˜ë¦¬ í•¨ìˆ˜\"\"\"\n",
        "    print(f\"ì „ì²˜ë¦¬ ì „ ë°ì´í„° í¬ê¸°: {len(df)}\")\n",
        "\n",
        "    # 1. ê²°ì¸¡ì¹˜ ì œê±°\n",
        "    df = df.dropna(subset=[text_col])\n",
        "    print(f\"ê²°ì¸¡ì¹˜ ì œê±° í›„: {len(df)}\")\n",
        "\n",
        "    # 2. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì ìš©\n",
        "    df[text_col] = df[text_col].apply(preprocess_text)\n",
        "\n",
        "    # 3. ë¹ˆ ë¬¸ìì—´ ì œê±°\n",
        "    df = df[df[text_col].str.strip() != '']\n",
        "    print(f\"ë¹ˆ ë¬¸ìì—´ ì œê±° í›„: {len(df)}\")\n",
        "\n",
        "    # 4. ì¤‘ë³µ í–‰ ì œê±°\n",
        "    df = df.drop_duplicates(subset=[text_col])\n",
        "    print(f\"ì¤‘ë³µ ì œê±° í›„: {len(df)}\")\n",
        "\n",
        "    return df.reset_index(drop=True)\n",
        "\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\nğŸ“Š Train ë°ì´í„° ì „ì²˜ë¦¬:\")\n",
        "train_processed = preprocess_dataframe(train.copy())\n",
        "\n",
        "print(\"\\nğŸ“Š Test ë°ì´í„° ì „ì²˜ë¦¬:\")\n",
        "test_processed = preprocess_dataframe(test.copy())\n",
        "\n",
        "print(f\"\\nâœ… ì „ì²˜ë¦¬ ì™„ë£Œ!\")\n",
        "print(f\"Train ë°ì´í„°: {len(train_processed)}ê°œ\")\n",
        "print(f\"Test ë°ì´í„°: {len(test_processed)}ê°œ\")\n",
        "print(\"\\nì „ì²˜ë¦¬ ê²°ê³¼ ìƒ˜í”Œ:\")\n",
        "print(train_processed.head(10))\n",
        "\n",
        "\n",
        "# ================================================================================\n",
        "# âœ… Cell 4: SentencePiece ëª¨ë¸ í•™ìŠµ (OKT ê¸°ë°˜)\n",
        "# ================================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"SentencePiece ëª¨ë¸ í•™ìŠµ (OKT ê¸°ë°˜)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Step 1: OKTë¡œ í˜•íƒœì†Œ ë¶„ì„\n",
        "print(\"\\nğŸ”„ Step 1: OKT í˜•íƒœì†Œ ë¶„ì„ ì§„í–‰ ì¤‘...\")\n",
        "corpus = train_processed['document'].tolist()\n",
        "\n",
        "# í˜•íƒœì†Œ ë¶„ì„ëœ í…ìŠ¤íŠ¸ë¥¼ ì„ì‹œ íŒŒì¼ì— ì €ì¥\n",
        "temp_corpus_path = '/tmp/morphs_corpus_okt.txt'\n",
        "\n",
        "with open(temp_corpus_path, 'w', encoding='utf-8') as f:\n",
        "    for text in tqdm(corpus, desc=\"OKT morphs\"):\n",
        "        # â­ OKT í˜•íƒœì†Œ ë¶„ì„\n",
        "        morphs = okt.morphs(text)\n",
        "        f.write(' '.join(morphs) + '\\n')\n",
        "\n",
        "print(f\"âœ… í˜•íƒœì†Œ ë¶„ì„ ì™„ë£Œ: {temp_corpus_path}\")\n",
        "\n",
        "# Step 2: SentencePiece ëª¨ë¸ í•™ìŠµ\n",
        "print(\"\\nğŸ”„ Step 2: SentencePiece ëª¨ë¸ í•™ìŠµ ì¤‘...\")\n",
        "\n",
        "sp_model_path = '/tmp/sentencepiece_okt'\n",
        "vocab_size = 8000\n",
        "\n",
        "spm.SentencePieceTrainer.train(\n",
        "    input=temp_corpus_path,\n",
        "    model_prefix=sp_model_path,\n",
        "    vocab_size=vocab_size,\n",
        "    character_coverage=1.0,\n",
        "    model_type='unigram',\n",
        "    unk_id=0,\n",
        "    bos_id=1,\n",
        "    eos_id=2,\n",
        "    pad_id=3,\n",
        "    normalization_rule_name='identity'\n",
        ")\n",
        "\n",
        "print(f\"âœ… SentencePiece ëª¨ë¸ í•™ìŠµ ì™„ë£Œ\")\n",
        "print(f\"   Model: {sp_model_path}.model\")\n",
        "print(f\"   Vocab size: {vocab_size:,}\")\n",
        "\n",
        "# Step 3: SentencePiece ëª¨ë¸ ë¡œë“œ\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.Load(f'{sp_model_path}.model')\n",
        "\n",
        "print(f\"\\nâœ… SentencePiece ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\")\n",
        "print(f\"   Vocab size: {sp.vocab_size()}\")\n",
        "\n",
        "\n",
        "# ================================================================================\n",
        "# âœ… Cell 5: OKT + SentencePiece í† í¬ë‚˜ì´ì œì´ì…˜ í…ŒìŠ¤íŠ¸\n",
        "# ================================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"OKT + SentencePiece í† í¬ë‚˜ì´ì œì´ì…˜ í…ŒìŠ¤íŠ¸\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "test_texts = [\n",
        "    \"ì´ ì˜í™” ì •ë§ ì¬ë¯¸ìˆì—ˆì–´\",\n",
        "    \"ìµœì•…ì˜ ì˜í™”ì˜€ë‹¤\",\n",
        "    \"ë°°ìš°ë“¤ì˜ ì—°ê¸°ê°€ í›Œë¥­í–ˆë‹¤\"\n",
        "]\n",
        "\n",
        "for text in test_texts:\n",
        "    # â­ OKT í˜•íƒœì†Œ ë¶„ì„\n",
        "    morphs = okt.morphs(text)\n",
        "    morphs_text = ' '.join(morphs)\n",
        "\n",
        "    # SentencePiece ì¸ì½”ë”©\n",
        "    encoded = sp.EncodeAsIds(morphs_text)\n",
        "\n",
        "    # ë””ì½”ë”© í™•ì¸\n",
        "    decoded = sp.DecodeIds(encoded)\n",
        "\n",
        "    print(f\"\\nì›ë³¸: {text}\")\n",
        "    print(f\"í˜•íƒœì†Œ: {morphs_text}\")\n",
        "    print(f\"ì¸ì½”ë”©: {encoded}\")\n",
        "    print(f\"ë””ì½”ë”©: {decoded}\")\n",
        "\n",
        "\n",
        "# ================================================================================\n",
        "# âœ… Cell 6: í† í¬ë‚˜ì´ì§• í•¨ìˆ˜\n",
        "# ================================================================================\n",
        "\n",
        "def okt_sentencepiece_tokenize(okt, sp, corpus, max_length=130):\n",
        "    \"\"\"\n",
        "    OKT í˜•íƒœì†Œ ë¶„ì„ + SentencePiece í† í¬ë‚˜ì´ì œì´ì…˜\n",
        "\n",
        "    Args:\n",
        "        okt: OKT í† í¬ë‚˜ì´ì € ê°ì²´\n",
        "        sp: SentencePiece í”„ë¡œì„¸ì„œ ê°ì²´\n",
        "        corpus: ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸\n",
        "        max_length: ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´\n",
        "\n",
        "    Returns:\n",
        "        tensor: íŒ¨ë”©ëœ í† í° ID í…ì„œ\n",
        "        vocab_size: SentencePiece ì–´íœ˜ì§‘ í¬ê¸°\n",
        "    \"\"\"\n",
        "    # Step 1: OKT í˜•íƒœì†Œ ë¶„ì„\n",
        "    tokenized_texts = []\n",
        "    all_token_ids = []\n",
        "\n",
        "    print(\"ğŸ”„ OKT í˜•íƒœì†Œ ë¶„ì„ + SentencePiece í† í¬ë‚˜ì´ì œì´ì…˜ ì§„í–‰ ì¤‘...\")\n",
        "    for text in tqdm(corpus, desc=\"OKT + SentencePiece\"):\n",
        "        # â­ OKTë¡œ í˜•íƒœì†Œ ë¶„ì„\n",
        "        morphs = okt.morphs(text)\n",
        "        morphs_text = ' '.join(morphs)\n",
        "\n",
        "        # SentencePieceë¡œ ì„œë¸Œì›Œë“œ í† í¬ë‚˜ì´ì œì´ì…˜\n",
        "        token_ids = sp.EncodeAsIds(morphs_text)\n",
        "\n",
        "        # ìµœëŒ€ ê¸¸ì´ë¡œ ìë¥´ê¸°\n",
        "        if len(token_ids) > max_length:\n",
        "            token_ids = token_ids[:max_length]\n",
        "\n",
        "        tokenized_texts.append(token_ids)\n",
        "        all_token_ids.extend(token_ids)\n",
        "\n",
        "    # Step 2: íŒ¨ë”©\n",
        "    tensor_list = []\n",
        "    for token_ids in tokenized_texts:\n",
        "        tensor_list.append(torch.tensor(token_ids, dtype=torch.long))\n",
        "\n",
        "    tensor = pad_sequence(tensor_list, batch_first=True, padding_value=0)\n",
        "\n",
        "    vocab_size = sp.vocab_size()\n",
        "\n",
        "    print(f\"\\nâœ… í† í¬ë‚˜ì´ì œì´ì…˜ ì™„ë£Œ!\")\n",
        "    print(f\"   ìµœì¢… í…ì„œ shape: {tensor.shape}\")\n",
        "    print(f\"   Vocab size: {vocab_size:,}\")\n",
        "    print(f\"   ê³ ìœ  í† í° ìˆ˜: {len(set(all_token_ids)):,}\")\n",
        "\n",
        "    return tensor, vocab_size\n",
        "\n",
        "\n",
        "# ================================================================================\n",
        "# âœ… Cell 7: DataLoader ìƒì„± í•¨ìˆ˜\n",
        "# ================================================================================\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "\n",
        "def create_dataloaders(train_input_ids, train_labels,\n",
        "                       test_input_ids, test_labels,\n",
        "                       batch_size=32, val_ratio=0.2):\n",
        "    \"\"\"\n",
        "    Train/Val/Test DataLoader ìƒì„±\n",
        "\n",
        "    Args:\n",
        "        train_input_ids: í›ˆë ¨ ë°ì´í„° ì…ë ¥ ID í…ì„œ\n",
        "        train_labels: í›ˆë ¨ ë ˆì´ë¸”\n",
        "        test_input_ids: í…ŒìŠ¤íŠ¸ ë°ì´í„° ì…ë ¥ ID í…ì„œ\n",
        "        test_labels: í…ŒìŠ¤íŠ¸ ë ˆì´ë¸”\n",
        "        batch_size: ë°°ì¹˜ í¬ê¸°\n",
        "        val_ratio: validation ë¹„ìœ¨\n",
        "\n",
        "    Returns:\n",
        "        train_loader, val_loader, test_loader\n",
        "    \"\"\"\n",
        "    # Train ë°ì´í„°ì—ì„œ train/val split\n",
        "    train_dataset = TensorDataset(train_input_ids, train_labels)\n",
        "    val_size = int(len(train_dataset) * val_ratio)\n",
        "    train_size = len(train_dataset) - val_size\n",
        "    train_split, val_split = random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "    # DataLoader ìƒì„±\n",
        "    train_loader = DataLoader(train_split, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_split, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Test DataLoader\n",
        "    test_dataset = TensorDataset(test_input_ids, test_labels)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    print(f\"âœ… DataLoader ìƒì„± ì™„ë£Œ!\")\n",
        "    print(f\"   Train samples: {train_size:,} â†’ {len(train_loader)} batches\")\n",
        "    print(f\"   Val samples: {val_size:,} â†’ {len(val_loader)} batches\")\n",
        "    print(f\"   Test samples: {len(test_dataset):,} â†’ {len(test_loader)} batches\")\n",
        "    print(f\"   Batch size: {batch_size}\")\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "\n",
        "# ë°ì´í„° ì¤€ë¹„\n",
        "corpus = train_processed['document'].tolist()\n",
        "labels = torch.tensor(train_processed['label'].values, dtype=torch.long)\n",
        "\n",
        "print(f\"\\nğŸ“Š ì›ë³¸ ë°ì´í„°:\")\n",
        "print(f\"   Corpus í¬ê¸°: {len(corpus):,}\")\n",
        "print(f\"   Labels í¬ê¸°: {len(labels):,}\\n\")\n",
        "\n",
        "# â­ OKT + SentencePiece í† í¬ë‚˜ì´ì§•\n",
        "train_input_ids, VOCAB_SIZE = okt_sentencepiece_tokenize(okt, sp, corpus, max_length=130)\n",
        "print()\n",
        "\n",
        "# Test ë°ì´í„°ë„ ì²˜ë¦¬\n",
        "test_corpus = test_processed['document'].tolist()\n",
        "test_labels = torch.tensor(test_processed['label'].values, dtype=torch.long)\n",
        "\n",
        "test_input_ids, _ = okt_sentencepiece_tokenize(okt, sp, test_corpus, max_length=130)\n",
        "print()\n",
        "\n",
        "# DataLoader ìƒì„±\n",
        "train_loader, val_loader, test_loader = create_dataloaders(\n",
        "    train_input_ids,\n",
        "    labels,\n",
        "    test_input_ids,\n",
        "    test_labels,\n",
        "    batch_size=32,\n",
        "    val_ratio=0.2\n",
        ")\n",
        "\n",
        "# í™•ì¸\n",
        "print(\"=\" * 60)\n",
        "print(\"ğŸ“Š DataLoader ìƒ˜í”Œ í™•ì¸\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for batch in train_loader:\n",
        "    x, y = batch\n",
        "    print(f\"Train - ì…ë ¥ í…ì„œ shape: {x.shape}\")\n",
        "    print(f\"Train - ë ˆì´ë¸” shape: {y.shape}\")\n",
        "    break\n",
        "\n",
        "for batch in val_loader:\n",
        "    x, y = batch\n",
        "    print(f\"Val - ì…ë ¥ í…ì„œ shape: {x.shape}\")\n",
        "    print(f\"Val - ë ˆì´ë¸” shape: {y.shape}\")\n",
        "    break\n",
        "\n",
        "for batch in test_loader:\n",
        "    x, y = batch\n",
        "    print(f\"Test - ì…ë ¥ í…ì„œ shape: {x.shape}\")\n",
        "    print(f\"Test - ë ˆì´ë¸” shape: {y.shape}\")\n",
        "    break\n",
        "\n",
        "\n",
        "# ================================================================================\n",
        "# âœ… Cell 8: í•˜ì´í¼íŒŒë¼ë¯¸í„° ì •ì˜\n",
        "# ================================================================================\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# ê³µí†µ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì •ì˜\n",
        "EMBEDDING_DIM = 100                # ì„ë² ë”© ë²¡í„°ì˜ ì°¨ì›\n",
        "HIDDEN_DIM = 128                   # LSTMì˜ ì€ë‹‰ ìƒíƒœ ì°¨ì›\n",
        "OUTPUT_DIM = 1                     # ì¶œë ¥ ì°¨ì› (ê¸ì •=1, ë¶€ì •=0 -> 1ê°œ)\n",
        "N_LAYERS = 2                       # LSTM ë ˆì´ì–´ ê°œìˆ˜\n",
        "BIDIRECTIONAL = True               # ì–‘ë°©í–¥ RNN/LSTM ì—¬ë¶€\n",
        "DROPOUT_RATE = 0.2                 # ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨\n",
        "PAD_IDX = 0                        # íŒ¨ë”© ì¸ë±ìŠ¤ (0)\n",
        "\n",
        "print(f\"ğŸ“Š ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°:\")\n",
        "print(f\"   VOCAB_SIZE: {VOCAB_SIZE:,} (SentencePiece ë™ì  ì„¤ì •)\")\n",
        "\n",
        "\n",
        "# ================================================================================\n",
        "# âœ… Cell 9: LSTM ëª¨ë¸\n",
        "# ================================================================================\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers,\n",
        "                 bidirectional, dropout, pad_idx):\n",
        "        super().__init__()\n",
        "\n",
        "        # 1. ì„ë² ë”© ë ˆì´ì–´\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "\n",
        "        # 2. LSTM ë ˆì´ì–´\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim,\n",
        "                           hidden_size=hidden_dim,\n",
        "                           num_layers=n_layers,\n",
        "                           bidirectional=bidirectional,\n",
        "                           batch_first=True,\n",
        "                           dropout=dropout)\n",
        "\n",
        "        # 3. FC ë ˆì´ì–´\n",
        "        fc_input_dim = hidden_dim * 2\n",
        "        self.fc = nn.Linear(fc_input_dim, output_dim)\n",
        "\n",
        "        # 4. ë“œë¡­ì•„ì›ƒ\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text):\n",
        "\n",
        "        embedded = self.embedding(text)\n",
        "\n",
        "        # 2. LSTM\n",
        "        _output, (hidden, cell) = self.lstm(embedded)\n",
        "\n",
        "        # 3. ë§ˆì§€ë§‰ ë ˆì´ì–´ì˜ ì€ë‹‰ ìƒíƒœ ê²°í•© (ì–‘ë°©í–¥ ì²˜ë¦¬)\n",
        "        if self.lstm.bidirectional:\n",
        "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
        "        else:\n",
        "            hidden = self.dropout(hidden[-1,:,:])\n",
        "\n",
        "        # 4. FC ë ˆì´ì–´ í†µê³¼\n",
        "        prediction = self.fc(hidden)\n",
        "\n",
        "        return prediction.squeeze(1)\n",
        "\n",
        "\n",
        "# ================================================================================\n",
        "# âœ… Cell 10: í›ˆë ¨ í•¨ìˆ˜\n",
        "# ================================================================================\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import time\n",
        "\n",
        "# 0. GPU ì¥ì¹˜ ì„¤ì •\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# 1. í—¬í¼ í•¨ìˆ˜ ì •ì˜\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "def binary_accuracy(preds, y):\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    correct = (rounded_preds == y).float()\n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n",
        "\n",
        "# 2. í›ˆë ¨ í•¨ìˆ˜ ì •ì˜\n",
        "def train(model, iterator, optimizer, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    model.train()\n",
        "    for texts, labels in iterator:\n",
        "        texts = texts.to(device)\n",
        "        labels = labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(texts)\n",
        "        loss = criterion(predictions, labels.float())\n",
        "        acc = binary_accuracy(predictions, labels)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "# 3. í‰ê°€ í•¨ìˆ˜ ì •ì˜\n",
        "def evaluate(model, iterator, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for texts, labels in iterator:\n",
        "            texts = texts.to(device)\n",
        "            labels = labels.to(device)\n",
        "            predictions = model(texts)\n",
        "            loss = criterion(predictions, labels.float())\n",
        "            acc = binary_accuracy(predictions, labels)\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "\n",
        "# ================================================================================\n",
        "# âœ… Cell 11: ëª¨ë¸ í›ˆë ¨\n",
        "# ================================================================================\n",
        "# ëª¨ë¸ ì„¤ì •\n",
        "lstm_model = LSTMModel(\n",
        "    VOCAB_SIZE,\n",
        "    EMBEDDING_DIM,\n",
        "    HIDDEN_DIM,\n",
        "    OUTPUT_DIM,\n",
        "    N_LAYERS,\n",
        "    BIDIRECTIONAL,\n",
        "    DROPOUT_RATE,\n",
        "    PAD_IDX\n",
        ").to(device)\n",
        "\n",
        "save_path = 'best_model_lstm_okt.pt'\n",
        "N_EPOCHS = 20\n",
        "patience = 5\n",
        "criterion = nn.BCEWithLogitsLoss().to(device)\n",
        "optimizer = optim.Adam(filter(lambda p: p.requires_grad, lstm_model.parameters()), lr=0.0001)\n",
        "\n",
        "# Early stopping ë³€ìˆ˜\n",
        "best_valid_loss = float('inf')\n",
        "patience_counter = 0\n",
        "\n",
        "train_losses, train_accs, valid_losses, valid_accs = [], [], [], []\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"--- LSTM Model Training starts (OKT + SentencePiece) ---\")\n",
        "print(f\"{'='*60}\\n\")\n",
        "\n",
        "# í•™ìŠµ ë£¨í”„\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss, train_acc = train(lstm_model, train_loader, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(lstm_model, val_loader, criterion)\n",
        "\n",
        "    end_time = time.time()\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
        "\n",
        "    # ê¸°ë¡\n",
        "    train_losses.append(train_loss)\n",
        "    train_accs.append(train_acc)\n",
        "    valid_losses.append(valid_loss)\n",
        "    valid_accs.append(valid_acc)\n",
        "\n",
        "    # Early Stopping\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(lstm_model.state_dict(), save_path)\n",
        "        patience_counter = 0\n",
        "        print(f'\\t>> Validation loss improved ({best_valid_loss:.3f}). Model saved.')\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        print(f'\\t>> Validation loss did not improve. Counter: {patience_counter}/{patience}')\n",
        "        if patience_counter >= patience:\n",
        "            print(f'--- Early stopping triggered after {epoch+1} epochs ---')\n",
        "            break\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸ í‰ê°€\n",
        "print(f\"\\n--- Loading best LSTM model for test evaluation ---\")\n",
        "lstm_model.load_state_dict(torch.load(save_path))\n",
        "test_loss, test_acc = evaluate(lstm_model, test_loader, criterion)\n",
        "\n",
        "print(f\"\\n--- LSTM Model Test Results (Best Model) ---\")\n",
        "print(f'\\tTest Loss: {test_loss:.3f}')\n",
        "print(f'\\tTest Acc:  {test_acc*100:.2f}%')\n",
        "\n",
        "# ================================================================================\n",
        "# ğŸ‰ ì½”ë“œ ë (OKT + SentencePiece ë²„ì „)\n",
        "# ================================================================================"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBLW3oIJg6YG",
        "outputId": "79bc06eb-3e35-4a0b-e6a7-41c65e1a6ef0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ\n",
            "Train shape: (150000, 3)\n",
            "Test shape: (50000, 3)\n",
            "\n",
            "ì „ì²˜ë¦¬ ê²°ê³¼ ìƒ˜í”Œ:\n",
            "         id                                           document  label\n",
            "0   9976970                                ì•„ ë”ë¹™.. ì§„ì§œ ì§œì¦ë‚˜ë„¤ìš” ëª©ì†Œë¦¬      0\n",
            "1   3819312                  í ...í¬ìŠ¤í„°ë³´ê³  ì´ˆë”©ì˜í™”ì¤„....ì˜¤ë²„ì—°ê¸°ì¡°ì°¨ ê°€ë³ì§€ ì•Šêµ¬ë‚˜      1\n",
            "2  10265843                                  ë„ˆë¬´ì¬ë°“ì—ˆë‹¤ê·¸ë˜ì„œë³´ëŠ”ê²ƒì„ì¶”ì²œí•œë‹¤      0\n",
            "3   9045019                      êµë„ì†Œ ì´ì•¼ê¸°êµ¬ë¨¼ ..ì†”ì§íˆ ì¬ë¯¸ëŠ” ì—†ë‹¤..í‰ì  ì¡°ì •      0\n",
            "4   6483659  ì‚¬ì´ëª¬í˜ê·¸ì˜ ìµì‚´ìŠ¤ëŸ° ì—°ê¸°ê°€ ë‹ë³´ì˜€ë˜ ì˜í™”!ìŠ¤íŒŒì´ë”ë§¨ì—ì„œ ëŠ™ì–´ë³´ì´ê¸°ë§Œ í–ˆë˜ ì»¤ìŠ¤í‹´ ...      1\n",
            "================================================================================\n",
            "ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘\n",
            "================================================================================\n",
            "\n",
            "ğŸ“Š Train ë°ì´í„° ì „ì²˜ë¦¬:\n",
            "ì „ì²˜ë¦¬ ì „ ë°ì´í„° í¬ê¸°: 150000\n",
            "ê²°ì¸¡ì¹˜ ì œê±° í›„: 149995\n",
            "ë¹ˆ ë¬¸ìì—´ ì œê±° í›„: 149607\n",
            "ì¤‘ë³µ ì œê±° í›„: 144478\n",
            "\n",
            "ğŸ“Š Test ë°ì´í„° ì „ì²˜ë¦¬:\n",
            "ì „ì²˜ë¦¬ ì „ ë°ì´í„° í¬ê¸°: 50000\n",
            "ê²°ì¸¡ì¹˜ ì œê±° í›„: 49997\n",
            "ë¹ˆ ë¬¸ìì—´ ì œê±° í›„: 49843\n",
            "ì¤‘ë³µ ì œê±° í›„: 48700\n",
            "\n",
            "âœ… ì „ì²˜ë¦¬ ì™„ë£Œ!\n",
            "Train ë°ì´í„°: 144478ê°œ\n",
            "Test ë°ì´í„°: 48700ê°œ\n",
            "\n",
            "ì „ì²˜ë¦¬ ê²°ê³¼ ìƒ˜í”Œ:\n",
            "         id                                           document  label\n",
            "0   9976970                                ì•„ ë”ë¹™ . ì§„ì§œ ì§œì¦ë‚˜ë„¤ìš” ëª©ì†Œë¦¬      0\n",
            "1   3819312                   í  . í¬ìŠ¤í„°ë³´ê³  ì´ˆë”©ì˜í™”ì¤„ . ì˜¤ë²„ì—°ê¸°ì¡°ì°¨ ê°€ë³ì§€ ì•Šêµ¬ë‚˜      1\n",
            "2  10265843                                  ë„ˆë¬´ì¬ë°“ì—ˆë‹¤ê·¸ë˜ì„œë³´ëŠ”ê²ƒì„ì¶”ì²œí•œë‹¤      0\n",
            "3   9045019                      êµë„ì†Œ ì´ì•¼ê¸°êµ¬ë¨¼ . ì†”ì§íˆ ì¬ë¯¸ ì—†ë‹¤ . í‰ì  ì¡°ì •      0\n",
            "4   6483659  ì‚¬ì´ëª¬í˜ê·¸ ìµì‚´ìŠ¤ëŸ° ì—°ê¸° ë‹ë³´ì˜€ë˜ ì˜í™” ! ìŠ¤íŒŒì´ë”ë§¨ì—ì„œ ëŠ™ì–´ë³´ì´ê¸°ë§Œ í–ˆë˜ ì»¤ìŠ¤í‹´ ...      1\n",
            "5   5403919        ë§‰ ê±¸ìŒë§ˆ ë—€ 3ì„¸ë¶€í„° ì´ˆë“±í•™êµ 1í•™ë…„ìƒì¸ 8ì‚´ìš©ì˜í™” . . ë³„ë°˜ê°œ ì•„ê¹Œì›€ .      0\n",
            "6   7797314                              ì›ì‘ ê¸´ì¥ê°ì„ ì œëŒ€ë¡œ ì‚´ë ¤ë‚´ì§€ëª»í–ˆë‹¤ .      0\n",
            "7   9443947  ë³„ ë°˜ê°œ ì•„ê¹ë‹¤ ìš•ë‚˜ì˜¨ë‹¤ ì´ì‘ê²½ ê¸¸ìš©ìš° ì—°ê¸°ìƒí™œì´ëª‡ë…„ì¸ì§€ . ì •ë§ ë°œë¡œí•´ ê·¸ê²ƒë³´ë‹¨ ...      0\n",
            "8   7156791                                ì•¡ì…˜ ì—†ëŠ”ë° ì¬ë¯¸ ìˆëŠ” ëª‡ì•ˆë˜ ì˜í™”      1\n",
            "9   5912145     ì™œì¼€ í‰ì  ë‚®ì€ê±´ë° ? ê½¤ ë³¼ë§Œí•œë° . í—ë¦¬ìš°ë“œì‹ í™”ë ¤í•¨ì—ë§Œ ë„ˆë¬´ ê¸¸ë“¤ì—¬ì ¸ ìˆë‚˜ ?      1\n",
            "\n",
            "================================================================================\n",
            "SentencePiece ëª¨ë¸ í•™ìŠµ (OKT ê¸°ë°˜)\n",
            "================================================================================\n",
            "\n",
            "ğŸ”„ Step 1: OKT í˜•íƒœì†Œ ë¶„ì„ ì§„í–‰ ì¤‘...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OKT morphs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 144478/144478 [04:24<00:00, 545.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… í˜•íƒœì†Œ ë¶„ì„ ì™„ë£Œ: /tmp/morphs_corpus_okt.txt\n",
            "\n",
            "ğŸ”„ Step 2: SentencePiece ëª¨ë¸ í•™ìŠµ ì¤‘...\n",
            "âœ… SentencePiece ëª¨ë¸ í•™ìŠµ ì™„ë£Œ\n",
            "   Model: /tmp/sentencepiece_okt.model\n",
            "   Vocab size: 8,000\n",
            "\n",
            "âœ… SentencePiece ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\n",
            "   Vocab size: 8000\n",
            "\n",
            "================================================================================\n",
            "OKT + SentencePiece í† í¬ë‚˜ì´ì œì´ì…˜ í…ŒìŠ¤íŠ¸\n",
            "================================================================================\n",
            "\n",
            "ì›ë³¸: ì´ ì˜í™” ì •ë§ ì¬ë¯¸ìˆì—ˆì–´\n",
            "í˜•íƒœì†Œ: ì´ ì˜í™” ì •ë§ ì¬ë¯¸ìˆì—ˆì–´\n",
            "ì¸ì½”ë”©: [7, 5, 17, 3089, 71]\n",
            "ë””ì½”ë”©: ì´ ì˜í™” ì •ë§ ì¬ë¯¸ìˆì—ˆì–´\n",
            "\n",
            "ì›ë³¸: ìµœì•…ì˜ ì˜í™”ì˜€ë‹¤\n",
            "í˜•íƒœì†Œ: ìµœì•… ì˜ ì˜í™” ì˜€ë‹¤\n",
            "ì¸ì½”ë”©: [169, 76, 5, 415]\n",
            "ë””ì½”ë”©: ìµœì•… ì˜ ì˜í™” ì˜€ë‹¤\n",
            "\n",
            "ì›ë³¸: ë°°ìš°ë“¤ì˜ ì—°ê¸°ê°€ í›Œë¥­í–ˆë‹¤\n",
            "í˜•íƒœì†Œ: ë°°ìš° ë“¤ ì˜ ì—°ê¸° ê°€ í›Œë¥­í–ˆë‹¤\n",
            "ì¸ì½”ë”©: [63, 11, 76, 31, 25, 729, 635]\n",
            "ë””ì½”ë”©: ë°°ìš° ë“¤ ì˜ ì—°ê¸° ê°€ í›Œë¥­í–ˆë‹¤\n",
            "\n",
            "ğŸ“Š ì›ë³¸ ë°ì´í„°:\n",
            "   Corpus í¬ê¸°: 144,478\n",
            "   Labels í¬ê¸°: 144,478\n",
            "\n",
            "ğŸ”„ OKT í˜•íƒœì†Œ ë¶„ì„ + SentencePiece í† í¬ë‚˜ì´ì œì´ì…˜ ì§„í–‰ ì¤‘...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OKT + SentencePiece: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 144478/144478 [06:51<00:00, 351.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ… í† í¬ë‚˜ì´ì œì´ì…˜ ì™„ë£Œ!\n",
            "   ìµœì¢… í…ì„œ shape: torch.Size([144478, 111])\n",
            "   Vocab size: 8,000\n",
            "   ê³ ìœ  í† í° ìˆ˜: 7,989\n",
            "\n",
            "ğŸ”„ OKT í˜•íƒœì†Œ ë¶„ì„ + SentencePiece í† í¬ë‚˜ì´ì œì´ì…˜ ì§„í–‰ ì¤‘...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OKT + SentencePiece: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48700/48700 [02:09<00:00, 376.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ… í† í¬ë‚˜ì´ì œì´ì…˜ ì™„ë£Œ!\n",
            "   ìµœì¢… í…ì„œ shape: torch.Size([48700, 110])\n",
            "   Vocab size: 8,000\n",
            "   ê³ ìœ  í† í° ìˆ˜: 7,355\n",
            "\n",
            "âœ… DataLoader ìƒì„± ì™„ë£Œ!\n",
            "   Train samples: 115,583 â†’ 3612 batches\n",
            "   Val samples: 28,895 â†’ 903 batches\n",
            "   Test samples: 48,700 â†’ 1522 batches\n",
            "   Batch size: 32\n",
            "============================================================\n",
            "ğŸ“Š DataLoader ìƒ˜í”Œ í™•ì¸\n",
            "============================================================\n",
            "Train - ì…ë ¥ í…ì„œ shape: torch.Size([32, 111])\n",
            "Train - ë ˆì´ë¸” shape: torch.Size([32])\n",
            "Val - ì…ë ¥ í…ì„œ shape: torch.Size([32, 111])\n",
            "Val - ë ˆì´ë¸” shape: torch.Size([32])\n",
            "Test - ì…ë ¥ í…ì„œ shape: torch.Size([32, 110])\n",
            "Test - ë ˆì´ë¸” shape: torch.Size([32])\n",
            "ğŸ“Š ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°:\n",
            "   VOCAB_SIZE: 8,000 (SentencePiece ë™ì  ì„¤ì •)\n",
            "Using device: cuda\n",
            "\n",
            "============================================================\n",
            "--- LSTM Model Training starts (OKT + SentencePiece) ---\n",
            "============================================================\n",
            "\n",
            "Epoch: 01 | Time: 0m 32s\n",
            "\tTrain Loss: 0.538 | Train Acc: 71.77%\n",
            "\t Val. Loss: 0.453 |  Val. Acc: 78.59%\n",
            "\t>> Validation loss improved (0.453). Model saved.\n",
            "Epoch: 02 | Time: 0m 31s\n",
            "\tTrain Loss: 0.426 | Train Acc: 80.36%\n",
            "\t Val. Loss: 0.407 |  Val. Acc: 81.10%\n",
            "\t>> Validation loss improved (0.407). Model saved.\n",
            "Epoch: 03 | Time: 0m 32s\n",
            "\tTrain Loss: 0.382 | Train Acc: 82.94%\n",
            "\t Val. Loss: 0.401 |  Val. Acc: 82.07%\n",
            "\t>> Validation loss improved (0.401). Model saved.\n",
            "Epoch: 04 | Time: 0m 32s\n",
            "\tTrain Loss: 0.354 | Train Acc: 84.53%\n",
            "\t Val. Loss: 0.391 |  Val. Acc: 82.82%\n",
            "\t>> Validation loss improved (0.391). Model saved.\n",
            "Epoch: 05 | Time: 0m 32s\n",
            "\tTrain Loss: 0.331 | Train Acc: 85.84%\n",
            "\t Val. Loss: 0.394 |  Val. Acc: 83.23%\n",
            "\t>> Validation loss did not improve. Counter: 1/5\n",
            "Epoch: 06 | Time: 0m 32s\n",
            "\tTrain Loss: 0.314 | Train Acc: 86.53%\n",
            "\t Val. Loss: 0.367 |  Val. Acc: 84.12%\n",
            "\t>> Validation loss improved (0.367). Model saved.\n",
            "Epoch: 07 | Time: 0m 32s\n",
            "\tTrain Loss: 0.296 | Train Acc: 87.53%\n",
            "\t Val. Loss: 0.376 |  Val. Acc: 84.11%\n",
            "\t>> Validation loss did not improve. Counter: 1/5\n",
            "Epoch: 08 | Time: 0m 32s\n",
            "\tTrain Loss: 0.282 | Train Acc: 88.25%\n",
            "\t Val. Loss: 0.374 |  Val. Acc: 84.53%\n",
            "\t>> Validation loss did not improve. Counter: 2/5\n",
            "Epoch: 09 | Time: 0m 32s\n",
            "\tTrain Loss: 0.267 | Train Acc: 88.96%\n",
            "\t Val. Loss: 0.380 |  Val. Acc: 84.24%\n",
            "\t>> Validation loss did not improve. Counter: 3/5\n",
            "Epoch: 10 | Time: 0m 32s\n",
            "\tTrain Loss: 0.254 | Train Acc: 89.70%\n",
            "\t Val. Loss: 0.393 |  Val. Acc: 84.71%\n",
            "\t>> Validation loss did not improve. Counter: 4/5\n",
            "Epoch: 11 | Time: 0m 32s\n",
            "\tTrain Loss: 0.242 | Train Acc: 90.30%\n",
            "\t Val. Loss: 0.391 |  Val. Acc: 84.43%\n",
            "\t>> Validation loss did not improve. Counter: 5/5\n",
            "--- Early stopping triggered after 11 epochs ---\n",
            "\n",
            "--- Loading best LSTM model for test evaluation ---\n",
            "\n",
            "--- LSTM Model Test Results (Best Model) ---\n",
            "\tTest Loss: 0.373\n",
            "\tTest Acc:  83.72%\n"
          ]
        }
      ]
    }
  ]
}