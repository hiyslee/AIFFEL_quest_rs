{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# 필요한 라이브러리를 가져옵니다.\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "import sentencepiece as spm\n",
        "\n",
        "import math\n",
        "import os\n",
        "import re\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "xz_3HH6UO976"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sentence piece를 설치합니다\n",
        "!pip install sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fAMzQcGcNMFY",
        "outputId": "dcbed089-505c-4631-b659-1c474a54f0b7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (0.2.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 포지셔널 행렬을 구현합니다.\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, position, d_model):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.position = position\n",
        "\n",
        "        self.pos_encoding = self._build_pos_encoding(position, d_model)\n",
        "\n",
        "    def _get_angles(self, position, i, d_model):\n",
        "        return 1.0 / (10000.0 ** ((2.0 * (i // 2)) / d_model)) * position\n",
        "\n",
        "    def _build_pos_encoding(self, position, d_model):\n",
        "        pos = torch.arange(position, dtype=torch.float32).unsqueeze(1)\n",
        "        i = torch.arange(d_model, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "        angle_rads = self._get_angles(pos, i, d_model)\n",
        "        sines = torch.sin(angle_rads[:, 0::2])\n",
        "        cosines = torch.cos(angle_rads[:, 1::2])\n",
        "\n",
        "        pos_encoding = torch.zeros(position, d_model)\n",
        "        pos_encoding[:, 0::2] = sines\n",
        "        pos_encoding[:, 1::2] = cosines\n",
        "\n",
        "        pos_encoding = pos_encoding.unsqueeze(0)  # shape: [1, position, d_model]\n",
        "        return pos_encoding\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pos_encoding[:, :x.size(1), :].to(x.device)"
      ],
      "metadata": {
        "id": "L9iMlWE7PEFR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 스케일드 닷 프러덕트 어텐션 합수를 구현함\n",
        "def scaled_dot_product_attention(query, key, value, mask=None):\n",
        "\n",
        "    # 1) Q와 K의 내적을 통해 score(유사도) 계산\n",
        "    # key.transpose(-1, -2): (batch_size, heads, depth, seq_len)\n",
        "    # matmul 결과 shape: (batch_size, heads, seq_len, seq_len)\n",
        "    matmul_qk = torch.matmul(query, key.transpose(-1, -2))\n",
        "\n",
        "    # 2) depth에 따라 정규화\n",
        "    depth = key.size(-1)  # depth = d_model / heads\n",
        "    logits = matmul_qk / math.sqrt(depth)\n",
        "\n",
        "    # 3) 마스크가 주어졌다면 -1e9(아주 작은 값)를 더해 소프트맥스에서 제외시키도록 함\n",
        "    if mask is not None:\n",
        "        # 텐서플로우: logits += (mask * -1e9)\n",
        "        # 파이토치 동일 적용\n",
        "        logits = logits + (mask * -1e9)\n",
        "\n",
        "    # 4) 소프트맥스 계산해 attention weights 생성\n",
        "    attention_weights = F.softmax(logits, dim=-1)\n",
        "\n",
        "    # 5) attention weights와 value의 내적\n",
        "    output = torch.matmul(attention_weights, value)\n",
        "\n",
        "    return output, attention_weights"
      ],
      "metadata": {
        "id": "NdHfaT23QTFN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # d_model은 num_heads로 나누어 떨어져야 함.\n",
        "        assert d_model % num_heads == 0\n",
        "\n",
        "        self.depth = d_model // num_heads\n",
        "\n",
        "        # 파이토치에서 Dense는 nn.Linear로 대응\n",
        "        self.query_dense = nn.Linear(d_model, d_model)\n",
        "        self.key_dense = nn.Linear(d_model, d_model)\n",
        "        self.value_dense = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.out_dense = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        \"\"\"\n",
        "        x: (batch_size, seq_len, d_model)\n",
        "        => (batch_size, num_heads, seq_len, depth) 형태로 변환\n",
        "        \"\"\"\n",
        "        x = x.view(batch_size, -1, self.num_heads, self.depth)\n",
        "        x = x.permute(0, 2, 1, 3)  # (batch_size, num_heads, seq_len, depth)\n",
        "        return x\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        \"\"\"\n",
        "        query, key, value: (batch_size, seq_len, d_model)\n",
        "        mask: (batch_size, 1, seq_len, seq_len) 등으로 broadcast 가능하도록 구성\n",
        "        \"\"\"\n",
        "        batch_size = query.size(0)\n",
        "\n",
        "        # Q, K, V에 각각 Linear 적용\n",
        "        query = self.query_dense(query)\n",
        "        key = self.key_dense(key)\n",
        "        value = self.value_dense(value)\n",
        "\n",
        "        # Head 분할\n",
        "        query = self.split_heads(query, batch_size)\n",
        "        key = self.split_heads(key, batch_size)\n",
        "        value = self.split_heads(value, batch_size)\n",
        "\n",
        "        # 스케일드 닷 프로덕트 어텐션\n",
        "        scaled_attention, _ = scaled_dot_product_attention(query, key, value, mask)\n",
        "\n",
        "        # (batch_size, num_heads, seq_len, depth) -> (batch_size, seq_len, num_heads, depth)\n",
        "        scaled_attention = scaled_attention.permute(0, 2, 1, 3).contiguous()\n",
        "\n",
        "        # 다시 (batch_size, seq_len, d_model)로 합치기\n",
        "        concat_attention = scaled_attention.view(batch_size, -1, self.d_model)\n",
        "\n",
        "        # 최종 Dense\n",
        "        output = self.out_dense(concat_attention)\n",
        "        return output"
      ],
      "metadata": {
        "id": "F-ss49uLc5fg"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 패딩 마스킹을 구현합니다.\n",
        "def create_padding_mask(x):\n",
        "    # x == 0 위치를 찾아 float형 1로 변환\n",
        "    mask = (x == 0).float()\n",
        "    # (batch_size, seq_len) -> (batch_size, 1, 1, seq_len)\n",
        "    mask = mask.unsqueeze(1).unsqueeze(2)\n",
        "    return mask"
      ],
      "metadata": {
        "id": "2x2r5_cjdgYR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 록 어헤드 마스킹 함수를 구현합니다.\n",
        "def create_look_ahead_mask(x):\n",
        "    seq_len = x.size(1)\n",
        "\n",
        "    # (seq_len, seq_len) 크기의 하삼각 행렬(tril) 생성 후 1에서 빼서\n",
        "    # 상삼각이 1, 하삼각(자기 자신 포함)이 0이 되도록 설정\n",
        "    # => 미래 토큰(자신 인덱스보다 큰 위치) 마스킹\n",
        "    look_ahead_mask = 1 - torch.tril(torch.ones((seq_len, seq_len)))\n",
        "\n",
        "    # 패딩 마스크 생성 (shape: (batch_size, 1, 1, seq_len))\n",
        "    padding_mask = create_padding_mask(x)\n",
        "\n",
        "    # look_ahead_mask: (seq_len, seq_len) -> (1, seq_len, seq_len)\n",
        "    look_ahead_mask = look_ahead_mask.unsqueeze(0)\n",
        "    # -> (1, seq_len, seq_len) -> (1, 1, seq_len, seq_len)\n",
        "    look_ahead_mask = look_ahead_mask.unsqueeze(1)\n",
        "    look_ahead_mask = look_ahead_mask.to(x.device)\n",
        "\n",
        "    # look-ahead 마스크와 패딩 마스크를 합성 (둘 중 하나라도 1이면 마스킹)\n",
        "    # 최종 shape은 브로드캐스팅으로 (batch_size, 1, seq_len, seq_len)\n",
        "    combined_mask = torch.max(look_ahead_mask, padding_mask)\n",
        "    return combined_mask"
      ],
      "metadata": {
        "id": "XFOn2KVlk5Tb"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 인코더 층을 구현하는 함수입니다\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, ff_dim, dropout=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)  # 이전에 구현한 MHA\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.norm1 = nn.LayerNorm(d_model, eps=1e-6)\n",
        "\n",
        "        # 피드포워드 부분 (Dense -> ReLU -> Dense)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(d_model, ff_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(ff_dim, d_model)\n",
        "        )\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.norm2 = nn.LayerNorm(d_model, eps=1e-6)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # (1) 멀티 헤드 어텐션 (셀프 어텐션)\n",
        "        attn_output = self.mha(x, x, x, mask)  # (batch_size, seq_len, d_model)\n",
        "        attn_output = self.dropout1(attn_output)\n",
        "        out1 = self.norm1(x + attn_output)     # 잔차 연결 + LayerNorm\n",
        "\n",
        "        # (2) 피드포워드 신경망\n",
        "        ffn_output = self.ffn(out1)            # (batch_size, seq_len, d_model)\n",
        "        ffn_output = self.dropout2(ffn_output)\n",
        "        out2 = self.norm2(out1 + ffn_output)   # 잔차 연결 + LayerNorm\n",
        "\n",
        "        return out2\n"
      ],
      "metadata": {
        "id": "usfH3NR6lS5A"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 인코더 층을 쌓아 인코더를 만듭니다.\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 vocab_size,\n",
        "                 num_layers,\n",
        "                 ff_dim,\n",
        "                 d_model,\n",
        "                 num_heads,\n",
        "                 dropout=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # (1) 임베딩 레이어\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "        # (2) 포지셔널 인코딩\n",
        "        self.pos_encoding = PositionalEncoding(position=vocab_size, d_model=d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # (3) EncoderLayer 쌓기\n",
        "        self.enc_layers = nn.ModuleList([\n",
        "            EncoderLayer(d_model, num_heads, ff_dim, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # (1) 임베딩 & sqrt(d_model)로 스케일링\n",
        "        x = self.embedding(x) * math.sqrt(self.d_model)\n",
        "\n",
        "        # (2) 포지셔널 인코딩 적용 + 드롭아웃\n",
        "        x = self.pos_encoding(x)  # shape: (batch_size, seq_len, d_model)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # (3) num_layers만큼 쌓아올린 EncoderLayer 통과\n",
        "        for layer in self.enc_layers:\n",
        "            x = layer(x, mask)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "FmVUGo62li5Q"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 디코더 layer를 구현합니다\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, ff_dim, dropout=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        # 첫 번째 서브 레이어 (디코더 내부 셀프 어텐션)\n",
        "        self.self_mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(d_model, eps=1e-6)\n",
        "\n",
        "        # 두 번째 서브 레이어 (인코더-디코더 어텐션)\n",
        "        self.encdec_mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm2 = nn.LayerNorm(d_model, eps=1e-6)\n",
        "\n",
        "        # 세 번째 서브 레이어 (피드포워드 네트워크)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(d_model, ff_dim),  # Dense(units=ff_dim)\n",
        "            nn.ReLU(),                   # activation='relu'\n",
        "            nn.Linear(ff_dim, d_model)   # Dense(units=d_model)\n",
        "        )\n",
        "        self.norm3 = nn.LayerNorm(d_model, eps=1e-6)\n",
        "\n",
        "        # 드롭아웃\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_outputs, look_ahead_mask=None, padding_mask=None):\n",
        "        # 1) 셀프 어텐션 (디코더 내부)\n",
        "        self_attn_out = self.self_mha(x, x, x, mask=look_ahead_mask)\n",
        "        self_attn_out = self.dropout1(self_attn_out)\n",
        "        out1 = self.norm1(x + self_attn_out)  # 잔차 연결 + LayerNorm\n",
        "\n",
        "        # 2) 인코더-디코더 어텐션\n",
        "        encdec_attn_out = self.encdec_mha(out1, enc_outputs, enc_outputs, mask=padding_mask)\n",
        "        encdec_attn_out = self.dropout2(encdec_attn_out)\n",
        "        out2 = self.norm2(out1 + encdec_attn_out)  # 잔차 연결 + LayerNorm\n",
        "\n",
        "        # 3) 피드포워드 (Dense -> ReLU -> Dense)\n",
        "        ffn_out = self.ffn(out2)\n",
        "        ffn_out = self.dropout3(ffn_out)\n",
        "        out3 = self.norm3(out2 + ffn_out)  # 잔차 연결 + LayerNorm\n",
        "\n",
        "        return out3"
      ],
      "metadata": {
        "id": "1FmVU9u5lnHf"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 디코더 layer를 쌓아서 디코더를 만듭니다.\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 vocab_size,\n",
        "                 num_layers,\n",
        "                 ff_dim,\n",
        "                 d_model,\n",
        "                 num_heads,\n",
        "                 dropout=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # (1) 임베딩 레이어\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "        # (2) 포지셔널 인코딩\n",
        "        # 실제 학습 시에는 최대 시퀀스 길이에 맞추어 쓰기도 함\n",
        "        self.pos_encoding = PositionalEncoding(position=vocab_size, d_model=d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # (3) DecoderLayer 쌓기\n",
        "        self.dec_layers = nn.ModuleList([\n",
        "            DecoderLayer(d_model, num_heads, ff_dim, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x, enc_outputs, look_ahead_mask=None, padding_mask=None):\n",
        "        # (1) 임베딩 + sqrt(d_model)로 스케일링\n",
        "        x = self.embedding(x) * math.sqrt(self.d_model)\n",
        "\n",
        "        # (2) 포지셔널 인코딩 + 드롭아웃\n",
        "        x = self.pos_encoding(x)    # (batch_size, tgt_seq_len, d_model)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # (3) num_layers만큼 쌓인 DecoderLayer 통과\n",
        "        for layer in self.dec_layers:\n",
        "            x = layer(x, enc_outputs, look_ahead_mask, padding_mask)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "0YmCF7VVltWi"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 송영숙님이 공개한 챗봇 데이터를 가져옵니다.\n",
        "! mkdir -p ~/work/transformer_chatbot/data/ && cd ~/work/transformer_chatbot/data/\n",
        "! wget https://github.com/songys/Chatbot_data/raw/master/ChatbotData.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHO30uoTfl6p",
        "outputId": "5b71ae7c-b2ae-4969-89ee-5eae6e133cf2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-11-04 23:51:26--  https://github.com/songys/Chatbot_data/raw/master/ChatbotData.csv\n",
            "Resolving github.com (github.com)... 140.82.121.3\n",
            "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv [following]\n",
            "--2025-11-04 23:51:26--  https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 889842 (869K) [text/plain]\n",
            "Saving to: ‘ChatbotData.csv’\n",
            "\n",
            "ChatbotData.csv     100%[===================>] 868.99K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2025-11-04 23:51:26 (45.8 MB/s) - ‘ChatbotData.csv’ saved [889842/889842]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data description**\n",
        "\n",
        "인공데이터입니다. 일부 이별과 관련된 질문에서 다음카페 \"사랑보다 아름다운 실연( http://cafe116.daum.net/_c21_/home?grpid=1bld )\"에서 자주 나오는 이야기들을 참고하여 제작하였습니다. 가령 \"이별한 지 열흘(또는 100일) 되었어요\"라는 질문에 챗봇이 위로한다는 취지로 답변을 작성하였습니다.\n",
        "\n",
        "챗봇 트레이닝용 문답 페어 11,876개\n",
        "일상다반사 0, 이별(부정) 1, 사랑(긍정) 2로 레이블링\n",
        "\n",
        "**관련 코드 : Korean Language Model for Wellness Conversation**\n",
        "\n",
        "이 곳에 저장된 데이터를 만들면서 누군가에게 위로가 되는 모델이 나오면 좋겠다고 생각했었는데 제 생각보다 더 잘 만든 모델이 있어서 링크 걸어 둡니다. 부족한 데이터지만 이곳에 저장된 데이터와 AI 허브 정신건강 상담 데이터 를 토대로 만들었다고 합니다.\n",
        "전창욱 외(2020), 텐서플로2와 머신러닝으로 시작하는 자연어처리, 위키북스( http://cafe116.daum.net/_c21_/home?grpid=1bld )의 챗봇 부분에도 이 데이터가 사용된 것으로 알고 있습니다. 빠르게 챗봇 만들고 싶으신 분들은 참고하셔도 좋을 것 같습니다.\n",
        "데이터 로더를 통한 다운로드는 다음 링크 Korpora: Korean Corpora Archives를 참고하시면 편하게 사용하실 수 있을 듯합니다.\n",
        "\n",
        "#인용\n",
        "Youngsook Song.(2018). Chatbot_data_for_Korean v1.0)[Online]. Available : https://github.com/songys/Chatbot_data (downloaded 2022. June. 29.)"
      ],
      "metadata": {
        "id": "w3_1qtiof8eL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터 형태가 어떤가 체크\n",
        "import pandas as pd\n",
        "\n",
        "path_to_dataset = \"/content/ChatbotData.csv\"\n",
        "\n",
        "df = pd.read_csv(\"/content/ChatbotData.csv\")\n",
        "\n",
        "print(\"컬럼:\", df.columns.tolist())\n",
        "print(\"\\n앞 데이터 5개:\")\n",
        "print(df.head())\n",
        "print(\"\\n데이터 정보:\")\n",
        "print(df.info())\n",
        "print(df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsXoBppjzzyJ",
        "outputId": "4da68783-b331-43f1-d45d-067027fe9615"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "컬럼: ['Q', 'A', 'label']\n",
            "\n",
            "앞 데이터 5개:\n",
            "                 Q            A  label\n",
            "0           12시 땡!   하루가 또 가네요.      0\n",
            "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
            "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
            "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
            "4          PPL 심하네   눈살이 찌푸려지죠.      0\n",
            "\n",
            "데이터 정보:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 11823 entries, 0 to 11822\n",
            "Data columns (total 3 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   Q       11823 non-null  object\n",
            " 1   A       11823 non-null  object\n",
            " 2   label   11823 non-null  int64 \n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 277.2+ KB\n",
            "None\n",
            "(11823, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 전처리 함수\n",
        "def preprocess_sentence(sentence):\n",
        "  # 입력받은 sentence를 소문자로 변경하고 양쪽 공백을 제거\n",
        "  # 한국어는 소문자 변환이 크게 의미 없을 수 있으나, 일관성을 위해 유지\n",
        "  sentence = sentence.lower().strip()\n",
        "\n",
        "  # 단어와 구두점(punctuation) 사이의 거리를 만듭니다.\n",
        "  # ? !와 한글, 영어, 숫자 외의 문자는 공백으로 치환. .,가 너무 많이 나옴\n",
        "  # 한글 자음, 모음 분리 현상은 SentencePiece가 처리하므로 여기서는 제거 안함\n",
        "  sentence = re.sub(r\"([?!])\", r\" \\1 \", sentence)\n",
        "  sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
        "\n",
        "  # 한글, 영어, 숫자, 그리고 \"?\", \"!\" 만 남기고 모두 제거\n",
        "  sentence = re.sub(r\"[^가-힣a-zA-Z0-9?!]+\", \" \", sentence)\n",
        "  sentence = sentence.strip()\n",
        "  return sentence\n",
        "print(\"슝=3\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oa2lh3mdawU7",
        "outputId": "01047535-5c69-4b01-cfd1-f450638c2975"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "슝=3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_cb_data(path_to_dataset):\n",
        "    # Load the data from the CSV file\n",
        "    df = pd.read_csv(path_to_dataset)\n",
        "\n",
        "    # Drop rows with any missing values in 'Q' or 'A' columns\n",
        "    df = df.dropna(subset=['Q', 'A'])\n",
        "\n",
        "    # Drop duplicate rows\n",
        "    df = df.drop_duplicates(subset=['Q', 'A'])\n",
        "\n",
        "    pairs = []\n",
        "    for index, row in df.iterrows():\n",
        "        q = preprocess_sentence(row['Q'])\n",
        "        a = preprocess_sentence(row['A'])\n",
        "        # Only add the pair if both question and answer are not empty after preprocessing\n",
        "        if q and a:\n",
        "            pairs.append((q, a))\n",
        "\n",
        "    return pairs"
      ],
      "metadata": {
        "id": "veWC560JbXAL"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pairs = read_cb_data(path_to_dataset)\n",
        "\n",
        "print('전체 샘플 수 :', len(pairs))\n",
        "print(pairs[:5]) # Print first few pairs to verify"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hkWSA_HdH2S",
        "outputId": "ac82474c-3280-45c4-d115-0722af5dde21"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전체 샘플 수 : 11750\n",
            "[('12시 땡 !', '하루가 또 가네요'), ('1지망 학교 떨어졌어', '위로해 드립니다'), ('3박4일 놀러가고 싶다', '여행은 언제나 좋죠'), ('3박4일 정도 놀러가고 싶다', '여행은 언제나 좋죠'), ('ppl 심하네', '눈살이 찌푸려지죠')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_file = \"clean_corpus.txt\"\n",
        "with open(corpus_file, 'w', encoding='utf-8') as f:\n",
        "    for q, a in pairs:\n",
        "        # Add special tokens for start and end of sentence\n",
        "        f.write(q + \"\\n\")\n",
        "        f.write(a + \"\\n\")"
      ],
      "metadata": {
        "id": "pUkA5vjumLuO"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spm.SentencePieceTrainer.Train(\n",
        "    input=corpus_file,\n",
        "    model_prefix=\"spm_cb\",\n",
        "    vocab_size=1200,  # Increase vocabulary size now that data is correctly preprocessed\n",
        "    character_coverage=1.0,\n",
        "    model_type=\"bpe\",\n",
        "    max_sentence_length=40, #40 설정\n",
        "    bos_id=1,  # <s> (Beginning of Sentence) 설정\n",
        "    eos_id=2,  # </s> (End of Sentence) 설정\n",
        "    pad_id=0,  # Padding ID 설정\n",
        "    unk_id=3   # Unknown Token ID 설정\n",
        ")\n",
        "\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.Load(\"spm_cb.model\")\n",
        "\n",
        "print(f\"SentencePiece vocabulary size: {sp.GetPieceSize()}\")"
      ],
      "metadata": {
        "id": "NJ3ChDXWn2KL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f6cf50e-3f96-4ed8-ff10-145ea8ceb001"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SentencePiece vocabulary size: 1200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sp = spm.SentencePieceProcessor()\n",
        "sp.Load(\"spm_cb.model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHS9VRuJn9Sz",
        "outputId": "58efc26a-5334-46e3-937f-917b3631753e"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 예제 문장\n",
        "sentence = \"한국말을 테스트 하는 중이야!\"\n",
        "\n",
        "sentence = preprocess_sentence(sentence)\n",
        "print(\"전처리 후의 문장:\", sentence)\n",
        "\n",
        "# 1. 토크나이징 (subword 단위로 분할)\n",
        "tokens = sp.encode(sentence, out_type=str)\n",
        "print(\"Tokenized:\", tokens)\n",
        "\n",
        "# 2. 인코딩 (서브워드를 정수 ID로 변환)\n",
        "encoded = sp.encode(sentence, out_type=int)\n",
        "print(\"Encoded:\", encoded)\n",
        "\n",
        "# 3. 디코딩 (정수 ID → 원본 문장 복원)\n",
        "decoded = sp.decode(encoded)\n",
        "print(\"Decoded:\", decoded)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_WQOdV9oWZL",
        "outputId": "8659a9ae-23d3-4d8c-a378-abddc3eb540e"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전처리 후의 문장: 한국말을 테스트 하는 중이야 !\n",
            "Tokenized: ['▁', '한', '국', '말', '을', '▁', '테', '스', '트', '▁하', '는', '▁', '중', '이', '야', '▁', '!']\n",
            "Encoded: [47, 75, 362, 102, 63, 47, 214, 136, 197, 19, 58, 47, 161, 49, 85, 47, 148]\n",
            "Decoded: 한국말을 테스트 하는 중이야 !\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class cbDataset(Dataset):\n",
        "    def __init__(self, pairs, sp, max_length=40):\n",
        "        super().__init__()\n",
        "        self.sp = sp\n",
        "        self.max_length = max_length\n",
        "        self.data = []\n",
        "\n",
        "        for q_text, a_text in pairs:\n",
        "            # 1) 토크나이즈\n",
        "            q_ids = sp.EncodeAsIds(q_text)\n",
        "            a_ids = sp.EncodeAsIds(a_text)\n",
        "\n",
        "            # 2) [CLS]/[SEP] 같은 별도 스페셜 토큰을 쓸 수도 있으나,\n",
        "            #    여기서는 SentencePiece 기본 <s>, </s> 등 혹은 사용자 정의 토큰 활용 가능\n",
        "            #    간단히 <s>=sp.bos_id(), </s>=sp.eos_id()로 가정해본다면:\n",
        "            #    sp.SetEncodeExtraOptions(\"bos:eos\") 등으로 설정하는 방법도 있음.\n",
        "            # 여기서는 수동으로 bos/eos id를 붙인다고 가정\n",
        "            bos_id = sp.bos_id() if sp.bos_id() >= 0 else 1  # 혹은 임의값\n",
        "            eos_id = sp.eos_id() if sp.eos_id() >= 0 else 2\n",
        "\n",
        "            q_tokens = [bos_id] + q_ids + [eos_id]\n",
        "            a_tokens = [bos_id] + a_ids + [eos_id]\n",
        "\n",
        "            # 3) 길이 제한\n",
        "            if len(q_tokens) > max_length or len(a_tokens) > max_length:\n",
        "                continue\n",
        "\n",
        "            # 4) 고정 길이 패딩\n",
        "            q_tokens += [0]*(max_length - len(q_tokens))  # 0 -> <pad> 가정\n",
        "            a_tokens += [0]*(max_length - len(a_tokens))\n",
        "\n",
        "            # 5) 디코더 입력(dec_input): a_tokens[:-1], 타겟(outputs): a_tokens[1:]\n",
        "            #    (teacher forcing용)\n",
        "            dec_input = a_tokens[:-1]\n",
        "            target = a_tokens[1:]\n",
        "\n",
        "            self.data.append({\n",
        "                \"enc_input\": q_tokens,\n",
        "                \"dec_input\": dec_input,\n",
        "                \"target\": target\n",
        "            })\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data[idx]\n",
        "        enc_input = torch.tensor(sample[\"enc_input\"], dtype=torch.long)\n",
        "        dec_input = torch.tensor(sample[\"dec_input\"], dtype=torch.long)\n",
        "        target = torch.tensor(sample[\"target\"], dtype=torch.long)\n",
        "        return enc_input, dec_input, target"
      ],
      "metadata": {
        "id": "MDuo1_vHobRY"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = cbDataset(pairs, sp, max_length=40)"
      ],
      "metadata": {
        "id": "ztuhKqXrC_Rc"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for encoder_input, decoder_input, decoder_label  in dataset:\n",
        "    print(\"텐서 크기 :\",encoder_input.size())\n",
        "    print(encoder_input)\n",
        "    print(sp.decode(encoder_input.tolist()))\n",
        "    print(decoder_input)\n",
        "    print(sp.decode(decoder_input.tolist()))\n",
        "    print(decoder_label)\n",
        "    print(sp.decode(decoder_label.tolist()))\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6Hk-LRfstxp",
        "outputId": "867d5ca5-c1c4-4eee-9206-9f93e2f15ba1"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "텐서 크기 : torch.Size([40])\n",
            "tensor([  1,  47, 322, 350,  84,  47, 723,  47, 148,   2,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0])\n",
            "12시 땡 !\n",
            "tensor([  1,  19, 327,  52,  47, 217,  20,  15,   2,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0])\n",
            "하루가 또 가네요\n",
            "tensor([ 19, 327,  52,  47, 217,  20,  15,   2,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0])\n",
            "하루가 또 가네요\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = DataLoader(dataset,batch_size=32,shuffle=True)"
      ],
      "metadata": {
        "id": "vfGbp5VCs0C4"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# decoder에 들어갈 데이터는 하나가 줄어들게 된다. 다음과 같은 데이터 형태가 맞다.\n",
        "for encoder_input, decoder_input, decoder_label in dataloader:\n",
        "    print(encoder_input.size())\n",
        "    print(decoder_input.size())\n",
        "    print(decoder_label.size())\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbFBAa-Cs3vG",
        "outputId": "1cabedb2-b75b-49ab-ec1f-dfc3119c5725"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 40])\n",
            "torch.Size([32, 39])\n",
            "torch.Size([32, 39])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 vocab_size,\n",
        "                 num_layers,      # 인코더/디코더 층 수\n",
        "                 units,           # feed-forward 네트워크의 중간 차원(ff_dim)\n",
        "                 d_model,         # 임베딩 및 내부 표현 차원\n",
        "                 num_heads,       # 멀티헤드 어텐션의 헤드 수\n",
        "                 dropout=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        # 인코더\n",
        "        self.encoder = Encoder(\n",
        "            vocab_size=vocab_size,\n",
        "            num_layers=num_layers,\n",
        "            ff_dim=units,\n",
        "            d_model=d_model,\n",
        "            num_heads=num_heads,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        # 디코더\n",
        "        self.decoder = Decoder(\n",
        "            vocab_size=vocab_size,\n",
        "            num_layers=num_layers,\n",
        "            ff_dim=units,\n",
        "            d_model=d_model,\n",
        "            num_heads=num_heads,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        # 최종 출력층: (d_model) -> (vocab_size)\n",
        "        self.final_linear = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "        # 참고: 텐서플로우 코드의 `name=\"transformer\"`는 파이토치에선 보통 사용 안 함\n",
        "\n",
        "    def forward(self, inputs, dec_inputs):\n",
        "        # 1) 인코더 패딩 마스크 생성\n",
        "        enc_padding_mask = create_padding_mask(inputs)     # shape (batch_size, 1, 1, src_seq_len)\n",
        "\n",
        "        # 2) 디코더 look-ahead + 패딩 마스크\n",
        "        look_ahead_mask = create_look_ahead_mask(dec_inputs)  # shape (batch_size, 1, tgt_seq_len, tgt_seq_len)\n",
        "\n",
        "        # 3) 디코더에서 인코더 출력 쪽을 마스킹할 때 쓸 패딩 마스크\n",
        "        dec_padding_mask = create_padding_mask(inputs)        # shape (batch_size, 1, 1, src_seq_len)\n",
        "\n",
        "        # 4) 인코더 수행\n",
        "        enc_outputs = self.encoder(\n",
        "            x=inputs,\n",
        "            mask=enc_padding_mask\n",
        "        )  # shape: (batch_size, src_seq_len, d_model)\n",
        "\n",
        "        # 5) 디코더 수행\n",
        "        dec_outputs = self.decoder(\n",
        "            x=dec_inputs,           # (batch_size, tgt_seq_len)\n",
        "            enc_outputs=enc_outputs,# (batch_size, src_seq_len, d_model)\n",
        "            look_ahead_mask=look_ahead_mask,\n",
        "            padding_mask=dec_padding_mask\n",
        "        )  # shape: (batch_size, tgt_seq_len, d_model)\n",
        "\n",
        "        # 6) 최종 Dense (vocab_size)\n",
        "        logits = self.final_linear(dec_outputs)  # (batch_size, tgt_seq_len, vocab_size)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "FQMqQKjXs6Nm"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 예: 하이퍼파라미터 설정\n",
        "NUM_LAYERS = 3     # 인코더/디코더 층 수\n",
        "D_MODEL = 256      # 임베딩 및 내부 표현 차원\n",
        "NUM_HEADS = 8      # 멀티헤드 어텐션에서의 헤드 수\n",
        "UNITS = 512        # 피드포워드 신경망의 은닉 차원\n",
        "DROPOUT = 0.1      # 드롭아웃 비율\n",
        "VOCAB_SIZE = 1200 # Update vocab size to match SentencePiece\n",
        "\n",
        "# 모델 생성\n",
        "model = Transformer(\n",
        "    vocab_size=VOCAB_SIZE,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    units=UNITS,\n",
        "    d_model=D_MODEL,\n",
        "    num_heads=NUM_HEADS,\n",
        "    dropout=DROPOUT\n",
        ")\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QzDI8Vqs_jf",
        "outputId": "b3039630-8729-4942-cf14-ae3ed5f71529"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformer(\n",
            "  (encoder): Encoder(\n",
            "    (embedding): Embedding(1200, 256)\n",
            "    (pos_encoding): PositionalEncoding()\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "    (enc_layers): ModuleList(\n",
            "      (0-2): 3 x EncoderLayer(\n",
            "        (mha): MultiHeadAttention(\n",
            "          (query_dense): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (key_dense): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (value_dense): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (out_dense): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (dropout1): Dropout(p=0.1, inplace=False)\n",
            "        (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
            "        (ffn): Sequential(\n",
            "          (0): Linear(in_features=256, out_features=512, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=512, out_features=256, bias=True)\n",
            "        )\n",
            "        (dropout2): Dropout(p=0.1, inplace=False)\n",
            "        (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (embedding): Embedding(1200, 256)\n",
            "    (pos_encoding): PositionalEncoding()\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "    (dec_layers): ModuleList(\n",
            "      (0-2): 3 x DecoderLayer(\n",
            "        (self_mha): MultiHeadAttention(\n",
            "          (query_dense): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (key_dense): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (value_dense): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (out_dense): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
            "        (encdec_mha): MultiHeadAttention(\n",
            "          (query_dense): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (key_dense): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (value_dense): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (out_dense): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
            "        (ffn): Sequential(\n",
            "          (0): Linear(in_features=256, out_features=512, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=512, out_features=256, bias=True)\n",
            "        )\n",
            "        (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout1): Dropout(p=0.1, inplace=False)\n",
            "        (dropout2): Dropout(p=0.1, inplace=False)\n",
            "        (dropout3): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (final_linear): Linear(in_features=256, out_features=1200, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_function = nn.CrossEntropyLoss(ignore_index=sp.pad_id())"
      ],
      "metadata": {
        "id": "AD_HSMULB0v-"
      },
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_lr_lambda(d_model, warmup_steps=4000):\n",
        "    d_model = float(d_model)\n",
        "    def lr_lambda(step):\n",
        "        # step은 0부터 시작하므로 +1로 보정\n",
        "        step = step + 1\n",
        "        return (d_model ** -0.5) * min(step ** -0.5, step * (warmup_steps ** -1.5))\n",
        "    return lr_lambda"
      ],
      "metadata": {
        "id": "MBlpwKuGB32d"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 하이퍼파라미터 설정\n",
        "d_model = 256\n",
        "warmup_steps = 4000\n",
        "total_steps = 200000  # 총 학습 스텝\n",
        "\n",
        "# 학습률 스케줄 시각화\n",
        "steps = np.arange(1, total_steps + 1)\n",
        "learning_rates = [get_lr_lambda(d_model, warmup_steps)(step) for step in steps]\n",
        "\n",
        "# 그래프 출력\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(steps, learning_rates, label=\"Learning Rate\")\n",
        "plt.xlabel(\"Training Steps\")\n",
        "plt.ylabel(\"Learning Rate\")\n",
        "plt.title(\"Transformer Learning Rate Schedule\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "UNIWNsiUB8Gc",
        "outputId": "d48c1b80-f7b9-4976-d0aa-2534dfba04e6"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizer 정의\n",
        "optimizer = optim.Adam(model.parameters(), betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "# Scheduler 정의\n",
        "scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=get_lr_lambda(D_MODEL, warmup_steps=4000))\n",
        "\n",
        "def accuracy_function(y_pred, y_true, pad_id=0):\n",
        "    \"\"\"\n",
        "    y_pred: (batch_size, seq_len, vocab_size)\n",
        "    y_true: (batch_size, seq_len)\n",
        "    \"\"\"\n",
        "    preds = y_pred.argmax(dim=-1)  # (batch_size, seq_len)\n",
        "    mask = (y_true != pad_id)\n",
        "    correct = (preds == y_true) & mask\n",
        "    acc = correct.float().sum() / mask.float().sum()\n",
        "    return acc"
      ],
      "metadata": {
        "id": "okXtNlMvCFBr"
      },
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1162c00",
        "outputId": "70b9cdf3-39ed-4af1-d300-58953e4022b4"
      },
      "source": [
        "# Define the device to use for training\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(model, batch, optimizer, loss_function, device):\n",
        "    model.train()\n",
        "    enc_input, dec_input, target = [x.to(device) for x in batch]\n",
        "\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # 모델 포워드 패스\n",
        "    logits = model(enc_input, dec_input)  # (batch_size, seq_len, vocab_size)\n",
        "\n",
        "    # Loss 계산 (패딩 토큰 무시)\n",
        "    loss = loss_function(logits.permute(0, 2, 1), target)  # (batch_size, vocab_size, seq_len) 필요\n",
        "\n",
        "    # Backpropagation\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item(), accuracy_function(logits, target, pad_id=sp.pad_id())"
      ],
      "metadata": {
        "id": "gwNdwPpQCMhW"
      },
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, dataloader, optimizer, loss_function, scheduler, num_epochs, device):\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss, total_acc = 0, 0\n",
        "        for step, batch in enumerate(dataloader):\n",
        "            loss, acc = train_step(model, batch, optimizer, loss_function, device)\n",
        "            total_loss += loss\n",
        "            total_acc += acc\n",
        "\n",
        "            # 일정 스텝마다 로그 출력 - 출력 다하니 넘 느려지네\n",
        "           # if step % 100 == 0:\n",
        "           #     print(f\"[Epoch {epoch+1}, Step {step}] Loss: {loss:.4f}, Acc: {acc:.4f}\")\n",
        "\n",
        "            # 학습률 스케줄러 업데이트\n",
        "            scheduler.step()\n",
        "\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        avg_acc = total_acc / len(dataloader)\n",
        "        print(f\"Epoch {epoch+1} Completed - Avg Loss: {avg_loss:.4f}, Avg Acc: {avg_acc:.4f}\")"
      ],
      "metadata": {
        "id": "rgiN5BiQCU7K"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "104c452c",
        "outputId": "08c395be-f53c-4796-af98-bd27008d337c"
      },
      "source": [
        "\n",
        "%%time\n",
        "\n",
        "train(\n",
        "    model=model,\n",
        "    dataloader=dataloader,\n",
        "    optimizer=optimizer,\n",
        "    loss_function=loss_function,\n",
        "    scheduler=scheduler,\n",
        "    num_epochs=500,  # 원하는 에폭 수\n",
        "    device=device\n",
        ")"
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Completed - Avg Loss: 7.1879, Avg Acc: 0.0014\n",
            "Epoch 2 Completed - Avg Loss: 7.1376, Avg Acc: 0.0019\n",
            "Epoch 3 Completed - Avg Loss: 7.0338, Avg Acc: 0.0120\n",
            "Epoch 4 Completed - Avg Loss: 6.8796, Avg Acc: 0.0708\n",
            "Epoch 5 Completed - Avg Loss: 6.6815, Avg Acc: 0.1323\n",
            "Epoch 6 Completed - Avg Loss: 6.4517, Avg Acc: 0.1543\n",
            "Epoch 7 Completed - Avg Loss: 6.2278, Avg Acc: 0.1559\n",
            "Epoch 8 Completed - Avg Loss: 6.0402, Avg Acc: 0.1573\n",
            "Epoch 9 Completed - Avg Loss: 5.8921, Avg Acc: 0.1609\n",
            "Epoch 10 Completed - Avg Loss: 5.7742, Avg Acc: 0.1674\n",
            "Epoch 11 Completed - Avg Loss: 5.6760, Avg Acc: 0.1738\n",
            "Epoch 12 Completed - Avg Loss: 5.5877, Avg Acc: 0.1808\n",
            "Epoch 13 Completed - Avg Loss: 5.5102, Avg Acc: 0.1889\n",
            "Epoch 14 Completed - Avg Loss: 5.4407, Avg Acc: 0.1943\n",
            "Epoch 15 Completed - Avg Loss: 5.3751, Avg Acc: 0.1998\n",
            "Epoch 16 Completed - Avg Loss: 5.3152, Avg Acc: 0.2036\n",
            "Epoch 17 Completed - Avg Loss: 5.2596, Avg Acc: 0.2079\n",
            "Epoch 18 Completed - Avg Loss: 5.2058, Avg Acc: 0.2118\n",
            "Epoch 19 Completed - Avg Loss: 5.1546, Avg Acc: 0.2153\n",
            "Epoch 20 Completed - Avg Loss: 5.1068, Avg Acc: 0.2182\n",
            "Epoch 21 Completed - Avg Loss: 5.0605, Avg Acc: 0.2207\n",
            "Epoch 22 Completed - Avg Loss: 5.0154, Avg Acc: 0.2223\n",
            "Epoch 23 Completed - Avg Loss: 4.9750, Avg Acc: 0.2236\n",
            "Epoch 24 Completed - Avg Loss: 4.9345, Avg Acc: 0.2244\n",
            "Epoch 25 Completed - Avg Loss: 4.8974, Avg Acc: 0.2257\n",
            "Epoch 26 Completed - Avg Loss: 4.8619, Avg Acc: 0.2262\n",
            "Epoch 27 Completed - Avg Loss: 4.8270, Avg Acc: 0.2268\n",
            "Epoch 28 Completed - Avg Loss: 4.7950, Avg Acc: 0.2273\n",
            "Epoch 29 Completed - Avg Loss: 4.7640, Avg Acc: 0.2280\n",
            "Epoch 30 Completed - Avg Loss: 4.7371, Avg Acc: 0.2287\n",
            "Epoch 31 Completed - Avg Loss: 4.7085, Avg Acc: 0.2296\n",
            "Epoch 32 Completed - Avg Loss: 4.6820, Avg Acc: 0.2306\n",
            "Epoch 33 Completed - Avg Loss: 4.6591, Avg Acc: 0.2315\n",
            "Epoch 34 Completed - Avg Loss: 4.6360, Avg Acc: 0.2321\n",
            "Epoch 35 Completed - Avg Loss: 4.6136, Avg Acc: 0.2329\n",
            "Epoch 36 Completed - Avg Loss: 4.5939, Avg Acc: 0.2338\n",
            "Epoch 37 Completed - Avg Loss: 4.5740, Avg Acc: 0.2347\n",
            "Epoch 38 Completed - Avg Loss: 4.5548, Avg Acc: 0.2356\n",
            "Epoch 39 Completed - Avg Loss: 4.5373, Avg Acc: 0.2365\n",
            "Epoch 40 Completed - Avg Loss: 4.5207, Avg Acc: 0.2375\n",
            "Epoch 41 Completed - Avg Loss: 4.5048, Avg Acc: 0.2382\n",
            "Epoch 42 Completed - Avg Loss: 4.4876, Avg Acc: 0.2393\n",
            "Epoch 43 Completed - Avg Loss: 4.4729, Avg Acc: 0.2403\n",
            "Epoch 44 Completed - Avg Loss: 4.4587, Avg Acc: 0.2417\n",
            "Epoch 45 Completed - Avg Loss: 4.4444, Avg Acc: 0.2431\n",
            "Epoch 46 Completed - Avg Loss: 4.4316, Avg Acc: 0.2441\n",
            "Epoch 47 Completed - Avg Loss: 4.4197, Avg Acc: 0.2457\n",
            "Epoch 48 Completed - Avg Loss: 4.4052, Avg Acc: 0.2472\n",
            "Epoch 49 Completed - Avg Loss: 4.3931, Avg Acc: 0.2489\n",
            "Epoch 50 Completed - Avg Loss: 4.3813, Avg Acc: 0.2506\n",
            "Epoch 51 Completed - Avg Loss: 4.3704, Avg Acc: 0.2522\n",
            "Epoch 52 Completed - Avg Loss: 4.3589, Avg Acc: 0.2535\n",
            "Epoch 53 Completed - Avg Loss: 4.3467, Avg Acc: 0.2550\n",
            "Epoch 54 Completed - Avg Loss: 4.3360, Avg Acc: 0.2569\n",
            "Epoch 55 Completed - Avg Loss: 4.3252, Avg Acc: 0.2586\n",
            "Epoch 56 Completed - Avg Loss: 4.3150, Avg Acc: 0.2593\n",
            "Epoch 57 Completed - Avg Loss: 4.3052, Avg Acc: 0.2602\n",
            "Epoch 58 Completed - Avg Loss: 4.2950, Avg Acc: 0.2619\n",
            "Epoch 59 Completed - Avg Loss: 4.2854, Avg Acc: 0.2629\n",
            "Epoch 60 Completed - Avg Loss: 4.2765, Avg Acc: 0.2639\n",
            "Epoch 61 Completed - Avg Loss: 4.2665, Avg Acc: 0.2655\n",
            "Epoch 62 Completed - Avg Loss: 4.2584, Avg Acc: 0.2662\n",
            "Epoch 63 Completed - Avg Loss: 4.2483, Avg Acc: 0.2675\n",
            "Epoch 64 Completed - Avg Loss: 4.2397, Avg Acc: 0.2680\n",
            "Epoch 65 Completed - Avg Loss: 4.2313, Avg Acc: 0.2692\n",
            "Epoch 66 Completed - Avg Loss: 4.2207, Avg Acc: 0.2703\n",
            "Epoch 67 Completed - Avg Loss: 4.2135, Avg Acc: 0.2707\n",
            "Epoch 68 Completed - Avg Loss: 4.2065, Avg Acc: 0.2716\n",
            "Epoch 69 Completed - Avg Loss: 4.1984, Avg Acc: 0.2726\n",
            "Epoch 70 Completed - Avg Loss: 4.1909, Avg Acc: 0.2734\n",
            "Epoch 71 Completed - Avg Loss: 4.1819, Avg Acc: 0.2740\n",
            "Epoch 72 Completed - Avg Loss: 4.1752, Avg Acc: 0.2750\n",
            "Epoch 73 Completed - Avg Loss: 4.1672, Avg Acc: 0.2761\n",
            "Epoch 74 Completed - Avg Loss: 4.1596, Avg Acc: 0.2768\n",
            "Epoch 75 Completed - Avg Loss: 4.1511, Avg Acc: 0.2775\n",
            "Epoch 76 Completed - Avg Loss: 4.1454, Avg Acc: 0.2784\n",
            "Epoch 77 Completed - Avg Loss: 4.1388, Avg Acc: 0.2787\n",
            "Epoch 78 Completed - Avg Loss: 4.1307, Avg Acc: 0.2796\n",
            "Epoch 79 Completed - Avg Loss: 4.1245, Avg Acc: 0.2799\n",
            "Epoch 80 Completed - Avg Loss: 4.1179, Avg Acc: 0.2811\n",
            "Epoch 81 Completed - Avg Loss: 4.1113, Avg Acc: 0.2818\n",
            "Epoch 82 Completed - Avg Loss: 4.1048, Avg Acc: 0.2817\n",
            "Epoch 83 Completed - Avg Loss: 4.0984, Avg Acc: 0.2829\n",
            "Epoch 84 Completed - Avg Loss: 4.0905, Avg Acc: 0.2834\n",
            "Epoch 85 Completed - Avg Loss: 4.0857, Avg Acc: 0.2842\n",
            "Epoch 86 Completed - Avg Loss: 4.0799, Avg Acc: 0.2846\n",
            "Epoch 87 Completed - Avg Loss: 4.0735, Avg Acc: 0.2856\n",
            "Epoch 88 Completed - Avg Loss: 4.0677, Avg Acc: 0.2860\n",
            "Epoch 89 Completed - Avg Loss: 4.0623, Avg Acc: 0.2863\n",
            "Epoch 90 Completed - Avg Loss: 4.0560, Avg Acc: 0.2866\n",
            "Epoch 91 Completed - Avg Loss: 4.0499, Avg Acc: 0.2877\n",
            "Epoch 92 Completed - Avg Loss: 4.0441, Avg Acc: 0.2874\n",
            "Epoch 93 Completed - Avg Loss: 4.0394, Avg Acc: 0.2886\n",
            "Epoch 94 Completed - Avg Loss: 4.0349, Avg Acc: 0.2885\n",
            "Epoch 95 Completed - Avg Loss: 4.0283, Avg Acc: 0.2891\n",
            "Epoch 96 Completed - Avg Loss: 4.0239, Avg Acc: 0.2893\n",
            "Epoch 97 Completed - Avg Loss: 4.0178, Avg Acc: 0.2905\n",
            "Epoch 98 Completed - Avg Loss: 4.0121, Avg Acc: 0.2911\n",
            "Epoch 99 Completed - Avg Loss: 4.0070, Avg Acc: 0.2911\n",
            "Epoch 100 Completed - Avg Loss: 4.0021, Avg Acc: 0.2911\n",
            "Epoch 101 Completed - Avg Loss: 3.9969, Avg Acc: 0.2920\n",
            "Epoch 102 Completed - Avg Loss: 3.9916, Avg Acc: 0.2922\n",
            "Epoch 103 Completed - Avg Loss: 3.9873, Avg Acc: 0.2925\n",
            "Epoch 104 Completed - Avg Loss: 3.9817, Avg Acc: 0.2935\n",
            "Epoch 105 Completed - Avg Loss: 3.9791, Avg Acc: 0.2936\n",
            "Epoch 106 Completed - Avg Loss: 3.9727, Avg Acc: 0.2937\n",
            "Epoch 107 Completed - Avg Loss: 3.9697, Avg Acc: 0.2941\n",
            "Epoch 108 Completed - Avg Loss: 3.9641, Avg Acc: 0.2944\n",
            "Epoch 109 Completed - Avg Loss: 3.9590, Avg Acc: 0.2947\n",
            "Epoch 110 Completed - Avg Loss: 3.9553, Avg Acc: 0.2951\n",
            "Epoch 111 Completed - Avg Loss: 3.9506, Avg Acc: 0.2953\n",
            "Epoch 112 Completed - Avg Loss: 3.9465, Avg Acc: 0.2956\n",
            "Epoch 113 Completed - Avg Loss: 3.9420, Avg Acc: 0.2958\n",
            "Epoch 114 Completed - Avg Loss: 3.9383, Avg Acc: 0.2959\n",
            "Epoch 115 Completed - Avg Loss: 3.9334, Avg Acc: 0.2968\n",
            "Epoch 116 Completed - Avg Loss: 3.9287, Avg Acc: 0.2974\n",
            "Epoch 117 Completed - Avg Loss: 3.9263, Avg Acc: 0.2970\n",
            "Epoch 118 Completed - Avg Loss: 3.9212, Avg Acc: 0.2977\n",
            "Epoch 119 Completed - Avg Loss: 3.9171, Avg Acc: 0.2974\n",
            "Epoch 120 Completed - Avg Loss: 3.9132, Avg Acc: 0.2981\n",
            "Epoch 121 Completed - Avg Loss: 3.9095, Avg Acc: 0.2981\n",
            "Epoch 122 Completed - Avg Loss: 3.9049, Avg Acc: 0.2989\n",
            "Epoch 123 Completed - Avg Loss: 3.9017, Avg Acc: 0.2985\n",
            "Epoch 124 Completed - Avg Loss: 3.8982, Avg Acc: 0.2987\n",
            "Epoch 125 Completed - Avg Loss: 3.8932, Avg Acc: 0.2992\n",
            "Epoch 126 Completed - Avg Loss: 3.8897, Avg Acc: 0.2999\n",
            "Epoch 127 Completed - Avg Loss: 3.8865, Avg Acc: 0.3000\n",
            "Epoch 128 Completed - Avg Loss: 3.8830, Avg Acc: 0.2998\n",
            "Epoch 129 Completed - Avg Loss: 3.8792, Avg Acc: 0.3003\n",
            "Epoch 130 Completed - Avg Loss: 3.8762, Avg Acc: 0.3006\n",
            "Epoch 131 Completed - Avg Loss: 3.8726, Avg Acc: 0.3008\n",
            "Epoch 132 Completed - Avg Loss: 3.8677, Avg Acc: 0.3010\n",
            "Epoch 133 Completed - Avg Loss: 3.8643, Avg Acc: 0.3016\n",
            "Epoch 134 Completed - Avg Loss: 3.8617, Avg Acc: 0.3022\n",
            "Epoch 135 Completed - Avg Loss: 3.8583, Avg Acc: 0.3023\n",
            "Epoch 136 Completed - Avg Loss: 3.8548, Avg Acc: 0.3022\n",
            "Epoch 137 Completed - Avg Loss: 3.8524, Avg Acc: 0.3026\n",
            "Epoch 138 Completed - Avg Loss: 3.8479, Avg Acc: 0.3029\n",
            "Epoch 139 Completed - Avg Loss: 3.8443, Avg Acc: 0.3036\n",
            "Epoch 140 Completed - Avg Loss: 3.8410, Avg Acc: 0.3037\n",
            "Epoch 141 Completed - Avg Loss: 3.8378, Avg Acc: 0.3039\n",
            "Epoch 142 Completed - Avg Loss: 3.8359, Avg Acc: 0.3038\n",
            "Epoch 143 Completed - Avg Loss: 3.8308, Avg Acc: 0.3046\n",
            "Epoch 144 Completed - Avg Loss: 3.8274, Avg Acc: 0.3042\n",
            "Epoch 145 Completed - Avg Loss: 3.8264, Avg Acc: 0.3044\n",
            "Epoch 146 Completed - Avg Loss: 3.8214, Avg Acc: 0.3046\n",
            "Epoch 147 Completed - Avg Loss: 3.8192, Avg Acc: 0.3048\n",
            "Epoch 148 Completed - Avg Loss: 3.8152, Avg Acc: 0.3051\n",
            "Epoch 149 Completed - Avg Loss: 3.8143, Avg Acc: 0.3059\n",
            "Epoch 150 Completed - Avg Loss: 3.8099, Avg Acc: 0.3060\n",
            "Epoch 151 Completed - Avg Loss: 3.8058, Avg Acc: 0.3065\n",
            "Epoch 152 Completed - Avg Loss: 3.8038, Avg Acc: 0.3062\n",
            "Epoch 153 Completed - Avg Loss: 3.8028, Avg Acc: 0.3064\n",
            "Epoch 154 Completed - Avg Loss: 3.7982, Avg Acc: 0.3074\n",
            "Epoch 155 Completed - Avg Loss: 3.7957, Avg Acc: 0.3070\n",
            "Epoch 156 Completed - Avg Loss: 3.7918, Avg Acc: 0.3076\n",
            "Epoch 157 Completed - Avg Loss: 3.7891, Avg Acc: 0.3079\n",
            "Epoch 158 Completed - Avg Loss: 3.7866, Avg Acc: 0.3077\n",
            "Epoch 159 Completed - Avg Loss: 3.7838, Avg Acc: 0.3081\n",
            "Epoch 160 Completed - Avg Loss: 3.7802, Avg Acc: 0.3092\n",
            "Epoch 161 Completed - Avg Loss: 3.7803, Avg Acc: 0.3082\n",
            "Epoch 162 Completed - Avg Loss: 3.7753, Avg Acc: 0.3085\n",
            "Epoch 163 Completed - Avg Loss: 3.7716, Avg Acc: 0.3089\n",
            "Epoch 164 Completed - Avg Loss: 3.7705, Avg Acc: 0.3089\n",
            "Epoch 165 Completed - Avg Loss: 3.7669, Avg Acc: 0.3097\n",
            "Epoch 166 Completed - Avg Loss: 3.7659, Avg Acc: 0.3091\n",
            "Epoch 167 Completed - Avg Loss: 3.7630, Avg Acc: 0.3097\n",
            "Epoch 168 Completed - Avg Loss: 3.7597, Avg Acc: 0.3099\n",
            "Epoch 169 Completed - Avg Loss: 3.7573, Avg Acc: 0.3104\n",
            "Epoch 170 Completed - Avg Loss: 3.7548, Avg Acc: 0.3108\n",
            "Epoch 171 Completed - Avg Loss: 3.7537, Avg Acc: 0.3104\n",
            "Epoch 172 Completed - Avg Loss: 3.7494, Avg Acc: 0.3105\n",
            "Epoch 173 Completed - Avg Loss: 3.7472, Avg Acc: 0.3114\n",
            "Epoch 174 Completed - Avg Loss: 3.7443, Avg Acc: 0.3114\n",
            "Epoch 175 Completed - Avg Loss: 3.7431, Avg Acc: 0.3115\n",
            "Epoch 176 Completed - Avg Loss: 3.7401, Avg Acc: 0.3121\n",
            "Epoch 177 Completed - Avg Loss: 3.7381, Avg Acc: 0.3127\n",
            "Epoch 178 Completed - Avg Loss: 3.7363, Avg Acc: 0.3126\n",
            "Epoch 179 Completed - Avg Loss: 3.7336, Avg Acc: 0.3120\n",
            "Epoch 180 Completed - Avg Loss: 3.7308, Avg Acc: 0.3131\n",
            "Epoch 181 Completed - Avg Loss: 3.7280, Avg Acc: 0.3128\n",
            "Epoch 182 Completed - Avg Loss: 3.7267, Avg Acc: 0.3133\n",
            "Epoch 183 Completed - Avg Loss: 3.7246, Avg Acc: 0.3133\n",
            "Epoch 184 Completed - Avg Loss: 3.7216, Avg Acc: 0.3143\n",
            "Epoch 185 Completed - Avg Loss: 3.7193, Avg Acc: 0.3139\n",
            "Epoch 186 Completed - Avg Loss: 3.7158, Avg Acc: 0.3137\n",
            "Epoch 187 Completed - Avg Loss: 3.7166, Avg Acc: 0.3146\n",
            "Epoch 188 Completed - Avg Loss: 3.7117, Avg Acc: 0.3148\n",
            "Epoch 189 Completed - Avg Loss: 3.7096, Avg Acc: 0.3151\n",
            "Epoch 190 Completed - Avg Loss: 3.7083, Avg Acc: 0.3152\n",
            "Epoch 191 Completed - Avg Loss: 3.7046, Avg Acc: 0.3152\n",
            "Epoch 192 Completed - Avg Loss: 3.7012, Avg Acc: 0.3164\n",
            "Epoch 193 Completed - Avg Loss: 3.7006, Avg Acc: 0.3163\n",
            "Epoch 194 Completed - Avg Loss: 3.6981, Avg Acc: 0.3168\n",
            "Epoch 195 Completed - Avg Loss: 3.6982, Avg Acc: 0.3160\n",
            "Epoch 196 Completed - Avg Loss: 3.6945, Avg Acc: 0.3167\n",
            "Epoch 197 Completed - Avg Loss: 3.6918, Avg Acc: 0.3171\n",
            "Epoch 198 Completed - Avg Loss: 3.6906, Avg Acc: 0.3172\n",
            "Epoch 199 Completed - Avg Loss: 3.6891, Avg Acc: 0.3173\n",
            "Epoch 200 Completed - Avg Loss: 3.6868, Avg Acc: 0.3181\n",
            "Epoch 201 Completed - Avg Loss: 3.6848, Avg Acc: 0.3177\n",
            "Epoch 202 Completed - Avg Loss: 3.6828, Avg Acc: 0.3177\n",
            "Epoch 203 Completed - Avg Loss: 3.6800, Avg Acc: 0.3177\n",
            "Epoch 204 Completed - Avg Loss: 3.6785, Avg Acc: 0.3185\n",
            "Epoch 205 Completed - Avg Loss: 3.6759, Avg Acc: 0.3186\n",
            "Epoch 206 Completed - Avg Loss: 3.6744, Avg Acc: 0.3191\n",
            "Epoch 207 Completed - Avg Loss: 3.6721, Avg Acc: 0.3192\n",
            "Epoch 208 Completed - Avg Loss: 3.6716, Avg Acc: 0.3191\n",
            "Epoch 209 Completed - Avg Loss: 3.6680, Avg Acc: 0.3206\n",
            "Epoch 210 Completed - Avg Loss: 3.6664, Avg Acc: 0.3193\n",
            "Epoch 211 Completed - Avg Loss: 3.6638, Avg Acc: 0.3201\n",
            "Epoch 212 Completed - Avg Loss: 3.6617, Avg Acc: 0.3203\n",
            "Epoch 213 Completed - Avg Loss: 3.6595, Avg Acc: 0.3205\n",
            "Epoch 214 Completed - Avg Loss: 3.6590, Avg Acc: 0.3211\n",
            "Epoch 215 Completed - Avg Loss: 3.6578, Avg Acc: 0.3209\n",
            "Epoch 216 Completed - Avg Loss: 3.6548, Avg Acc: 0.3214\n",
            "Epoch 217 Completed - Avg Loss: 3.6518, Avg Acc: 0.3219\n",
            "Epoch 218 Completed - Avg Loss: 3.6510, Avg Acc: 0.3216\n",
            "Epoch 219 Completed - Avg Loss: 3.6500, Avg Acc: 0.3217\n",
            "Epoch 220 Completed - Avg Loss: 3.6475, Avg Acc: 0.3220\n",
            "Epoch 221 Completed - Avg Loss: 3.6458, Avg Acc: 0.3221\n",
            "Epoch 222 Completed - Avg Loss: 3.6445, Avg Acc: 0.3227\n",
            "Epoch 223 Completed - Avg Loss: 3.6418, Avg Acc: 0.3220\n",
            "Epoch 224 Completed - Avg Loss: 3.6399, Avg Acc: 0.3229\n",
            "Epoch 225 Completed - Avg Loss: 3.6385, Avg Acc: 0.3227\n",
            "Epoch 226 Completed - Avg Loss: 3.6366, Avg Acc: 0.3225\n",
            "Epoch 227 Completed - Avg Loss: 3.6361, Avg Acc: 0.3232\n",
            "Epoch 228 Completed - Avg Loss: 3.6333, Avg Acc: 0.3237\n",
            "Epoch 229 Completed - Avg Loss: 3.6315, Avg Acc: 0.3249\n",
            "Epoch 230 Completed - Avg Loss: 3.6280, Avg Acc: 0.3242\n",
            "Epoch 231 Completed - Avg Loss: 3.6272, Avg Acc: 0.3241\n",
            "Epoch 232 Completed - Avg Loss: 3.6246, Avg Acc: 0.3244\n",
            "Epoch 233 Completed - Avg Loss: 3.6247, Avg Acc: 0.3248\n",
            "Epoch 234 Completed - Avg Loss: 3.6236, Avg Acc: 0.3242\n",
            "Epoch 235 Completed - Avg Loss: 3.6213, Avg Acc: 0.3252\n",
            "Epoch 236 Completed - Avg Loss: 3.6212, Avg Acc: 0.3252\n",
            "Epoch 237 Completed - Avg Loss: 3.6179, Avg Acc: 0.3256\n",
            "Epoch 238 Completed - Avg Loss: 3.6161, Avg Acc: 0.3256\n",
            "Epoch 239 Completed - Avg Loss: 3.6137, Avg Acc: 0.3261\n",
            "Epoch 240 Completed - Avg Loss: 3.6138, Avg Acc: 0.3259\n",
            "Epoch 241 Completed - Avg Loss: 3.6108, Avg Acc: 0.3264\n",
            "Epoch 242 Completed - Avg Loss: 3.6102, Avg Acc: 0.3264\n",
            "Epoch 243 Completed - Avg Loss: 3.6083, Avg Acc: 0.3264\n",
            "Epoch 244 Completed - Avg Loss: 3.6070, Avg Acc: 0.3271\n",
            "Epoch 245 Completed - Avg Loss: 3.6066, Avg Acc: 0.3269\n",
            "Epoch 246 Completed - Avg Loss: 3.6036, Avg Acc: 0.3271\n",
            "Epoch 247 Completed - Avg Loss: 3.6016, Avg Acc: 0.3265\n",
            "Epoch 248 Completed - Avg Loss: 3.6000, Avg Acc: 0.3274\n",
            "Epoch 249 Completed - Avg Loss: 3.5994, Avg Acc: 0.3272\n",
            "Epoch 250 Completed - Avg Loss: 3.5974, Avg Acc: 0.3282\n",
            "Epoch 251 Completed - Avg Loss: 3.5963, Avg Acc: 0.3277\n",
            "Epoch 252 Completed - Avg Loss: 3.5942, Avg Acc: 0.3283\n",
            "Epoch 253 Completed - Avg Loss: 3.5929, Avg Acc: 0.3278\n",
            "Epoch 254 Completed - Avg Loss: 3.5911, Avg Acc: 0.3285\n",
            "Epoch 255 Completed - Avg Loss: 3.5886, Avg Acc: 0.3284\n",
            "Epoch 256 Completed - Avg Loss: 3.5872, Avg Acc: 0.3286\n",
            "Epoch 257 Completed - Avg Loss: 3.5864, Avg Acc: 0.3294\n",
            "Epoch 258 Completed - Avg Loss: 3.5857, Avg Acc: 0.3284\n",
            "Epoch 259 Completed - Avg Loss: 3.5827, Avg Acc: 0.3293\n",
            "Epoch 260 Completed - Avg Loss: 3.5827, Avg Acc: 0.3296\n",
            "Epoch 261 Completed - Avg Loss: 3.5802, Avg Acc: 0.3296\n",
            "Epoch 262 Completed - Avg Loss: 3.5796, Avg Acc: 0.3301\n",
            "Epoch 263 Completed - Avg Loss: 3.5790, Avg Acc: 0.3295\n",
            "Epoch 264 Completed - Avg Loss: 3.5762, Avg Acc: 0.3299\n",
            "Epoch 265 Completed - Avg Loss: 3.5751, Avg Acc: 0.3302\n",
            "Epoch 266 Completed - Avg Loss: 3.5729, Avg Acc: 0.3303\n",
            "Epoch 267 Completed - Avg Loss: 3.5724, Avg Acc: 0.3304\n",
            "Epoch 268 Completed - Avg Loss: 3.5718, Avg Acc: 0.3306\n",
            "Epoch 269 Completed - Avg Loss: 3.5704, Avg Acc: 0.3306\n",
            "Epoch 270 Completed - Avg Loss: 3.5680, Avg Acc: 0.3314\n",
            "Epoch 271 Completed - Avg Loss: 3.5662, Avg Acc: 0.3306\n",
            "Epoch 272 Completed - Avg Loss: 3.5642, Avg Acc: 0.3313\n",
            "Epoch 273 Completed - Avg Loss: 3.5636, Avg Acc: 0.3308\n",
            "Epoch 274 Completed - Avg Loss: 3.5629, Avg Acc: 0.3317\n",
            "Epoch 275 Completed - Avg Loss: 3.5606, Avg Acc: 0.3318\n",
            "Epoch 276 Completed - Avg Loss: 3.5595, Avg Acc: 0.3326\n",
            "Epoch 277 Completed - Avg Loss: 3.5593, Avg Acc: 0.3323\n",
            "Epoch 278 Completed - Avg Loss: 3.5569, Avg Acc: 0.3325\n",
            "Epoch 279 Completed - Avg Loss: 3.5559, Avg Acc: 0.3318\n",
            "Epoch 280 Completed - Avg Loss: 3.5533, Avg Acc: 0.3329\n",
            "Epoch 281 Completed - Avg Loss: 3.5523, Avg Acc: 0.3334\n",
            "Epoch 282 Completed - Avg Loss: 3.5515, Avg Acc: 0.3330\n",
            "Epoch 283 Completed - Avg Loss: 3.5510, Avg Acc: 0.3323\n",
            "Epoch 284 Completed - Avg Loss: 3.5491, Avg Acc: 0.3335\n",
            "Epoch 285 Completed - Avg Loss: 3.5482, Avg Acc: 0.3325\n",
            "Epoch 286 Completed - Avg Loss: 3.5468, Avg Acc: 0.3336\n",
            "Epoch 287 Completed - Avg Loss: 3.5454, Avg Acc: 0.3338\n",
            "Epoch 288 Completed - Avg Loss: 3.5433, Avg Acc: 0.3334\n",
            "Epoch 289 Completed - Avg Loss: 3.5419, Avg Acc: 0.3339\n",
            "Epoch 290 Completed - Avg Loss: 3.5404, Avg Acc: 0.3340\n",
            "Epoch 291 Completed - Avg Loss: 3.5389, Avg Acc: 0.3339\n",
            "Epoch 292 Completed - Avg Loss: 3.5383, Avg Acc: 0.3339\n",
            "Epoch 293 Completed - Avg Loss: 3.5378, Avg Acc: 0.3344\n",
            "Epoch 294 Completed - Avg Loss: 3.5359, Avg Acc: 0.3346\n",
            "Epoch 295 Completed - Avg Loss: 3.5347, Avg Acc: 0.3343\n",
            "Epoch 296 Completed - Avg Loss: 3.5327, Avg Acc: 0.3355\n",
            "Epoch 297 Completed - Avg Loss: 3.5311, Avg Acc: 0.3350\n",
            "Epoch 298 Completed - Avg Loss: 3.5294, Avg Acc: 0.3359\n",
            "Epoch 299 Completed - Avg Loss: 3.5303, Avg Acc: 0.3342\n",
            "Epoch 300 Completed - Avg Loss: 3.5278, Avg Acc: 0.3355\n",
            "Epoch 301 Completed - Avg Loss: 3.5278, Avg Acc: 0.3358\n",
            "Epoch 302 Completed - Avg Loss: 3.5270, Avg Acc: 0.3360\n",
            "Epoch 303 Completed - Avg Loss: 3.5253, Avg Acc: 0.3353\n",
            "Epoch 304 Completed - Avg Loss: 3.5223, Avg Acc: 0.3358\n",
            "Epoch 305 Completed - Avg Loss: 3.5227, Avg Acc: 0.3360\n",
            "Epoch 306 Completed - Avg Loss: 3.5215, Avg Acc: 0.3366\n",
            "Epoch 307 Completed - Avg Loss: 3.5194, Avg Acc: 0.3363\n",
            "Epoch 308 Completed - Avg Loss: 3.5199, Avg Acc: 0.3360\n",
            "Epoch 309 Completed - Avg Loss: 3.5163, Avg Acc: 0.3374\n",
            "Epoch 310 Completed - Avg Loss: 3.5169, Avg Acc: 0.3368\n",
            "Epoch 311 Completed - Avg Loss: 3.5153, Avg Acc: 0.3369\n",
            "Epoch 312 Completed - Avg Loss: 3.5141, Avg Acc: 0.3364\n",
            "Epoch 313 Completed - Avg Loss: 3.5122, Avg Acc: 0.3374\n",
            "Epoch 314 Completed - Avg Loss: 3.5130, Avg Acc: 0.3368\n",
            "Epoch 315 Completed - Avg Loss: 3.5104, Avg Acc: 0.3377\n",
            "Epoch 316 Completed - Avg Loss: 3.5079, Avg Acc: 0.3377\n",
            "Epoch 317 Completed - Avg Loss: 3.5072, Avg Acc: 0.3372\n",
            "Epoch 318 Completed - Avg Loss: 3.5092, Avg Acc: 0.3369\n",
            "Epoch 319 Completed - Avg Loss: 3.5059, Avg Acc: 0.3380\n",
            "Epoch 320 Completed - Avg Loss: 3.5053, Avg Acc: 0.3378\n",
            "Epoch 321 Completed - Avg Loss: 3.5051, Avg Acc: 0.3380\n",
            "Epoch 322 Completed - Avg Loss: 3.5022, Avg Acc: 0.3376\n",
            "Epoch 323 Completed - Avg Loss: 3.5025, Avg Acc: 0.3379\n",
            "Epoch 324 Completed - Avg Loss: 3.5003, Avg Acc: 0.3384\n",
            "Epoch 325 Completed - Avg Loss: 3.4993, Avg Acc: 0.3383\n",
            "Epoch 326 Completed - Avg Loss: 3.4975, Avg Acc: 0.3384\n",
            "Epoch 327 Completed - Avg Loss: 3.4954, Avg Acc: 0.3389\n",
            "Epoch 328 Completed - Avg Loss: 3.4960, Avg Acc: 0.3392\n",
            "Epoch 329 Completed - Avg Loss: 3.4943, Avg Acc: 0.3388\n",
            "Epoch 330 Completed - Avg Loss: 3.4941, Avg Acc: 0.3388\n",
            "Epoch 331 Completed - Avg Loss: 3.4928, Avg Acc: 0.3392\n",
            "Epoch 332 Completed - Avg Loss: 3.4917, Avg Acc: 0.3389\n",
            "Epoch 333 Completed - Avg Loss: 3.4902, Avg Acc: 0.3396\n",
            "Epoch 334 Completed - Avg Loss: 3.4900, Avg Acc: 0.3392\n",
            "Epoch 335 Completed - Avg Loss: 3.4886, Avg Acc: 0.3395\n",
            "Epoch 336 Completed - Avg Loss: 3.4871, Avg Acc: 0.3402\n",
            "Epoch 337 Completed - Avg Loss: 3.4856, Avg Acc: 0.3403\n",
            "Epoch 338 Completed - Avg Loss: 3.4856, Avg Acc: 0.3397\n",
            "Epoch 339 Completed - Avg Loss: 3.4846, Avg Acc: 0.3398\n",
            "Epoch 340 Completed - Avg Loss: 3.4825, Avg Acc: 0.3403\n",
            "Epoch 341 Completed - Avg Loss: 3.4813, Avg Acc: 0.3410\n",
            "Epoch 342 Completed - Avg Loss: 3.4812, Avg Acc: 0.3408\n",
            "Epoch 343 Completed - Avg Loss: 3.4801, Avg Acc: 0.3408\n",
            "Epoch 344 Completed - Avg Loss: 3.4784, Avg Acc: 0.3411\n",
            "Epoch 345 Completed - Avg Loss: 3.4776, Avg Acc: 0.3406\n",
            "Epoch 346 Completed - Avg Loss: 3.4774, Avg Acc: 0.3406\n",
            "Epoch 347 Completed - Avg Loss: 3.4759, Avg Acc: 0.3412\n",
            "Epoch 348 Completed - Avg Loss: 3.4754, Avg Acc: 0.3416\n",
            "Epoch 349 Completed - Avg Loss: 3.4743, Avg Acc: 0.3412\n",
            "Epoch 350 Completed - Avg Loss: 3.4716, Avg Acc: 0.3415\n",
            "Epoch 351 Completed - Avg Loss: 3.4723, Avg Acc: 0.3412\n",
            "Epoch 352 Completed - Avg Loss: 3.4703, Avg Acc: 0.3415\n",
            "Epoch 353 Completed - Avg Loss: 3.4703, Avg Acc: 0.3416\n",
            "Epoch 354 Completed - Avg Loss: 3.4696, Avg Acc: 0.3418\n",
            "Epoch 355 Completed - Avg Loss: 3.4669, Avg Acc: 0.3417\n",
            "Epoch 356 Completed - Avg Loss: 3.4659, Avg Acc: 0.3427\n",
            "Epoch 357 Completed - Avg Loss: 3.4655, Avg Acc: 0.3421\n",
            "Epoch 358 Completed - Avg Loss: 3.4645, Avg Acc: 0.3427\n",
            "Epoch 359 Completed - Avg Loss: 3.4643, Avg Acc: 0.3423\n",
            "Epoch 360 Completed - Avg Loss: 3.4627, Avg Acc: 0.3423\n",
            "Epoch 361 Completed - Avg Loss: 3.4629, Avg Acc: 0.3429\n",
            "Epoch 362 Completed - Avg Loss: 3.4601, Avg Acc: 0.3424\n",
            "Epoch 363 Completed - Avg Loss: 3.4588, Avg Acc: 0.3430\n",
            "Epoch 364 Completed - Avg Loss: 3.4588, Avg Acc: 0.3429\n",
            "Epoch 365 Completed - Avg Loss: 3.4582, Avg Acc: 0.3426\n",
            "Epoch 366 Completed - Avg Loss: 3.4578, Avg Acc: 0.3429\n",
            "Epoch 367 Completed - Avg Loss: 3.4562, Avg Acc: 0.3433\n",
            "Epoch 368 Completed - Avg Loss: 3.4556, Avg Acc: 0.3430\n",
            "Epoch 369 Completed - Avg Loss: 3.4526, Avg Acc: 0.3442\n",
            "Epoch 370 Completed - Avg Loss: 3.4529, Avg Acc: 0.3438\n",
            "Epoch 371 Completed - Avg Loss: 3.4509, Avg Acc: 0.3442\n",
            "Epoch 372 Completed - Avg Loss: 3.4511, Avg Acc: 0.3440\n",
            "Epoch 373 Completed - Avg Loss: 3.4503, Avg Acc: 0.3437\n",
            "Epoch 374 Completed - Avg Loss: 3.4481, Avg Acc: 0.3445\n",
            "Epoch 375 Completed - Avg Loss: 3.4471, Avg Acc: 0.3442\n",
            "Epoch 376 Completed - Avg Loss: 3.4481, Avg Acc: 0.3440\n",
            "Epoch 377 Completed - Avg Loss: 3.4474, Avg Acc: 0.3447\n",
            "Epoch 378 Completed - Avg Loss: 3.4463, Avg Acc: 0.3444\n",
            "Epoch 379 Completed - Avg Loss: 3.4445, Avg Acc: 0.3455\n",
            "Epoch 380 Completed - Avg Loss: 3.4425, Avg Acc: 0.3453\n",
            "Epoch 381 Completed - Avg Loss: 3.4433, Avg Acc: 0.3442\n",
            "Epoch 382 Completed - Avg Loss: 3.4416, Avg Acc: 0.3444\n",
            "Epoch 383 Completed - Avg Loss: 3.4405, Avg Acc: 0.3450\n",
            "Epoch 384 Completed - Avg Loss: 3.4393, Avg Acc: 0.3453\n",
            "Epoch 385 Completed - Avg Loss: 3.4384, Avg Acc: 0.3445\n",
            "Epoch 386 Completed - Avg Loss: 3.4384, Avg Acc: 0.3453\n",
            "Epoch 387 Completed - Avg Loss: 3.4371, Avg Acc: 0.3456\n",
            "Epoch 388 Completed - Avg Loss: 3.4387, Avg Acc: 0.3446\n",
            "Epoch 389 Completed - Avg Loss: 3.4360, Avg Acc: 0.3454\n",
            "Epoch 390 Completed - Avg Loss: 3.4352, Avg Acc: 0.3456\n",
            "Epoch 391 Completed - Avg Loss: 3.4347, Avg Acc: 0.3457\n",
            "Epoch 392 Completed - Avg Loss: 3.4323, Avg Acc: 0.3461\n",
            "Epoch 393 Completed - Avg Loss: 3.4330, Avg Acc: 0.3461\n",
            "Epoch 394 Completed - Avg Loss: 3.4312, Avg Acc: 0.3458\n",
            "Epoch 395 Completed - Avg Loss: 3.4314, Avg Acc: 0.3455\n",
            "Epoch 396 Completed - Avg Loss: 3.4272, Avg Acc: 0.3461\n",
            "Epoch 397 Completed - Avg Loss: 3.4299, Avg Acc: 0.3460\n",
            "Epoch 398 Completed - Avg Loss: 3.4288, Avg Acc: 0.3461\n",
            "Epoch 399 Completed - Avg Loss: 3.4288, Avg Acc: 0.3461\n",
            "Epoch 400 Completed - Avg Loss: 3.4264, Avg Acc: 0.3464\n",
            "Epoch 401 Completed - Avg Loss: 3.4249, Avg Acc: 0.3466\n",
            "Epoch 402 Completed - Avg Loss: 3.4229, Avg Acc: 0.3471\n",
            "Epoch 403 Completed - Avg Loss: 3.4242, Avg Acc: 0.3468\n",
            "Epoch 404 Completed - Avg Loss: 3.4234, Avg Acc: 0.3465\n",
            "Epoch 405 Completed - Avg Loss: 3.4217, Avg Acc: 0.3465\n",
            "Epoch 406 Completed - Avg Loss: 3.4207, Avg Acc: 0.3473\n",
            "Epoch 407 Completed - Avg Loss: 3.4213, Avg Acc: 0.3468\n",
            "Epoch 408 Completed - Avg Loss: 3.4205, Avg Acc: 0.3466\n",
            "Epoch 409 Completed - Avg Loss: 3.4195, Avg Acc: 0.3477\n",
            "Epoch 410 Completed - Avg Loss: 3.4167, Avg Acc: 0.3469\n",
            "Epoch 411 Completed - Avg Loss: 3.4170, Avg Acc: 0.3477\n",
            "Epoch 412 Completed - Avg Loss: 3.4154, Avg Acc: 0.3475\n",
            "Epoch 413 Completed - Avg Loss: 3.4156, Avg Acc: 0.3479\n",
            "Epoch 414 Completed - Avg Loss: 3.4148, Avg Acc: 0.3482\n",
            "Epoch 415 Completed - Avg Loss: 3.4139, Avg Acc: 0.3481\n",
            "Epoch 416 Completed - Avg Loss: 3.4123, Avg Acc: 0.3472\n",
            "Epoch 417 Completed - Avg Loss: 3.4130, Avg Acc: 0.3474\n",
            "Epoch 418 Completed - Avg Loss: 3.4117, Avg Acc: 0.3488\n",
            "Epoch 419 Completed - Avg Loss: 3.4102, Avg Acc: 0.3479\n",
            "Epoch 420 Completed - Avg Loss: 3.4106, Avg Acc: 0.3483\n",
            "Epoch 421 Completed - Avg Loss: 3.4079, Avg Acc: 0.3483\n",
            "Epoch 422 Completed - Avg Loss: 3.4050, Avg Acc: 0.3488\n",
            "Epoch 423 Completed - Avg Loss: 3.4073, Avg Acc: 0.3487\n",
            "Epoch 424 Completed - Avg Loss: 3.4081, Avg Acc: 0.3482\n",
            "Epoch 425 Completed - Avg Loss: 3.4065, Avg Acc: 0.3490\n",
            "Epoch 426 Completed - Avg Loss: 3.4051, Avg Acc: 0.3492\n",
            "Epoch 427 Completed - Avg Loss: 3.4042, Avg Acc: 0.3485\n",
            "Epoch 428 Completed - Avg Loss: 3.4048, Avg Acc: 0.3488\n",
            "Epoch 429 Completed - Avg Loss: 3.4028, Avg Acc: 0.3489\n",
            "Epoch 430 Completed - Avg Loss: 3.4003, Avg Acc: 0.3484\n",
            "Epoch 431 Completed - Avg Loss: 3.4007, Avg Acc: 0.3492\n",
            "Epoch 432 Completed - Avg Loss: 3.3991, Avg Acc: 0.3496\n",
            "Epoch 433 Completed - Avg Loss: 3.3990, Avg Acc: 0.3493\n",
            "Epoch 434 Completed - Avg Loss: 3.3996, Avg Acc: 0.3494\n",
            "Epoch 435 Completed - Avg Loss: 3.3974, Avg Acc: 0.3491\n",
            "Epoch 436 Completed - Avg Loss: 3.3963, Avg Acc: 0.3496\n",
            "Epoch 437 Completed - Avg Loss: 3.3953, Avg Acc: 0.3499\n",
            "Epoch 438 Completed - Avg Loss: 3.3963, Avg Acc: 0.3496\n",
            "Epoch 439 Completed - Avg Loss: 3.3934, Avg Acc: 0.3498\n",
            "Epoch 440 Completed - Avg Loss: 3.3929, Avg Acc: 0.3505\n",
            "Epoch 441 Completed - Avg Loss: 3.3928, Avg Acc: 0.3495\n",
            "Epoch 442 Completed - Avg Loss: 3.3911, Avg Acc: 0.3499\n",
            "Epoch 443 Completed - Avg Loss: 3.3911, Avg Acc: 0.3504\n",
            "Epoch 444 Completed - Avg Loss: 3.3909, Avg Acc: 0.3501\n",
            "Epoch 445 Completed - Avg Loss: 3.3909, Avg Acc: 0.3500\n",
            "Epoch 446 Completed - Avg Loss: 3.3893, Avg Acc: 0.3500\n",
            "Epoch 447 Completed - Avg Loss: 3.3878, Avg Acc: 0.3502\n",
            "Epoch 448 Completed - Avg Loss: 3.3872, Avg Acc: 0.3511\n",
            "Epoch 449 Completed - Avg Loss: 3.3878, Avg Acc: 0.3503\n",
            "Epoch 450 Completed - Avg Loss: 3.3868, Avg Acc: 0.3501\n",
            "Epoch 451 Completed - Avg Loss: 3.3842, Avg Acc: 0.3509\n",
            "Epoch 452 Completed - Avg Loss: 3.3853, Avg Acc: 0.3503\n",
            "Epoch 453 Completed - Avg Loss: 3.3829, Avg Acc: 0.3511\n",
            "Epoch 454 Completed - Avg Loss: 3.3832, Avg Acc: 0.3506\n",
            "Epoch 455 Completed - Avg Loss: 3.3818, Avg Acc: 0.3510\n",
            "Epoch 456 Completed - Avg Loss: 3.3804, Avg Acc: 0.3509\n",
            "Epoch 457 Completed - Avg Loss: 3.3799, Avg Acc: 0.3517\n",
            "Epoch 458 Completed - Avg Loss: 3.3808, Avg Acc: 0.3512\n",
            "Epoch 459 Completed - Avg Loss: 3.3801, Avg Acc: 0.3513\n",
            "Epoch 460 Completed - Avg Loss: 3.3791, Avg Acc: 0.3513\n",
            "Epoch 461 Completed - Avg Loss: 3.3779, Avg Acc: 0.3512\n",
            "Epoch 462 Completed - Avg Loss: 3.3775, Avg Acc: 0.3516\n",
            "Epoch 463 Completed - Avg Loss: 3.3780, Avg Acc: 0.3511\n",
            "Epoch 464 Completed - Avg Loss: 3.3753, Avg Acc: 0.3521\n",
            "Epoch 465 Completed - Avg Loss: 3.3751, Avg Acc: 0.3524\n",
            "Epoch 466 Completed - Avg Loss: 3.3748, Avg Acc: 0.3514\n",
            "Epoch 467 Completed - Avg Loss: 3.3747, Avg Acc: 0.3522\n",
            "Epoch 468 Completed - Avg Loss: 3.3731, Avg Acc: 0.3522\n",
            "Epoch 469 Completed - Avg Loss: 3.3727, Avg Acc: 0.3513\n",
            "Epoch 470 Completed - Avg Loss: 3.3707, Avg Acc: 0.3524\n",
            "Epoch 471 Completed - Avg Loss: 3.3700, Avg Acc: 0.3522\n",
            "Epoch 472 Completed - Avg Loss: 3.3714, Avg Acc: 0.3520\n",
            "Epoch 473 Completed - Avg Loss: 3.3695, Avg Acc: 0.3526\n",
            "Epoch 474 Completed - Avg Loss: 3.3689, Avg Acc: 0.3525\n",
            "Epoch 475 Completed - Avg Loss: 3.3680, Avg Acc: 0.3527\n",
            "Epoch 476 Completed - Avg Loss: 3.3684, Avg Acc: 0.3528\n",
            "Epoch 477 Completed - Avg Loss: 3.3664, Avg Acc: 0.3521\n",
            "Epoch 478 Completed - Avg Loss: 3.3656, Avg Acc: 0.3525\n",
            "Epoch 479 Completed - Avg Loss: 3.3643, Avg Acc: 0.3529\n",
            "Epoch 480 Completed - Avg Loss: 3.3638, Avg Acc: 0.3535\n",
            "Epoch 481 Completed - Avg Loss: 3.3624, Avg Acc: 0.3533\n",
            "Epoch 482 Completed - Avg Loss: 3.3637, Avg Acc: 0.3535\n",
            "Epoch 483 Completed - Avg Loss: 3.3612, Avg Acc: 0.3532\n",
            "Epoch 484 Completed - Avg Loss: 3.3628, Avg Acc: 0.3530\n",
            "Epoch 485 Completed - Avg Loss: 3.3615, Avg Acc: 0.3533\n",
            "Epoch 486 Completed - Avg Loss: 3.3609, Avg Acc: 0.3535\n",
            "Epoch 487 Completed - Avg Loss: 3.3614, Avg Acc: 0.3533\n",
            "Epoch 488 Completed - Avg Loss: 3.3599, Avg Acc: 0.3536\n",
            "Epoch 489 Completed - Avg Loss: 3.3591, Avg Acc: 0.3539\n",
            "Epoch 490 Completed - Avg Loss: 3.3584, Avg Acc: 0.3534\n",
            "Epoch 491 Completed - Avg Loss: 3.3583, Avg Acc: 0.3526\n",
            "Epoch 492 Completed - Avg Loss: 3.3570, Avg Acc: 0.3537\n",
            "Epoch 493 Completed - Avg Loss: 3.3565, Avg Acc: 0.3538\n",
            "Epoch 494 Completed - Avg Loss: 3.3566, Avg Acc: 0.3535\n",
            "Epoch 495 Completed - Avg Loss: 3.3539, Avg Acc: 0.3542\n",
            "Epoch 496 Completed - Avg Loss: 3.3549, Avg Acc: 0.3537\n",
            "Epoch 497 Completed - Avg Loss: 3.3529, Avg Acc: 0.3542\n",
            "Epoch 498 Completed - Avg Loss: 3.3518, Avg Acc: 0.3533\n",
            "Epoch 499 Completed - Avg Loss: 3.3509, Avg Acc: 0.3551\n",
            "Epoch 500 Completed - Avg Loss: 3.3512, Avg Acc: 0.3537\n",
            "CPU times: user 1h 13min 49s, sys: 13.8 s, total: 1h 14min 2s\n",
            "Wall time: 1h 14min 45s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "be99be3c"
      },
      "source": [
        "def decoder_inference(model, sentence, tokenizer, device='cpu'):\n",
        "    START_TOKEN = tokenizer.bos_id()\n",
        "    END_TOKEN = tokenizer.eos_id()\n",
        "    MAX_LENGTH = 40\n",
        "\n",
        "\n",
        "    # 전처리\n",
        "    sentence = preprocess_sentence(sentence)\n",
        "\n",
        "    # 인코더 입력: [START] + 인코딩 + [END]\n",
        "    enc_input_ids = [START_TOKEN] + tokenizer.encode(sentence) + [END_TOKEN]\n",
        "    # 차원 확장: (batch_size=1, seq_len)\n",
        "    enc_input = torch.tensor([enc_input_ids], dtype=torch.long, device=device)\n",
        "\n",
        "    # 디코더 입력(dec_input)을 START_TOKEN만 포함한 상태로 시작\n",
        "    dec_input = torch.tensor([[START_TOKEN]], dtype=torch.long, device=device)\n",
        "\n",
        "    model.eval()  # 모델 평가 모드\n",
        "    with torch.no_grad():\n",
        "        for i in range(MAX_LENGTH):\n",
        "            # 모델 forward: (enc_input, dec_input) -> (batch_size=1, seq_len, vocab_size)\n",
        "            logits = model(enc_input, dec_input)\n",
        "\n",
        "            # 마지막 타임스텝의 예측만 추출: shape (1, 1, vocab_size)\n",
        "            # logits[:, -1, :] -> (1, vocab_size)\n",
        "            last_step_logits = logits[:, -1, :]\n",
        "\n",
        "            # argmax로 가장 높은 확률의 토큰 선택\n",
        "            predicted_id = torch.argmax(last_step_logits, dim=-1)  # shape: (1,)\n",
        "\n",
        "            # 종료 토큰이면 중단\n",
        "            if predicted_id.item() == END_TOKEN:\n",
        "                break\n",
        "\n",
        "            # 디코더 입력(dec_input)에 예측 토큰을 이어붙임\n",
        "            predicted_id = predicted_id.unsqueeze(0)  # shape (1,1)\n",
        "            dec_input = torch.cat([dec_input, predicted_id], dim=1)\n",
        "\n",
        "    # 최종 시퀀스: dec_input: (1, seq_len)에서 (seq_len,)로\n",
        "    output_sequence = dec_input.squeeze(0).tolist()  # e.g. [START_TOKEN, ..., 토큰들...]\n",
        "\n",
        "    return output_sequence"
      ],
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2c5da0f"
      },
      "source": [
        "def sentence_generation(model, sentence, tokenizer, device='cpu'):\n",
        "    # 디코더 인퍼런스 -> 예측된 토큰 시퀀스\n",
        "    output_seq = decoder_inference(model, sentence, tokenizer, device=device)\n",
        "\n",
        "    # 토크나이저로 디코딩 (패딩, START/END 토큰 등은 제외하거나 처리)\n",
        "    # 여기서는 단순히 tokenizer.decode() 직접 호출\n",
        "    predicted_sentence = tokenizer.decode(\n",
        "        [token for token in output_seq if token < tokenizer.GetPieceSize()]\n",
        "    )\n",
        "\n",
        "    print(\"입력 :\", sentence)\n",
        "    print(\"출력 :\", predicted_sentence)\n",
        "    return predicted_sentence"
      ],
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "b01f1154",
        "outputId": "caa502ad-98f7-4f91-d466-31b6d7c4ab92"
      },
      "source": [
        "# 결과 테스트 - 그나마 비슷한 대응이 나옴\n",
        "sentence = '배 고프다'\n",
        "sentence_generation(model, sentence, sp, device)"
      ],
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력 : 배 고프다\n",
            "출력 : 저도 바랄게요\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'저도 바랄게요'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "소회\n",
        "\n",
        "1) 처음에는 학습이 거의 이루어지지 않았음\n",
        "1) 일부 전처리에서 이슈를 체크/ 조정함 (큰 이슈는 아니었던 거 같음)\n",
        "2) 하이퍼파라미터를 수정하고 epoch 횟수를 늘려보니 학습반응이 나타나기 시작했음\n",
        "3) 이후 파리미터 변화와 epoch 숫자를 늘려서 보다 높은 성능 개선이 필요해보임. 현재는 정확도가 35% 수준을 돌파하지 못하고 있음"
      ],
      "metadata": {
        "id": "kPx4RZ7QkVqX"
      }
    }
  ]
}